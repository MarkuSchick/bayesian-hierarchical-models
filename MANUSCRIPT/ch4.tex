\section{Application}
\label{sec:application}

In this section we discuss and review literature on the application of Bayesian hierarchical modelling approaches. We then use microdata on student testscore perfomance from the UK to give a practical illustration of the work we have developed in the previous sections of our paper and compare that to the frequentist approach to analyzing multilevel data with the aim of showcasing what differences/similarities there are between the two methods. We then depart from the comparative analysis to look deeper into the Bayesian approach and conduct robustness checks that vary certain aspects using Bayesian sensitivity analysis. We close the section off by a brief discussion of the limitations and challenges encountered in this section as an outlook for future work.


\subsection{Literature on the application of Hierarchical Models}

Bayesian hierarchical models or multilevel models are a suitable approach to consider social contexts as well as individual respondents or subjects. It becomes attractive to consider hierarchical models in place of the common (or popularized) frequentist approach as soon as there is a need to relax the independence of residuals assumption as a result of similarities in the characteristics of a group of respondents or when the researcher seeks to disentangle variability at various levels of the data. These models have been used in various applications throughout fields in research. In education research, \cite{buric2020teacher} use these models to examine the relationship between teacher self-efficacy (TSE), instructional quality (i.e., classroom management, cognitive activation, and supportive climate) and student motivational beliefs (i.e., self efficacy and intrinsic motivation) by using responses from both teachers and students and implementing a sophisticated doubly latent multilevel structural equation modelling approach. The results reflect the necessity to disentangle variability at various levels of the data as the researchers find that, at class level, TSE was positively related to the three dimensions of instructional quality but not to students' motivational beliefs. They also find, as expected, that instructional quality was positively related to studentsâ€™ motivational beliefs.

In agricultural research, \cite{ ramsey2019saying} employ bayesian methods to estimate a two-level model with farm households nested within towns to be able to make inferences on the different farm household behaviours across cities. They estimate the effect of off-farm opportunities on farm exit rates.  Off-farm employment opportunities are thought to influence farm exit rates, though evidence on the sign of this effect has been mixed. Examining this issue in the context of Japanese agriculture, the researchers find that off-farm exit rates are determined by the nature of the off-farm work available to the operators in a town or in which the
operators are already engaged. Towns with higher shares of farm households with no farm sales have higher rates of exit. Also, high shares of off-farm work with income exceeded by the farm income are associated with fewer exits from farming. Bayesian methods allow then to formulate policy reccomendations based on the different behaviours of farming households.

In development economics, \cite{ meager2019understanding} jointly estimates the average effect and the heterogeneity in effects across seven studies using Bayesian hierarchical models to answer questions about external validity that impede consensus on the results from randomized evaluations of microcredit. The researcher uses hierarchical models to disentangle sampling variation from the variation oberved across studies. Hierarchical models address this by jointly modeling sampling variation and true heterogeneity across studies on parallel randomized experiments.
The researcher finds reasonable external validity: true heterogeneity in effects is moderate, and approximately 60 percent of observed heterogeneity is sampling variation. This paper has the potential to revolutionize the field of development economics as the researcher provides a method to establish external validity using multiple studies from different countries. 

On this backdrop, we now demonstrate below, an application to education microdata of a comparison between the likelihood (frequentist)  and bayesian inference approaches. For each of these approaches, we fit a basic varying intercept and slope multilevel linear model with one predictor. We will use the \textit{lmer} function in the \textit{lme4} package for R to determine maximum likelihood estimates of the parameters in linear mixed-effects models. For the Bayesian approach, we then the use \textit{rstanarm} package in R as well. We seek to draw comparisons between the results of both estimation techniques. We expect there to be a difference in the random effects residuals as \cite{browne2006comparison} argue that the Maximum Likelihood approach does not take into account all the uncertainty in a multilevle model.


\subsection{Frequentist and Bayesian Approaches in Practice: Application to Education Data}

A common feature of data structures in education is that units of analysis (e.g., students) are nested in higher organizational clusters (e.g. schools). This kind of structure induces dependence among the responses observed for units within the same cluster. Students in the same school tend to be more alike in their academic and attitudinal characteristics than students chosen at random from the population at large. We will see in this section, a comparison between frequentist and Bayesian approaches, and why a researcher should consider opting for Bayesian hierarchical models in the case of multilevelled data.

\subsubsection{The data}
We will be analyzing the GCSE dataset from \cite{rasbash2000user}. The data include General Certificate of Secondary Education (GCSE) exam scores of 1,905 students from 73 schools in England on a science subject. The GCSE dataset consists of the following 5 variables:
\begin{itemize}
	\item \textit{school}: school identifier
	\item \textit{student}: student identifier
	\item \textit{gender}: gender of a student (M: Male, F: Female)
	\item \textit{written}: total score on written paper
	\item \textit{course}: total score on coursework paper
\end{itemize}
Two components of the exam were recorded as outcome variables: written paper and course work. In this application, only the total score on the coursework paper (course) will be analyzed. In our example, the model estimates the effect of gender on test scores.

\subsubsection{Likelihood inference approach}
In this subsection, we fit a basic varying intercept, varying slope multilevel linear model with one predictor using the \textit{lmer()} functions. Functions such as lmer are based on a combination of maximum likelihood (ML) estimation methods of the model parameters, and empirical Bayes (EB) predictions of the varying intercepts and/or slopes resulting in the Best Linear Unbiased Predictions (BLUPs) of the model parameters. We use these functions so that our parameter estimates from both the ML and the Bayesian framework are comparable.

\subsubsection*{Model: Varying intercept, varying slope with a single predictor}
\label{subsubsection:model}
In this model, we allow both the intercept and the slope to vary randomly across schools using the following model:

\begin{align}
	y_{ij} &= \alpha_j + \beta_j x_{ij} +\epsilon_{ij} \,, \text{ where } \epsilon_{ij} \sim \normal{0, \sigma_y^2} \\
	\alpha_j &= \mu_\alpha + u_j \,, \text{ where } u_j \sim \normal{0, \sigma_\alpha^2}  \\
	\beta_j &= \mu_\beta + v_j \,, \text{ where } v_j \sim \normal{0, \sigma_\beta^2}
\end{align}
or in reduced form as $y_{ij} = \mu_\alpha + \mu_\beta x_{ij} + u_j + v_j x_{ij} + \epsilon_{ij}$, where $\epsilon_{ij} \sim \normal{0,\sigma_{y}^{2}}$ and
\begin{align}
	\left( \begin{matrix} u_j \\ v_j \end{matrix} \right) \sim \normal{ \left( \begin{matrix} 0 \\ 0 \end{matrix} \right) ,\left( \begin{matrix} { \sigma  }_{ \alpha  }^{ 2 } & \rho { \sigma  }_{ \alpha  }{ \sigma  }_{ \beta  } \\ \rho { \sigma  }_{ \alpha  }{ \sigma  }_{ \beta  } & { \sigma  }_{ \beta  }^{ 2 } \end{matrix} \right)}.
\end{align}
Under the \textit{Fixed effects} part of the results in Table \ref{tab:results}, we see that the intercept $\mu_{\alpha}$, averaged over the population of schools, is estimated as $69.425$ and the slope $\mu_{\beta}$ as $7.128$. The average regression line across schools is thus estimated as $\hat{y}_{ij} = 69.424 + 7.128 x_{ij}$, with the residual within-school standard deviation estimated as $\hat{\sigma}_{y}=13.03$ (shown under the \textit{Random Effects} part). The estimated standard deviations of the school intercepts and the school slopes are $\hat{\sigma}_{\alpha}= 10.15$ and $\hat{\sigma}_{\beta}=6.92$ respectively. In Table \ref{tab:results} under the \textit{Random effects} part, we do not report the estimates as they are always zero for the within and between school variances.

\begin{table}[H]
	\centering
	\caption{{\small Comparison of Maximum Likelihood and Bayesian estimates.}}
	\label{}
	
	\smallskip
	\begin{tabular}{l*{2}{c}}
		\toprule \\[-1.0em]
		\multicolumn{3}{c}{Dependent variable: Course test score}\\ \\[-1.0em]
		&ML &Bayes\\
		\midrule \\[-1.0em]
		\emph{A. Random effects}\\
		Intercept & -- & --\\
		& (10.146) & (10.249)\\
		Female & -- & --\\
		& (6.924) & (7.099)\\
		Residual & -- & --\\
		& (13.030) & (13.0)\\
		\emph{B. Fixed effects} \\
		Intercept & 69.425 & 69.413\\
		& (1.352) & (1.287)\\
		Female & 7.128 & 7.132\\
		& (1.131) & (1.165)\\
		\hline
		Students&1725&1725\\
		Schools&73&73\\
		\bottomrule
		
	\end{tabular}
	\label{tab:results}
\end{table}
Treating the estimates of $\mu_\alpha$, $\mu_\beta$, $\sigma^2_{y}$, and $\sigma^2_{\alpha}$ as the true parameter values, we can then obtain the Best Linear Unbiased Predictions (BLUPs) for the school-level residuals $\hat{u}_j = \hat{\alpha}_{j} - \hat{\mu}_{\alpha}$ and $\hat{v}_j = \hat{\beta}_{j} - \hat{\mu}_{\beta}$. The BLUPs are equivalent to the so-called Empirical Bayes (EB) prediction, which is the mean of the posterior distribution of $u_{j}$ and $v_{j}$ given all the estimated parameters, as well as the random variables $y_{ij}$ and $x_{ij}$ for the cluster.  These predictions are called "Bayes" because they make use of the pre-specified prior distribution \footnote{We elaborate more on prior distributions in Section the full Bayesian approach section} $\alpha_j \sim \normal{\mu_\alpha, \sigma^2_\alpha}$, and by extension $u_j \sim \normal{0, \sigma^2_\alpha}$ (analogous for the slope $\beta_j$), and called "Empirical" because the parameters of these priors, $\mu_\alpha$, $\mu_\beta$,  $\sigma^2_{\beta}$ and $\sigma^2_{\alpha}$, in addition to $\sigma^2_{y}$, are estimated from the data.

Compared to the Maximum Likelihood (ML) approach of predicting values for $u_j$ and $v_j$ by using only the estimated parameters and data from cluster $j$, the EB approach additionally considers the prior distributions of $u_{j}$ and $v_{j}$, and produces predicted values closer to $0$ (a phenomenon described as shrinkage or partial pooling).  To see why this phenomenon is called shrinkage, we can take an example of the intercept $u_j$ and express the estimates obtained from EB prediction as $\hat{u}_j^{\text{EB}} = \hat{R}_j\hat{u}_j^{\text{ML}}$ where $\hat{u}_j^{\text{ML}}$ are the ML estimates, and $\hat{R}_j = \frac{\sigma_\alpha^2}{\sigma_\alpha^2 + \frac{\sigma_y^2}{n_j}}$ is the so-called Shrinkage factor. This would be analogous for $v_j$ as well.

By using the \textit{ranef} function, we can retrieve information on how much the intercept and slope are shrunk up or down in particular schools. For example, in the third school (22710) in the dataset shown in Table \ref{tab:shrinkage} (left), the estimated intercept is $10.66$ higher than average and the estimated slope is $-4.31$, so that the school-specific regression line is $(69.43 + 10.66) + (7.13 - 4.31) x_{ij}$ which turns out to be $ 80.09 + 2.82 x_{ij}$. This suggests that although the fixed effect average is positive, (thus on average and across schools, female students perform better than males), in this particular school, female students do on average perform $-4.31$ points less than the across-school average female test score.

\begin{table}[!htb]
	\caption{Global caption}
	
	\begin{minipage}{.5\linewidth}
		\caption{}
		\centering
		{
			\begin{tabular}{l | c c c c c}
				School ID & Intercept & Slope (FemaleF)\\
				\hline
				20920 & -19.25 & 14.64 \\
				22520 & -20.18 & 4.73 \\
				22710 & 10.66 & -4.31 \\
				22738  & 0.63 & -0.029 \\
				22908 & -3.46 & -6.13 \\
				23208 & 7.85 & -4.41
			\end{tabular}
		}
	\end{minipage}%
	\begin{minipage}{.5\linewidth}
		\centering
		\caption{}
		{
			\begin{tabular}{l | c c c c c}
				School ID & Intercept & Slope (FemaleF)\\
				\hline
				20920 & -18.79 & 14.16 \\
				22520 & -20.07 & 4.60 \\
				22710  & 10.46 & -3.90 \\
				22738  & 0.73 & -0.083  \\
				22908 & -3.62 & -5.97 \\
				23208  & 7.77 & -4.33
			\end{tabular}
		}
		
	\end{minipage}
	\caption{{\small Shrinkage factor for the ML estimates (left) and Bayesian estimates (right): an extract of 6 schools}}
	\label{tab:shrinkage}
\end{table}

We can also retrieve the information on how much the intercept and slope are shrinking up or down for the full bayesian estimation method to compare them to the BLUPs as shown in Table \ref{tab:shrinkage} (right). We indeed find that, for an extract of the first six schools, there is not a significant difference in the estimates except for a few instances. The BLUPs then allow us to compare results between ML and Bayes estimates.

\subsubsection*{Partial Pooling in multilevel models}
\cite{gelman2006data} characterize multilevel modelling as partial pooling (also called shrinkage), which is a compromise between two extremes: complete pooling in which the clustering is not considered in the model at all, and no pooling, in which separate intercepts are estimated for each school as coefficients of dummy variables. The estimated school-specific regression lines in the above ML model (obtained from BLUPs as shown above) are based on partial pooling estimates. To show this, we first estimate the intercept (the same could be done for the slope) in each school in three ways; 1) complete pooling with OLS, 2) no pooling with OLS, and 3) partial pooling with ML. We then plot the data and school-specific regression lines for a selection of eight schools as illustrated in Figure \ref{fig:pooling}. We see that the estimated school-specific regression line from the partial pooling estimates lies between the complete-pooling and no-pooling regression lines. There is more pooling (purple dotted line closer to red dotted line) in schools with larger sample sizes. By using the BLUPs method we ensure that the ML estimates are partial pooling estimates and therefore are comparable to the Bayesian framework.

\subsubsection{Full Bayesian Inference Approach}
As previously mentioned, functions such as \textit{lmer} are based on a combination of maximum likelihood (ML) estimation of the model parameters, and empirical Bayes (EB) predictions of the varying intercepts and/or slopes. However, in some instances, when the number of groups is small or when the model contains many varying coefficients or non-nested components, the ML approach may not work as well in part because there may not be enough information to estimate variance parameters precisely. In such cases, a fully Bayesian approach provides reasonable inferences with the added benefit of accounting for all the uncertainty in the parameter estimates when predicting the varying intercepts and slopes, and their associated uncertainty. This is one of the reasons why a fully Bayesian estimation is particularly interesting. Other reasons are discussed in section \ref{subsection:Deeper}. We now demonstrate below, how to fit the model from section \ref{subsubsection:model} in a fully Bayesian framework using the \textit{rstammarm} package. This package is a wrapper for the \textit{rstan} package that enables the most common applied regression models to be estimated using Markov Chain Monte Carlo (MCMC) but still be specified using customary R modelling syntax.

\subsubsection*{Model: Varying intercept, varying slope with a single predictor}

We can implement a fully Bayesian estimation for multilevel models with only minimal changes to our existing code with \textit{lmer} from the maximum likelihood application in the previous section by prepending \textit{stan\textunderscore} to the \textit{lmer} call. The \textit{stan\textunderscore lmer} function is similar in syntax to \textit{lmer} but rather than performing maximum likelihood estimation, Bayesian estimation is performed via MCMC. As each step in the MCMC estimation approach involves random draws from the parameter space, we include a seed option to ensure that each time the code is run, \textit{stan\textunderscore lmer} outputs the same results. We also need to specify prior distributions for each of our parameters.

\subsubsection*{Specifying prior distributions}

The normal distribution for the $\alpha_{j}$'s and $\beta_{j}$'s can be thought of as a prior distributions for these varying intercepts. In the full Bayesian inference setting, all the hyperparameters ($\mu_{\alpha}$, $\mu_{\beta}$, $\sigma_{\alpha}$ and $\sigma_{\beta}$), along with the other unmodeled parameters (in this case, $\sigma_{y}$) need a prior distribution. For this illustration, we use weakly informative priors that provide moderate regularization and help stabilize computation.

Before accounting for the scale of the variables, $\mu_{\alpha}$ and $\mu_{\beta}$ are given normal prior distributions with mean 0 and standard deviation 10.  That is, for example, $\mu_{\alpha} \sim N(0, 10^2)$. The standard deviation of this prior distribution, 10, is ten times as large as the standard deviation of the response if it were standardized. This should be a close approximation to a noninformative prior over the range supported by the likelihood, which should give inferences similar to those obtained by maximum likelihood methods if similarly weak priors are used for the other parameters. \textit{Rstanarm} scales the priors in relation to the scale of variables in the estimation process. The (unscaled) prior for $\sigma_{y}$ is set to an exponential distribution with the rate parameter set to 1.

Additionally, we are also required to specify a prior for the covariance matrix $\Sigma$ for varying (or "random") effects $\alpha_j$ and $\beta_j$ in this Model.  \textit{Stan\_lmer} decomposes this covariance matrix (up to a factor of $\sigma_y$) into (i) a correlation matrix $R$ and (ii) a matrix of variances $V$, and assigns them separate priors as shown below. 


\begin{align}
	\Sigma &=
	\left(\begin{matrix}
		\sigma_\alpha^2 & \rho\sigma_\alpha \sigma_\beta \\
		\rho\sigma_\alpha\sigma_\beta&\sigma_\beta^2
	\end{matrix} \right)\\ &=
	\sigma_y^2\left(\begin{matrix}
		\sigma_\alpha^2/\sigma_y^2 & \rho\sigma_\alpha \sigma_\beta/\sigma_y^2 \\
		\rho\sigma_\alpha\sigma_\beta/\sigma_y^2 & \sigma_\beta^2/\sigma_y^2
	\end{matrix} \right)\\ &=
	\sigma_y^2\left(\begin{matrix}
		\sigma_\alpha/\sigma_y & 0 \\
		0&\sigma_\beta/\sigma_y
	\end{matrix} \right)
	\left(\begin{matrix}
		1 & \rho\\
		\rho&1
	\end{matrix} \right)
	\left(\begin{matrix}
		\sigma_\alpha/\sigma_y & 0 \\
		0&\sigma_\beta/\sigma_y
	\end{matrix} \right)\\
	&= \sigma_y^2VRV.
\end{align}
The correlation matrix $R$ is 2 by 2 matrix with 1's on the diagonal and $\rho$'s on the off-diagonal. 

\textit{Stan\_lmer} assigns the correlation matrix $R$ an LKJ prior (\cite{lewandowski2009generating}), with regularization parameter 1.  This is equivalent to assigning a uniform prior for $\rho$.  The more the regularization parameter exceeds one, the more peaked the distribution for $\rho$ to take the value 0.

The matrix of (scaled) variances $V$ can first be collapsed into a vector of (scaled) variances, and then decomposed into three parts, $J$, $\tau^2$ and $\pi$ as shown below.

\begin{align}
	\left(\begin{matrix}
		\sigma_\alpha^2/\sigma_y^2 \\
		\sigma_\beta^2/\sigma_y^2
	\end{matrix} \right) =
	2\left(\frac{\sigma_\alpha^2/\sigma_y^2 + \sigma_\beta^2/\sigma_y^2}{2}\right)\left(\begin{matrix}
		\frac{\sigma_\alpha^2/\sigma_y^2}{\sigma_\alpha^2/\sigma_y^2 + \sigma_\beta^2/\sigma_y^2} \\
		\frac{\sigma_\beta^2/\sigma_y^2}{\sigma_\alpha^2/\sigma_y^2 + \sigma_\beta^2/\sigma_y^2}
	\end{matrix} \right)=
	J\tau^2 \pi.
\end{align}



In this formulation, $J$ is the number of varying effects in the model (here, $J=2$), $\tau^2$ can be viewed as an average (scaled) variance across the varying effects $\alpha_j$ and $\beta_j$, and $\pi$ is a non-negative vector that sums to 1 (called a Simplex/probability vector).  A symmetric Dirichlet \footnote{The Dirichlet distribution is a multivariate generalization of the beta distribution with one concentration parameter, which can be interpreted as prior counts of a multinomial random variable (the simplex vector in our context).} distribution with concentration parameter set to 1 is then used as the prior for $\pi$.  By default, this implies a jointly uniform prior over all Simplex vectors of the same size.  A scale-invariant Gamma prior with shape and scale parameters both set to 1 is then assigned for $\tau$.  This is equivalent to assigning as a prior, the exponential distribution with rate parameter set to 1 which is consistent with the prior assigned to $\sigma_y$. (\cite{lewandowski2009generating}) provides a more indepth discussion on the collapsing and decomposition of the matrix $R$.

\subsubsection*{Estimation results}
In Table \ref{tab:results}, we see the point estimate of $\mu_{\alpha}$ from the Bayesian estimation is $69.413$ and this corresponds to the median of the posterior draws.  This is similar to the ML estimate obtained previously ($69.425$).  The point estimate for $\mu_{\beta}$  is slightly different in this Model ($7.132$ compared to the ML estimate $7.128$). When using the Bayesian estimation function, standard errors are obtained by considering the median absolute deviation (MAD) of each draw from the median of those draws.  The Bayesian estimate for $\sigma_{\alpha}$ ($10.249$) and $\sigma_{\beta}$ ($7.099$) are larger than the ML estimate $10.146$ and ($6.924$) respectively. 

\subsubsection*{Remarks on the comparison excercise}
The discrepancy observed between the ML and Bayes residuals may be because the ML approach does not take into account the uncertainty in $\mu_{\alpha}$ and $\mu_{\beta}$ when estimating $\sigma_{\alpha}$ and $\sigma_{\beta}$. Full Bayes, on the other hand, propagates the uncertainty in the hyperparameters throughout all levels of the model and provides more appropriate estimates of uncertainty \cite{browne2006comparison}. Noting this and taking into account the outcome illustrated in the monte carlo study, we conclude that it may be beneficial as a researcher to start by quickly fitting many specifications in building a model using the ML approach, and then taking advantage of the flexibility of a fully Bayesian approach to obtain simulations summarizing uncertainty about coefficients, predictions, and other quantities of interest.

\subsection{Looking Deeper into the Bayesian Approach}
\label{subsection:Deeper}
In education research and practice it is often of interest to compare the schools included in the data. Relevant questions include 1) what are the rankings of these schools within the sample, 2) what is the difference between the means of schools A and B, and 3) is school A performing better than school B? When non-Bayesian methods are used, we can attempt to make such comparisons based on empirical Bayes (or Best Linear Unbiased) predictions of the varying slope and/or intercepts, but it will generally be impossible to express the uncertainty for nonlinear function such as rankings. \cite{goldstein1996league} discuss this in greater detail.

Having samples of all the parameters and varying intercepts and slopes from their joint posterior distribution makes it easy to draw inferences about functions of these parameters.
During estimation, four MCMC chains of 2,000 iterations each are generated. Half of these iterations in each chain are used as warm-up/burn-in (to allow the chain to converge to the posterior distribution), and hence we only use 1,000 samples per chain. These MCMC-generated samples are taken to be drawn from the posterior distributions of the parameters in the model. We can use these samples for predictions, summarizing uncertainty and estimating credible intervals for any function of the parameters.
We can then generate a matrix for varying intercepts $\alpha_j$ and slopes $\beta_j$ as well as vectors containing the draws for the within standard deviations and the between variance by manipulating this matrix.

For the purposes of simplicity and illustration, we will continue our analysis from here onwards using the varying intercept draws. We have saved 4,000 posterior draws (from all four chains) for the varying intercepts $\alpha_{j}$ of the 73 schools. For example, the first column of the 4,000 by 73 matrix is a vector of 4,000 posterior simulation draws for the first school's (School 20920) varying intercept $\alpha_{1}$.  One quantitative way to summarize the posterior probability distribution of these 4,000 estimates for $\alpha_{1}$ is to examine their quantiles by computing mean, SD, median, and $95\%$ credible interval of varying intercepts as shown in Table \ref{tab:summary_data}.

\begin{table}[ht]
	\centering
	\def\arraystretch{1.3}
	{\small
		\begin{tabular}{l | c c c c c}
			School & Posterior mean & Posterior SD & Q2.5 & Q50 & Q97.5\\
			\hline
			b[(Intercept) school:20920] & 50.47 & 5.90 & 38.59 & 50.45 & 61.73 \\
			b[(Intercept) school:22520] & 49.38 & 2.71 & 43.99 & 49.36 & 54.54 \\
			b[(Intercept) school:22710] & 79.84 & 4.36 &  71.42 & 79.87 & 88.54 \\
			b[(Intercept) school:22738] & 70.10 & 4.79 & 60.71 & 70.15 & 79.35  \\
			b[(Intercept) school:22908] & 65.88 & 6.99 & 52.18 & 65.73 & 79.93 \\
			b[(Intercept) school:23208] &  77.15 & 4.52 & 67.96 & 77.24 & 85.87
		\end{tabular}
	}
	\caption{{\small Summary statistics for posterior mean, SD and $95\%$ credible intervals.}}
	\label{tab:summary_data}
\end{table}

\subsubsection{Ranking varying intercepts by school}

We can answer question 1) by producing a caterpillar plot to show the fully Bayes estimates for the school varying intercepts in rank order together with their $95\%$ credible intervals in Figure \ref{fig:ranking}. We can also use the same approach to generate $95\%$ credible intervals for $\beta_j$ $\sigma_y$, $\sigma_\alpha$ and $\sigma_\beta$. From this plot, we are able to identify the schools in which males are performing below or above the mean school perfomance illustrated by the red line.

\subsubsection{Making comparisons between individual schools}

To answer question 2) and 3), we compare two randomly chosen schools as an example: Schools 60501 (the $21^{st}$ school) and 68271 (the $51^{st}$ school). We already have 4,000 posterior simulation draws for both schools. To make inferences regarding the difference between the average scores of the two schools, we can simply take the difference between the two vectors of draws $\alpha_{51} - \alpha_{21}$.

We can investigate the posterior distribution of the difference with descriptive statistics and a histogram. From Figure \ref{fig:differences}, we can see that the expected mean difference is 5.747 with a standard deviation of 6.094 and a wide range of uncertainty. The 95\% credible interval is [-6.55, 17.49], so we are 95\% certain that the true value of the difference between the two schools lies within this range, given the data.

We also can get the proportion of simulation runs that School 60501 has a higher mean than School 68271 as shown in Table \ref{tab:comparison}.

\begin{table}[ht]
	\centering
	\def\arraystretch{1.3}
	{\small
		\begin{tabular}{l | c c c c c}
			FALSE & TRUE \\
			\hline
			17.28 & 82.73
		\end{tabular}
	}
	\caption{{\small The  posterior probability that School 60501 is better than School 68271.}}
	\label{tab:comparison}
\end{table}

This means that the posterior probability that School 60501 is better than School 68271 is 82.7\%. Any pair of schools within the sample of schools can be compared in this manner.

\subsubsection{Convergence}
All chains must converge to the target distribution for inferences to be valid. The diagnostic which we use to assess whether the chains have converged to the posterior distribution is the statistic $\hat{R}$ (\cite{gelman1992inference}). Each parameter has the $\hat{R}$ statistic associated with it and this statistic is automatically generated during estimation.


The $\hat{R}$ is essentially the ratio of between-chain variance to within-chain variance analogous to ANOVA. The $\hat{R}$ statistic should be less than 1.1 if the chains have converged. Figure \ref{fig:rhat} illustrates the $\hat{R}$ statistic and shows that the chains converge.


\subsubsection{Robustness checks}

We apply a concept from Bayesian sensitivity analysis to explore how the inferences change as we change the prior standard deviation. We use three different priors: an informative prior, a weakly informative one and an uninformative prior. Although we vary the means for both $\mu_{\alpha}$ and $\mu_{\beta}$, we report the comparisons in output for $\mu_{\alpha}$ in keeping with the rest of Section \ref{subsection:Deeper}.

\begin{table}[!ht]
	\begin{center}
		\begin{tabular}{l | c c c c c}
			Prior specification & Posterior mean & Posterior SD & Q2.5 & Q50 & Q97.5\\
			\hline
			Uninformative prior $\normal{0,100}$ & 69.44 & 4.37 & 60.85 & 69.44 & 77.98 \\
			Weakly informative prior $\normal{0,10}$ & 69.32 &   4.38  & 60.75 &   69.33 &   77.88 \\
			Informative prior $\normal{0,1}$ & 69.20 & 4.61 &  60.17 &  69.19  & 78.24
		\end{tabular}
	\end{center}
	\caption{The average posterior mean, sd and credible interval monte carlo draws for $\mu_{\alpha}$.}
	\label{tab:robustness}
\end{table} 
As the results in Table \ref{tab:robustness} illustrate, we do not observe a large difference in the estimates which indicates that the Bayesian method is robust to changes in the standard deviation of the class of normally distributed priors. According to \cite{edwards1963bayesian}  the choice of prior within some class of candidate priors has little effect on the posterior, i.e., that the estimates found using one choice of prior within the same class are similar to what is found with another choice in the same distribution class.  

\subsection{Challenges, Limitations and Future Outlook}
For simplicity we have explored an empirical model with varying slopes and intercepts that includes one predictor. In as much as this provides parsimonity in estimation and illustration, there are other features of Bayesian hierarchical models that could be explored beyond our application. For example, by increasing the number of predictors, we can give a more meaningful interpretation to our findings from the estimation. We could also add more features to the school level equations where we vary the slope and intercepts, as illustrated in the monte carlo study. This provides more information to the estimation process about the students in the schools. It would also be interesting to estimate the same model on a non-nested dataset. For future research therefore, we hope to extend this discussion to explore more components of the varying slopes, varying intercept model and linear Bayesian Hierarchical models in general.
