\section{Bayesian Thinking and Estimation}
In this section we will introduce the core topics of Bayesian data analysis, and whenever possible compare proposed methods and results to their frequentist counterpart.
As we will see, Bayesian statistics differs from mainstream statistics on a fundamental level; Thus, we have to start there.

Before beginning let us talk shortly about our notation.
We try to use a standard notation whenever possible, nonetheless, we make one exception in that we write $p(X)$ for the probability density function of the random variable $X$, where $X$ might be scalar-valued or vector-valued.
In cases where this leads to confusion we introduce a subscript as $p_X(\cdot)$.

\subsection{Probabilistic Modeling}
Before going further, let us first formalize our notion of stochastic modeling.
Imagine being interested in some \textit{phenomena}, for example the effect of bigger class sizes on school children performance.
Most phenomena cannot be observed directly and only manifest themself through some latent variable system.
We can still hope to learn about the phenomena by studying the observational process around it.
Clearly the way in which a phenomena reveals itself is dependent on its environment;
The observed effects for school-children in sub-saharan africa might look very different to the ones in central europe.
To derive sensible results from our analysis we need to postulate the existence of a \textit{true data generating process}, which captures the way in which the observational process adheres to the effects of the phenomena of interest given its environment.
We define this object as a probability distribution $p_0$ on the observational space $\mathbb{Z}$.
In this step we move from a model to a probabilistic model, in that we allow our system of interest to be influenced by randomness and not only be characterized by a deterministic process.
In practice $p_0$ is rarely known, therefore the challenge lies in recovering a distribution using the observed data which is as close as possible to $p_0$.
This is usually done by assuming that the true data generating process falls in some class of models, for example a linear model with normal errors.
Formally, we restrict our attention to a subset of potential observational processes $\mathbb{M}$ over the whole space of distributions on $\mathbb{Z}$.
The beauty of using a model class approach in the construction of $\mathbb{M}$ is that for each distribution $p \in \mathbb{M}$, we can find a parameterization $\theta$ in the configuration space $\Theta$; For example, the class of multivariate normal distributions is parameterized by its mean and covariance $(\mu, \Sigma) = \theta \in \Theta$.
The goal of all subsequent statistical analysis is then to utilize the observed data to determine the regions in $\Theta$ which are most consistent with $p_0$ and simultaneously to capture our uncertainty about these statements.
If $p_0 \in \mathbb{M}$ we can find a parameterization $\theta_0$ which corresponds to the true data generating process; naturally we seek estimators that determine regions close to $\theta_0$. If, however, $p_0 \not\in \mathbb{M}$ we enter the world of model misspecification which leads to all sorts of problems.
For everything that follows let us therefore make the omnipresent assumption that $p_0 \in \mathbb{M}$.

\subsection{Schools of Thought}
Next we discuss how the Bayesian and the classical mindset differ. In particular we focus on the predominant way of thinking for most of statistical history, \textit{frequentist statistics}. We do not aim at an exhaustive overview here nor do we presume that the individual statistician belongs to one and only one of the following categories.

\paragraph{Frequentist.} The frequentist approach assumes that the true data generating process is completely specified by an unkown but fixed quantity $\theta_0 \in \Theta$.
For a sensible comparison we focus on approaches where the \emph{likelihood function} $p(z; \theta)$ is defined either explicitly or implicitly during the modeling process, as for example in maximum-likelihood estimation.
The main challenges include finding a (point) estimator for $\theta_0$, quantifying the uncertainty of the estimate and testing hypothesis.
One fundamental idea which stretches over all these topics is the interpretation of probability as the limit of an infinite sequence of relative frequencies ---hence the name.
That is, the probability of an event happening is just the limit of the frequency of that event happening over infinitely many independent experiments.
What are the implications of this understanding of probability?
Many interesting questions do not provide us with a thought experiment in which we can consider an ever increasing sequence of experiments.
In these cases using probability is either trivial or lacking an indisputable interpretation.
As we use the mathematical rigor of probability theory in our formal derivations, we will obtain (mathematically) correct results; however, the interpretation of these results might be highly unintuitive ---a known example is provided by the interpreation of confidence intervals; is the true value contained in the given confidence interval? We do not know, but were we to repeat the analysis with newly drawn data infinitely often, the true value would be in the interval 95\% of the time.
Since $\theta_0$ is fixed all probability statements regarding this object are trivial, that is, either one or zero. This propagates to the problem of hypothesis testing.
All hypothesis are either true or false and therefore have probabilities of one or zero.
A hypothesis $H_0$ is rejected if conditional on $H_0$ being true the probability of observing the data in the given sample is lower than some threshold, i.e. $P(\text{data} \mid H_0) < \alpha$.
Note that this statement does not tell us anything about $H_0$ directly, but only about the data at hand.

\paragraph{Bayesian.} The Bayesian approach also assumes that there may be a true data generating process specified by some (maybe fixed) quantity $\theta_0 \in \Theta$.
The main difference is their understanding of probability as a subjective quantification of uncertainty.
In this view one is not limited to assigning non trivial probability statements only to objects that appear random in sequential experiments.
We can see the direct utility of this liberation by considering a special case of Bayes theorem
\begin{align}
  p(\theta \mid \text{data}) = \frac{p(\text{data} \mid \theta) p(\theta)}{p(\text{data})} \propto p(\text{data} \mid \theta) p(\theta) \,,
\end{align}
which reads
\begin{align}
  posterior = \frac{likelihood \times prior}{evidence} \propto likelihood \times prior \,.
\end{align}
In the frequentist setting this is of no use, since the statement $p(\theta)$ is nonsensical ---remember that probabilistic statements about fixed quantities are meaningless from a frequentist perspective.
This already outlines the main criticism of Bayesian analysis: Where does the prior $p(\theta)$ come from?
With the scientific goal of objectivity in mind, many feel uneasy with results being dependent on a subjective choice of a prior.
In what follows we will embark on the Bayesian idea without providing much more fundamental criticism; nonetheless, when adequate we will consider the influence of different priors on the posterior.
To end this comparison, what then are the main tasks associated with a Bayesian analysis?
These can be categorized by (i) obtaining the posterior distribution (or something equivalent) and (ii) communicating the information held in the posterior.
The first consists of defining the likelihood and (additionally to the frequentist approach) constructing a prior distribution and afterwards combining those to compute the posterior.
This computation can sometimes be achieved analytically, but in most cases one has to rely on algorithms to obtain samples of the posterior.
The second part consists of plotting the marginal posterior distributions, computing expectations of the form $\Exp{h(\theta) \mid \text{data}}$ and testing hypothesis.
All of the above can be done independent of the posterior being available analytically or through samples.
A clear difference can be seen when considering hypothesis testing.
Sacrificing \textit{objectivity} allows us to answer the questions we usually want to ask:
\begin{align}
  P(H_0: \theta \in S \mid \text{data}) = \int_{\theta \in S} p(\theta \mid \text{data}) \mathrm{d}\theta \,.
\end{align}
In the subsequent paragraphs we will be mostly occupied with the first category, computing the posterior, with occasional remarks on the second; in particular, the approximation of expectations.

\subsection{Solving for the posterior analytically}
In this subsection we present the analytical derivation of the posterior distribution of mean and variance parameters in a univariate normal model for two priors.
We will compare the results to the appropriate classical method, namely maximum likelihood.

In both cases, let us assume that we observe an iid sample $y = (y_1, \mydots, y_n)$
with $y_i \sim \normal{\mu, \sigma^2}$.
Our interest lies in solving for the marginal posteriors $p(\mu \mid y)$ and $p(\sigma^2 \mid y)$.

To be precise, in a full bayesian analysis we assume $\theta = (\mu, \sigma^2)$ to be a random quantity, thus the correct statement should be $y_i \mid \theta \sim \normal{\mu, \sigma^2}$.
In situations where this conditional dependence is clear many writers will use the first notation.
Here we try to be as pedantic as possible to avoid any confusion and will therefore stick to the second notation.

As it will be of major importance in the subsequent sections we remind the reader of some probability distributions uncommon in the non-Bayesian world.

\begin{definition}{(Scaled inverse $\chi^2$ distribution).}
  Let $\nu > 0$ and $\tau^2 > 0$ be parameters representing degrees of freedom and scale, respectively. The family of \emph{scaled inverse $\chi^2$ distributions} is characterized by its probability density function, namely
  \begin{align}
    p(x) \propto x^{-(1 + \nu / 2)} \EXP{\frac{-\nu \tau^2}{2 x}} \quad \text{for} \, x \in (0, \infty) \,,
  \end{align}
  where the constant of integration is ignored for clarity.
  We write $X \sim \scaledInvChi{\nu, \tau^2}$ to denote that the random variable $X$ follows a scaled inverse $\chi^2$ distribution with parameters $\nu$ and $\tau^2$.
\end{definition}

\subsubsection*{Uninformative Prior}
We start our first Bayesian analysis by considering a prior which contains virtually no information.
This results in an analysis being mainly, if not completely, driven by the likelihood.
A common assumption is independence of the individual priors, that is $p(\theta) = p(\mu, \sigma^2) = p(\mu) p(\sigma^2)$.
A natural choice of declaring full ignorance of prior information is to assign a prior over the complete domain of the random parameter. For our case this mean $p(\mu) \propto 1$.
We note that this does not define a proper proability distribution, which will not matter in this case but can lead to problems in others; see for example section 4.2 in \citet{kass1996}.
Since the variance is restricted to be positive we impose a uniform prior on the log-transform thereof: $p(\log \sigma) \propto 1$.
Using that $x \mapsto \text{exp}^2(x)$ is one-to-one we can apply standard methods, which yield the density of the transformed variable
\begin{align}
  p(\mu, \sigma^2) \propto (\sigma^2)^{-1} \,.
\end{align}
As usual the likelihood is given by
\begin{align}
  p(y \mid \mu, \sigma^2) \propto (\sigma^2)^{-n/2} \EXP{-\frac{1}{2\sigma^2}\sum_i (y_i - \mu)^2} \,,
\end{align}
where we dropped all proportionality constants.
Using the above we can apply Bayes theorem to yield
\begin{align}
  p(\mu, \sigma^2 \mid y) &\propto p(y \mid \mu, \sigma^2) p(\mu, \sigma^2)\\
  &\propto (\sigma^2)^{-(n+2)/2} \EXP{-\frac{1}{2\sigma^2}\sum_i (y_i - \mu)^2} \,.
\end{align}
From here we can derive the marginals by integrating out the respective other parameter. This is formalized in the following two proposition.

\begin{proposition}
  Under the uniform prior from above we find
  \begin{align}
    \mu \mid y &\sim t_{n-1}(\bar{y}, s^2/n) \,,\\
    \sigma^2 \mid y &\sim \scaledInvChi{n-1, s^2} \,,
  \end{align}
  where $s^2 = \frac{1}{n-1} \sum_i (y_i - \bar{y})^2$ denotes the sample variance and $\bar{y} = \frac{1}{n} \sum_i y_i$ the sample mean.
\end{proposition}
\begin{proof}
See appendix.
\end{proof}

Having derived the marginal posterior distributions, we can compare the results to their maximum likelihood (ML) counterpart.
Since the ML estimates $\left(\argmax{\theta \in \Theta}p(y \mid \theta)\right)$ are point estimates, we consider the similar \emph{maximum a posteriori} (MAP) estimate $\left(\argmax{\theta \in \Theta} p(\theta \mid y)\right)$, as well as the posterior mean and variance. All results are summarized in table \ref{table:comp_uniform_bay_ml}.

\begin{table}[ht]
\centering
\def\arraystretch{1.3}
{\small
 \begin{tabular}{c | c c c c c}
 Parameter & ML Estimate & ML Variance & MAP & Posterior Mean & Posterior Variance\\[0.5ex]
 \hline
 $\mu$ & $\bar{y}$ & $\sigma^2/n$ & $\bar{y}$ & $\bar{y}$ & $s^2 / n$\\
 $\sigma^2$ & $\frac{n-1}{n} s^2$ & $2 \sigma^4 /n$ & $\frac{n-1}{n+1} s^2$ & $\frac{n-1}{n-3} s^2$ & $\frac{2 (n-1)^2}{(n-3)^2 (n-5)} s^4$\\
 \end{tabular}
 }
\caption{\small {Comparison of Bayesian estimates using an uninformative prior and ML estimates. See appendix for a detailed derivation.}}
\label{table:comp_uniform_bay_ml}
\end{table}


\subsubsection*{Conjugate Prior}
We have seen that using an uninformative prior leads to results that are very similar to the ones obtained by a ML approach.
In case information on the parameters is available prior to observing the data we can utilize this fact by properly modeling the prior distribution.
Since we are interested in analytical results in this section,b we cannot mix any prior with any likelihood, as the product might not be of known form.
This leads us to the class of \emph{conjugate priors}.

\begin{definition}{(Conjugate prior).}
Let the likelihood $p(y \mid \theta)$ be given and assume that the prior distribution $p(\theta)$ is a member of some family $\mathcal{F}$ of probability distributions.
We say that $p(\theta)$ is a \emph{conjugate prior} if the posterior $p(\theta \mid y)$ is also a member of $\mathcal{F}$.
\end{definition}

Conjugate priors were of particular importance in the early stages of Bayesian statistics since these give the practitioner certainty that the posterior follows a distribution which is known and computable. Moreover, nowadays we still see conjugate priors in use as they allow for a full or partial analytical derivation, which increases the accuracy of results or shortens the runtime of programs. For more complex models however conjugate priors can become too restrictive. We discuss solutions to this problem in the next section.

Consider again the likelihood but written to demonstrate its dependence on $\mu$ and $\sigma^2$
\begin{align}
  p(y \mid \mu, \sigma^2) \propto (\sigma^2)^{n/2} \EXP{-\frac{1}{2\sigma^2} n\left[ (\mu - \bar{y})^2 + (\bar{y^2} - \bar{y})^2 \right]} \,.
  \label{eq:likelihood2}
\end{align}
We want to construct a two dimensional prior for $(\mu, \sigma^2)$.
A theme to which we will be coming back is that modeling higher dimensional parameters by modeling many lower dimensional (sub)parameters using conditioning is often easier than modeling the complete distribution.
Here we utilize the equality $p(\mu, \sigma^2) = p(\mu \mid \sigma^2) p(\sigma^2)$.
By looking at the likelihood (equation \ref{eq:likelihood2}) we note that in order to \emph{not} change the inherent structual dependence on the parameters, $\mu \mid \sigma^2$ has to be distributed according to $\normal{\mu_0, \sigma^2 / \kappa_0}$ with so called \emph{hyperparameters} $\mu_0$ and $\kappa_0 > 0$.
Similary, we note that an informative prior for $\sigma^2$ has to respect the structure in which $\sigma^2$ appears in the likelihood.
We achieve this when $\sigma^2 \sim \scaledInvChi{\nu_0, \sigma_0^2}$ with hyperparameters $\nu_0$ and $\sigma_0^2 > 0$.
Following \citet{gelmanbda04} we write $(\mu, \sigma^2) \sim \NormalscaledInvChi{\mu_0, \sigma_0^2 / \kappa_0; \nu_0, \sigma_0^2}$ with corresponding density function
\begin{align}
  p(\mu, \sigma^2) = p(\mu \mid \sigma^2) p(\sigma^2) \propto (\sigma^2)^{\frac{3 + \nu_0}{2}} \EXP{-\frac{1}{2\sigma^2} \left[\nu_0 \sigma_0^2 + \kappa_0(\mu_0 - \mu)^2 \right]} \,.
\end{align}

Multiplying the likelihood with our constructed prior we get the joint posterior (up to an integration constant)
\begin{align}
  p(\mu, \sigma^2 \mid y) \propto& (\sigma^2)^{-\frac{3 + n + \nu_0}{2}} \times\\
  & \times \EXP{-\frac{1}{2 \sigma^2} \left[ (\mu - \bar{y})^2 + (\bar{y^2} - \bar{y})^2 + \nu_0\sigma_0^2 + \kappa_0(\mu - \mu_0)^2 \right]} \,.
  \label{eq:conjugate_posterior}
\end{align}

\begin{proposition}
  The posterior distribution of $(\mu, \sigma^2) \mid y$, as given by the conditional density in equation \ref{eq:conjugate_posterior}, is $\NormalscaledInvChi{\mu_n, \sigma_n^2/\kappa_n; \nu_n, \sigma_n^2}$ distributed, where
  \begin{align*}
    \nu_n &= \nu_0 + n \,; \quad \kappa_n = \kappa_0 + n \,; \quad \mu_n =\frac{\kappa_0}{\kappa_0 + n}\mu_0 + \frac{n}{\kappa_0 + n}\bar{y} \,,\\
    \sigma_n^2 &= \left[\nu_0 \sigma_0^2 + (n-1)s^2 + \frac{\kappa_0 n}{\kappa_0 + n} (\bar{y} - \mu_0)^2\right] /\nu_n \,.
  \end{align*}
  \label{prop:posterior_conjugate}
\end{proposition}
\begin{proof}
  See appendix.
\end{proof}

Since the prior and the posterior are both normal scaled inverse $\chi^2$ distributed, we can speak of a conjugate prior.
Using the intermediate finding from proposition \ref{prop:posterior_conjugate} we can derive the main result of this section.

\begin{proposition}
  The marginal posterior distributions are given by
  \begin{align*}
    \mu \mid y &\sim t_{\nu_n}(\mu_n, \sigma_n^2 / \kappa_n)\\
    \sigma^2 \mid y &\sim \scaledInvChi{\nu_n, \sigma_n^2}\,,
  \end{align*}
  where $\nu_n, \sigma_n^2, \mu_n$ and $\kappa_n$ are as in proposition \ref{prop:posterior_conjugate}.
\end{proposition}
\begin{proof}
  See appendix.
\end{proof}

\begin{table}[ht]
\centering
\def\arraystretch{1.3}
{\small
 \begin{tabular}{c | c c c c c}
 Parameter & ML Estimate & ML Variance & MAP & Posterior Mean & Posterior Variance\\
 \hline
 $\mu$ & $\bar{y}$ & $\sigma^2 / n$ & $\mu_n$ & $\mu_n$ & $\sigma_n^2 / \kappa_n$\\
 $\sigma^2$ & $\frac{n-1}{n}s^2$ & $2 \sigma^4 / n$ & $\frac{\nu_n}{\nu_n + 2} \sigma_n^2$ & $\frac{\nu_n}{\nu_n - 2} \sigma_n^2$ & $\frac{2 \nu_n^2}{(\nu_n - 2)^2(\nu_n - 4)} \sigma_n^4$
 \end{tabular}
 }
\caption{{\small Comparison of Bayesian estimates using a conjugate prios and ML estimates.}}
\label{table:comp_conjugate_bay_ml}
\end{table}


Let us first consider the parameter $\mu$.
We note that the posterior mean (and MAP) is given by $\mu_n =\frac{\kappa_0}{\kappa_0 + n}\mu_0 + \frac{n}{\kappa_0 + n}\bar{y}$, which forms a convex combination of the prior $\mu_0$ and the sample average $\bar{y}$, with weights given by the sample size and $\kappa_0$.
For any fixed $n$ this pulls our estimate of the posterior mean away from $\bar{y}$ and closer to $\mu_0$ (and vice versa).
Further, we can use the hyperparameter $\kappa_0$ to express our uncertainty in $\mu_0$ (or $\bar{y}$ for that matter).
Rewritting the posterior variance using the \emph{Laundau notation} we get $\sigma_n^2 / \kappa_n = \frac{n-1}{\nu_n \kappa_n} s^2 + \mathcal{O}(\frac{1}{\nu_n\kappa_n}) = \frac{n}{(\nu_0 + n)(\kappa_0 + n)} s^2 + \mathcal{O}(1/n^2)$.
As the sample size $n$ grows the information contained in the likelihood should dominate the prior.
We observe this phenomena as the approximate asymptotic behavior of the posterior resembles that of the maximum likelihood estimator.
First, as $n$ tends to infinity $\nu_n = \nu_0 + n$ tends to infinity and the t distribution becomes indistinguishable from a normal.
Second, as $n$ grows the posterior mean is dominated by $\bar{y}$. And at last, for large $n$ the posterior variance is accurately approximated by $\sigma^2 / n$.
We refrain from an analogous analysis for $\sigma^2$ and only note that similar results hold, as can be seen in table \ref{table:comp_conjugate_bay_ml}.

What is gained from using an informative prior here?
Using the conjugate prior from above we have four hyperparameters at hand to model our prior knowledge about the parameters.
These can be used to represent very detailed to very vague information.
In any case, we were able to see that as we collect more and more data, the likelihood dominates our results.
A clear advantage of an analytical derivation is that we know exactly how the prior influences the posterior.
However, we have also seen that even for this \emph{very} simple model, the derivation is far from obvious.
As we consider more complex models using more parameters we have to make more restrictive assumptions on the way we model our prior information, if an analytical analysis is even possible.
For this reason among others, in the next section we present a method which trades off the clarity of an analytical result for the generality of being able to combine near arbitrary priors with complex, possibly high-dimensional likelihoods.


\newpage
\subsection{Sampling From The Posterior}
In this section we consider approaches that allow us to characterize the posterior distribution in complex settings using sampling methods.

For the rest of this section let us assume that we observe data $z \in \mathbb{Z}$ and can compute the likelihood $p(z \mid \theta)$ and prior $p(\theta)$ for $\theta \in \Theta$.
As before, our goal lies in analyzing the posterior distribution given by $p(\theta \mid z) \propto p(z \mid \theta) p(\theta)$.
Unlike before however, we now consider cases where the posterior is highly complex or even non-existent in analytical form, which happens for example when the likelihood contribution stems from an algorithmic computational model.

Say we are somehow able to draw independent samples $\theta_1, \mydots, \theta_n$ from $p(\theta \mid z)$.
By independence we get the well known result $\frac{1}{\sqrt{n}} \sum_i h(\theta_i) \overset{d}{\longrightarrow} \mathcal{N} \left( \Exp{h(\theta) \mid z}, \var{h(\theta) \mid z} \right)$, under mild conditions on $h$ and $p(\theta \mid z)$.
As we are able to formulate many quantities of interest using expectations ---probability statements can be written as expectations--- and as we can approximate percentiles from a (large) sample, we should be able to adequately summarize the posterior distribution if we are able to draw (independent) samples from it.

In the subsequent paragraphs we will discuss efficient methods to sample from the posterior, even if we cannot compute the integration constant $\int p(z \mid \theta) p(\theta) \mathrm{d}\theta$.
We will see that these methods do \emph{not} produce independent but autocorrelated samples.
With this in mind, we follow the creational process of these methods and first state the assumptions which have to be satisfied by the sampling process in order to yield good properties as for example a central limit theorem for dependent samples.
Then we present the \emph{Metropolis-Hastings algorithm}, which creates samples that fulfill the above criteria.
At last we talk about cases in which the Metropolis-Hastings algorithm fails and what can be done instead.

\paragraph{Markov Chain Monte Carlo.}
Say we are able to construct a \emph{Markov chain} with unique invariant distribution equal to the posterior distribution we want to sample from.
Given we know the transition kernel, Markov chains are very easy to simulate.
Hence, we could start a chain, let it run \emph{long enough} and at some point consider all subsequent realizations as draws from the posterior; this is the core idea of MCMC ---we defer questions regarding the creation of transition kernels which result in specific invariant distributions until next paragraph.
In practice we never know for sure when a chain is run \emph{long enough}.
In part 3 we present some measures that help during the application.
Under mild conditions we can get something similar to a law of large numbers for Markov chains (see e.g. \citet{roberts2004}, Fact 5).
This tells us that if we run the chain forever, our average will eventually converge to the number we seek.
However, forever is a very long time.
That is why we focus on assumptions which admit a central limit theorem with the usual $\sqrt{n}$ convergence rate, as it allows us to make more rigorous statements about our confidence in the whereabouts of the estimator for large but finite samples.

\begin{remark}
  As is often the case, there are many different sets of assumptions that allow for a CLT.
  The following theorem presents a particular set of assumptions which will be seen to have favorable properties when also considering the creation process.
  We remark that we will \emph{not} formally introduce all concepts and will provide only a heuristic explaination of the assumptions.
  This is due to the fact that Markov chain theory on general state spaces requires a good understanding of measure theory which we do not want to assume as a prerequisite.
  The interested reader is refered to \citet{roberts2004} for a survey on recent advances with application to MCMC and to \citet{meynandtweedie09} for a comprehensive treatment of Markov chain theory.
\end{remark}

\begin{theorem}{(A Central Limit Theorem for Markov Chains).}\label{thm:mcclt}
  Let $\{X_n \}$ be a (discrete time) Markov chain and $\pi$ a probability distribution on the same space.
  Consider some measurable function $h$ with $\Expwrt{\pi}{h^2} < \infty$.
  Define $\sigma^2(h) := \varwrt{\pi}{h}\tau := \varwrt{\pi}{h} \sum_{k \in \mathrm{Z}} \text{Corr}\left(h(X_0), h(X_k)\right)$.\footnote{In the original paper by \citet{roberts2004} the statement of this theorem differs in that they write $\tau = \sum_{k \in \mathrm{Z}} \text{Corr}\left(X_0, X_k\right)$.
  We believe that this is an error as \citet{haggstrom2007} state in their comparison of different ways of writing the asymptotic variance that $\sigma^2(h) = \sum_{k \in \mathrm{Z}} \Cov{h(X_0), h(X_k)}$.
  Now if we use that $X_0 \sim \pi$ we get $\sigma^2(h) = \sum_{k \in \mathrm{Z}} \Cov{h(X_0), h(X_k)}= \var{h(X_0)} + \sum_{k \neq 0} \Cov{h(X_0), h(X_k)} = \varwrt{\pi}{h}(1 + \sum_{k \neq 0} \Cov{h(X_0), h(X_k)}) / \var{h(X_0)} =\varwrt{\pi}{h}(1 + \sum_{k \neq 0} \text{Corr}{(h(X_0), h(X_k))}) = \varwrt{\pi}{h} \sum_{k\in\mathrm{Z}} \text{Corr}{(h(X_0), h(X_k))}$.}
  Assume the Markov chain is $\phi$-irreducible, aperiodic, reversible with respect to $\pi$ and that $\sigma^2(h) < \infty$. Then $\pi$ is the stationary for the chain and
  \begin{align}
    \sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n h(X_i) - \Expwrt{\pi}{h} \right) \overset{d}{\longrightarrow} \normal{0, \sigma^2(h)} \,.
  \end{align}
\end{theorem}
\begin{proof}
  See \citet{roberts2004} Proposition 1 for the first claim and Theorem 27 for the second; see \citet{kipnis1986} for a complete proof of the second claim.
\end{proof}

\noindent
We end this paragraph by discussing the assumptions of Theorem \ref{thm:mcclt} on an intuitive level.

\textbf{\emph{$\phi$-irreducibility}} assumes that we can find a measure $\phi$ so that no matter where the chain starts, we eventually reach every region of the state space which has positive measure with respect to $\phi$.
In the next paragraph we will see that we can make sure that this condition is satisfied by construction such that the chain is $\pi$-irreducible.

\textbf{\emph{Aperiodicity}} assumes that we cannot find disjoint regions on which the chain jumps from one region to another in a cyclical predictable fashion.
It seems intuitive that such a behavior will prevent the chain of actually converging to its stationary distribution.

\textbf{\emph{Reversibility with respect to $\pi$}} is a technical assumption which is best explained by its implications.
In particular, it implies that the Markov chain has $\pi$ as its stationary distribution (which is unique by the other assumption).
Again, later we will see that we can construct a chain which fulfills this condition with $\pi$ being equal to the posterior distribution.

At last, how do we interpret the \textbf{\emph{finite variance}} assumption?
As we assume square integrability of $h$ we get that $\sigma^2(h)$ is finite if and only if the integrated correlation time $\tau$ is finite.
This happens if $\text{Corr}(h(X_0), h(X_k))$ goes fast enough to zero.
We then get the usual large sample variance approximation of the unnormalized sample mean: $\sigma^2(h) / n = \varwrt{\pi}{h} / ({}^n/{}_\tau)$.
In this sense we might say that ${}^n/{}_\tau$ denotes the \emph{effective sample size}, which corrects for the fact that we are not drawing independent samples and therefore need more samples to yield the same amount of information as in the independent case.

\paragraph{Metropolis-Hastings Algorithm.}
From the above we know that given a Markov chain with invariant distribution equal to the posterior distribution $p(\theta \mid z)$, we can treat the realizations of the chain as samples from the posterior, under some regularity conditions.
Here we consider one method which implicitly defines such a chain, namely the Metropolis-Hastings algorithm (\citet{Metropolis1953}, \citet{hastings70}).
For other approaches and more involved algorithms see for example \citet{roberts2004} or \citet{liang10}.

\begin{algorithm}
\caption{Metropolis-Hastings}\label{alg:metropolis-hastings}
\begin{algorithmic}[1]
  \Require $(\pi, q, T) =$ (target density, proposal density, number of samples to draw)
\State initialize $x_0$ with an arbitrary point from the support of $q$
\For{$t = 0,\mydots,T-1$}
  \State sample a candidate: $y \sim q(\cdot \mid x_t)$
  \State compute the acceptance probability: $\mathcal{A} \gets \min\left\{ \ddfrac{\pi(y)}{\pi(x_t)} \ddfrac{q(y \mid x_t)}{q(x_t \mid y)}, 1\right\}$
  \State update the chain: $x_{t+1} \gets \begin{cases} y &\mbox{,with probability } \mathcal{A}\\ x_t &\mbox{,with remaining probability} \end{cases}$
\EndFor{}
\State \textbf{return} $\{x_t : t = 1,\mydots,T\}$
\end{algorithmic}
\end{algorithm}

\noindent
Algorithm \ref{alg:metropolis-hastings} displays the Metropolis-Hastings algorithm.
Line 4 shows why we can use this algorithm with the unnormalized posterior, as the integration constant cancels out in the first fraction.
For different settings different proposal distributions are appropriate.
A common choice are so called \emph{random walk} proposals which add some random number to the current position of the chain; for example a gaussian random walk proposal is given by $q(\cdot \mid x_t) = \normal{\cdot \mid x_t, \sigma^2}$ or equivalently stated $y = x_t + \normal{0, \sigma^2}$.
If the resulting chain has an unique invariant distribution we only know that after some time the chain starts behaving accordingly.
Therefore in practice we choose $T = B + T^*$ and drop the first $B$ samples, where $B$, the so called \emph{burn-in} samples, is large and $T^*$ represents the actual size of samples we want to draw.
See \citet{sherlock2010} for a recent survey on random walk proposals.

The simplicity of the algorithm is remarkable, but the main question of concern is if the resulting Markov chain inherits favorable properties.
And indeed this is the case.
The algorithm creates by construction Markov chains which are reversible with respect to $\pi$ and aperiodic, and if additionally the proposal density is positiv and continuous and $\pi$ is finite then the chain is $\pi$-irreducible; see for example \citet{roberts2004}.

The ability to sample draws in complex settings using the Metropolis-Hastings algorithm (and other Markov chain Monte Carlo methods for that matter) made Bayesian statistics applicable for real problems.
Still, \citet{Au2001EstimationOS} show that the classical Metropolis-Hastings algorithm is highly dependent on the proposal density and fails in higher dimensions; \citet{zuev08} provide a geometric intution.
The following paragraph illustrates one of the problems of working in higher dimensions.

\paragraph{Volume in Higher Dimensions}
Classical MCMC methods can have too slow convergence rates; In higher dimensions this might be due to probability mass being distributed very far from where it is expected (\citet{betancourt2017convergence}).
In this paragraph we motivate this phenomenon and in the following we present methods which utilize it.

Let $B_d$ denote the unit ball in $\reals^d$ and define $C_d$ as the smallest cube containing $B_d$.
We consider two questions.
First, how does the ratio $\nicefrac{\vol{B_d}}{\vol{C_d}}$ change as $d$ increases.
And second, how does the ratio of probability mass distributed by a standard gaussian on these regions change as $d$ increases.
Since closed form expressions of volumina of geometrical objects exist the first questions needs little work.
Similary we can easily compute $P(X \in C_d) = \left[\Phi(1) - \Phi(-1)\right]^d$, where $\Phi$ denotes the one-dimensional gaussian cumulative distribution function.
However, to compute $P(X \in B_d)$ we need to integrate over the unit ball with respect to a gaussian distribution, which is non-trivial.
For this reason we decide to report an upper bound, as this is sufficient for our motivation.
In particular we compute $\overline{P(X \in B_d)} := \sup_{\bm{x} \in B_d} \phi(\bm{x}) \vol{B_d} = \sup_{\bm{x} \in B_d} \phi(\bm{x}) \int_{B_d}1 \mathrm{d}\bm{x} \geqslant \int_{B_d} \phi(\bm{x}) \mathrm{d}\bm{x} = P(X \in B_d)$.
The results of these computations are depicted in table \ref{tab:vol_high_dim}.
We note that both ratios tend to zero very fast as $d$ increases.
With this phenomenon in mind one has to be cautious when working in high-dimensional spaces, since the regions of interest, that is the regions containing non-negligible probability mass, might not be located where our low-dimensional intution says.
This idea is formalized by the \emph{Gaussian Annulus Theorem} (\citet{blum2017foundations}; theorem 2.9) which states, inter alia, that most probability mass lies within an annulus centered at the origin with an average distance to the origin of $\sqrt{d}$.

\begin{table}[ht]
\def\arraystretch{1.3}
\centering
 \begin{tabular}{c | c c c c c c c}
 $d$ & 1 & 2 & 3 & 5 & 7 & 10 & 15\\
 \hline
 ${\vol{B_d}} / {\vol{C_d}}$ & 1.00000 & 0.78540 & 0.52360 & 0.16449 & 0.03691 & 0.00249 & 0.00001\\
 ${\overline{P(X \in B_d)}} / {P(X \in C_d)}$ & 1.16874 & 1.07281 & 0.83589 & 0.35870 & 0.10995 & 0.01184 & 0.00012
 \end{tabular}
 \caption{Comparison of volume ratio of unit ball and cube, and probability ratio of gaussian falling in unit ball and cube for varying dimension $d$. Numbers are rounded to five decimal places.}
\label{tab:vol_high_dim}
\end{table}

We have seen some unintuitive behavior in higher dimensions which might explain why regular methods do not work or only work very slowly.
The next paragraph presents one method which utilizes this behavior to efficiently produce samples.

\paragraph{Hamiltonian Monte Carlo.}
We conclude our digression on Bayesian thinking by presenting \emph{Hamiltonian Monte Carlo} (HMC), an innovative Markov chain Monte Carlo method from the statistical physics literature which works in higher dimensions  \citet{duane87}.
There are of course multiple MCMC algorithms which work in higher dimensions with many more being actively developed.
Here we focus on HMC as it is the main algorithm used in the probabilistic programming language STAN (\citet{standev2018stancore}), which we will be using in our Monte Carlo study and application part.

We note that it is impossible to provide a rigorous introduction to HMC here, which is why we will focus on the general intution and refer to a series of papers by Michael Betancourt and several coauthors on, the geometric foundations of HMC (\citet{betancourt2014geometric}); geometric ergodicity of HMC (\citet{livingstone2016geometric}) and HMC for hiearchical models (\citet{betancourt2013hamiltonian}).\footnote{Besides doing theoretical research on HMC, Michael Betancourt worked for STAN on integrating the HMC algorithm and runs an educational blog where he presents his research using modern tools, see \url{https://betanalpha.github.io/}.}

One reason why ordinary MCMC methods might converge only very slowly in higher dimensions is that the proposal distribution used in the Metropolis-Hastings algorithm does not properly capture the geometry of the high-dimensional space which leads to many rejected proposals and therefore an inefficient exploration of the parameter space.
The main idea of HMC is to extend the parameter space by constructing a specific vector field on it, which moves the chain from one proposed point to another in a way so that we consider points that lie in regions with high probability mass and we regulary jump to faw away points as to explore the space as quickly as possible.
But how do we construct this vector field?
Note that when considering differentiable posterior densities the gradient defines a vector field.
However, this vector field points to the modes of the posterior and as we saw in the last paragraph, in higher dimensions we will find little to no probability mass near the modes.
This is where Hamiltonian mechanics comes into play by providing a set of equations (Hamilton's equations) that describe the time-evolution of the interplay of kinetic and potential energy of a system.
What this means for our case is best explained by imagining the mode as the center of gravity, with gravity pulling harder as we get closer to the mode ---this can be thought of as the gradient vector field.
But as most probability mass is spread around an annulus around the mode we do not want to move closer to the center of gravity, we want to move around an orbit around the mode.
The distance of the orbit to the mode and exact shape depent of course on the dimensionality and posterior distribution of the problem.
Hamilton's equations provide us with a way to construct a vector field so that moving along the field drifts the Markov chain into this orbit.
Once reached the chain explores the relevant space quickly.

Why is the above important?
Bayesian statistics and its application to real world problems has gained immense popularity with the invention of Markov chain Monte Carlo methods.
It's naive application to general complex high-dimensional problems is not computationally feasible however.
These problems are under active development and new approaches on the algorithmic and theoretical side, as the one presented above, prove fruitfull.
