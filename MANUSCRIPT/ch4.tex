\section{Application}

In this section we discuss the applications of the hierarchical modelling approach in the form of a literature review. We then use real world data to give a practical illustration of the work we have developed in the previous sections of our paper and compare that to the frequentist approach to analyzing multilevel data with the aim of showcasing what differences/similarities there are between the two methods. We then depart form the comparative analysis to look deeper into the Bayesian approach and conduct robustness checks that vary certain aspects (e.g. priors) of it so that we can see how the estimation results change in response to that. The baseline here would be the estimation results from the comparison between Bayesian and frequentist approaches. We close the section off by a discussion of the limitations and challenges encountered in this section.


\subsection{Literature review on the application of Hierarchical Models}

Bayesian Hierarchical models or multilevel models are a suitable approach to consider social contexts as well as individual respondents or subjects. It becomes attractive to consider hierarchical models in place of the common (or popularized) frequentist approach as soon as there is a need to relax the independence of residuals assumption as a result of similarities in the characteristics of a group of respondents or when the researcher seeks to disentangle variability at various levels of the data. These models have been used in various applications throughout fields in economic research. In education research, \cite{buric2020teacher} use these models to examine the relationship between teacher self-efficacy (TSE), instructional quality (i.e., classroom management, cognitive activation, and supportive climate) and student motivational beliefs (i.e., self efficacy and intrinsic motivation) by using responses from both teachers and students and implementing a sophisticated doubly latent multilevel structural equation modelling approach. The results reflect the necessity to disentangle variability at various levels of the data as the researchers find that, at class level, TSE was positively related to the three dimensions of instructional quality but not to students' motivational beliefs. They also find, as expected, that instructional quality was positively related to studentsâ€™ motivational beliefs.

In family economics, \cite{lamnisos2019demographic} project the total fertility rate and life expectancy at birth probabilistically using Bayesian hierarchical models and United Nations population data for Greece from the period of 1950 to 2015. These are then converted to age-specific mortality rates and combined with a cohort component projection model. This yields probabilistic projections of total population by sex and age groups, total fertility rate (TFR), female and male life expectancies at birth and potential support ratio PSR (persons aged 20-64 per person 65+) by the year 2100. If the forecasts prove in future to be accurate, these models can provide a powerful tool for policy formulation. In agricultural research, \cite{ ramsey2019saying} develop two econometric models: a hierarchical Bayesian linear model and a hierarchical Bayesian Poisson model to predict exit rates across the towns and prefectures of Japan resulting from off-farm employment opportunities. Off-farm employment opportunities are thought to have an effect on farm exit rates, though evidence on the sign of this effect has been mixed. Examining this issue in the context of Japanese agriculture, the researchers find that farm exits are related to off-farm income as a share of household income, and more specifically to the nature of off-farm work.  

In development economics, \cite{ meager2019understanding} jointly estimates the average effect and the heterogeneity in effects across seven studies using Bayesian hierarchical models to answer questions about external validity that impede consensus on the results from randomized evaluations of microcredit. The researcher finds reasonable external validity: true heterogeneity in effects is moderate, and approximately 60 percent of observed heterogeneity is sampling variation. These paper has the potential to revolutionize the field of development economics as the researcher provides a method to establish external validity using multiple studies form different countries. In health research, \cite{ rashid2019socio} uses a more advanced application of Bayesian Hierarchical Models in health research. The authors aim to identify the spatial distribution of the three types of misconception factors of HIV transmission (i.e. transmitted by mosquito bite, supernatural means and sharing food with HIV positive person). This study also provides the core socio-economic factors to stop the misconception about HIV/ Aids transmission and helped in reducing its epidemic in Pakistan. Spatial and Non-Spatial Bayesian Hierarchical model were applied to the data and results from them revealed that the Conditional Autoregressive Bayesian Hierarchical Models (Spatial Model) were more appropriate. The results showed that Conditional Autoregressive Bayesian Hierarchical models at level 2 are best fit to the data.

It is evident that Bayesian Hierarchical Models can be useful in the area of microeconomic research. There also have been applications to the Macroeconomics field. It is evident that Bayesian Hierarchical Models can be useful in the area of microeconomic research. There also have been applications to the Macroeconomic research field. \cite{ koop2010bayesian} notes that bayesian methods have become increasingly popular as a way of overcoming over-parameterization problems. In this paper, the authors discuss vector autoregressive multivariate time
series models (VARs), factor augmented VARs and time-varying parameter extensions and show how Bayesian inference proceeds. 

We now demonstrate below, an application to real world data of a comparison between the likelihood (frequentist)  and the bayesian inference approaches. For each of these approaches, we will fit a basic varying intercept and slope multilevel linear model with one predictor. We will use the \textit{lmer} function in the \textit{lme4} package for R to determine maximum likelihood estimates of the parameters in linear mixed-effects models. We then the use \textit{rstanarm} package (also in R) to implement a fully Bayesian approach.




\subsection{Frequentist and Bayesian Approaches in Practice: Application to Education Data}

A common feature of data structures in education is that units of analysis (e.g., students) are nested in higher organizational clusters (e.g. schools). This kind of structure induces dependence among the responses observed for units within the same cluster. Students in the same school tend to be more alike in their academic and attitudinal characteristics than students chosen at random from the population at large. Multilevel models are designed to model such within-cluster dependence. As mentioned earlier, one advantage of multilevel models is that it allows us to disentangle variability between levels and in our data example,  multilevel models recognize the existence of data clustering (at two or more levels) by allowing for residual components at each level in the hierarchy. For example, a two-level model that allows for grouping of student outcomes within schools would include residuals at both the student and school level. The residual variance is thus partitioned into a between-school component (the variance of the school-level residuals) and a within-school component (the variance of the student-level residuals). 


\subsubsection{The data}
We will be analyzing the Gcsemv dataset from \cite{rasbash2000user}. The data include the General Certificate of Secondary Education (GCSE) exam scores of 1,905 students from 73 schools in England on a science subject. The Gcsemv dataset consists of the following 5 variables:
\begin{itemize}
	\item \textit{school}: school identifier
	\item \textit{student}: student identifier
	\item \textit{gender}: gender of a student (M: Male, F: Female)
	\item \textit{written}: total score on written paper
	\item \textit{course}: total score on coursework paper
\end{itemize}
Two components of the exam were recorded as outcome variables: written paper and course work. In this application, only the total score on the coursework paper (course) will be analyzed. In our example, the model summarizes the difference in average test scores between male and female students.

\subsubsection{Likelihood inference approach}
In this sub-section, we fit the a basic varying intercept and slope multilevel linear model with one predictor using the \textit{lmer()} functions. Functions such as lmer() are based on a combination of maximum likelihood (ML) estimation of the model parameters, and empirical Bayes (EB) predictions of the varying intercepts and/or slopes resulting in the Best Linear Unbiased Predictions (BLUPs) of the model parameters. We use these functions so that our parameter estimates from both the ML and the Bayesian framework are comparable.

\subsubsection*{Model 1: Varying intercept and slope model with a single predictor}
In this model, we allow both the intercept and the slope to vary randomly across schools using the following model: expressed\footnote{Equivalently, the model can be expressed as: $$y_{ij}\sim N(\alpha_{j}+\beta_{j}x_{ij} , \sigma_y ^2 ),$$
$$
\left( \begin{matrix} \alpha _{ j } \\ \beta _{ j } \end{matrix} \right) \sim N\left( \left( \begin{matrix} { \mu  }_{ \alpha  } \\ { \mu  }_{ \beta  } \end{matrix} \right) , \left( \begin{matrix} { \sigma  }_{ \alpha  }^{ 2 } & \rho { \sigma  }_{ \alpha  }{ \sigma  }_{ \beta  } \\ \rho { \sigma  }_{ \alpha  }{ \sigma  }_{ \beta  } & { \sigma  }_{ \beta  }^{ 2 } \end{matrix} \right)  \right).$$}:

\begin{align}
	y_{ij} = \alpha_j + \beta_j x_{ij} +\epsilon_{ij},
\end{align}

\begin{align}
	\alpha_j = \mu_\alpha + u_j,
\end{align}

\begin{align}
	\beta_j = \mu_\beta + v_j,
\end{align}

or in a reduced form as
 
\begin{align}
	y_{ij} = \mu_\alpha + \mu_\beta x_{ij} + u_j + v_j x_{ij} + \epsilon_{ij}
\end{align}

where 
\begin{align}
\epsilon_{ij} \sim N(0, \sigma_{y}^{2}) 
\end{align}

and 
\begin{align}
\left( \begin{matrix} u_j \\ v_j \end{matrix} \right) \sim N\left( \left( \begin{matrix} 0 \\ 0 \end{matrix} \right) ,\left( \begin{matrix} { \sigma  }_{ \alpha  }^{ 2 } & \rho { \sigma  }_{ \alpha  }{ \sigma  }_{ \beta  } \\ \rho { \sigma  }_{ \alpha  }{ \sigma  }_{ \beta  } & { \sigma  }_{ \beta  }^{ 2 } \end{matrix} \right)  \right).
\end{align}
Note that now we have variation in the $\alpha_{j}$'s and the $\beta_{j}$'s, and also a correlation parameter $\rho$ between $\alpha_j$ and $\beta_j$. This model can be fit using \textit{lmer()}:

\textcolor{red}{input results here}

In this model, the residual within-school standard deviation is estimated as $\hat{\sigma}_{y}=13.03$. The estimated standard deviations of the school intercepts and the school slopes are $\hat{\sigma}_{\alpha}= 10.15$ and $\hat{\sigma}_{\beta}=6.92$ respectively. The estimated correlation between varying intercepts and slopes is $\hat{\rho} = -0.52$. 

\textcolor{red}{get facts for results straight here}
The average regression line across schools is thus estimated as $\hat{\mu}_{ij} = \textbf{69.73} + \textbf{6.74} x_{ij}$, with $\sigma_\alpha$ and $\sigma_y$ estimated as $8.76$ and $\textbf{13.41}$ respectively.  Treating these estimates of $\mu_\alpha$, $\beta$, $\sigma^2_{y}$, and $\sigma^2_{\alpha}$ as the true parameter values, we can then obtain the Best Linear Unbiased Predictions (BLUPs) for the school-level errors $\hat{u}_j = \hat{\alpha}_{j} - \hat{\mu}_{\alpha}$. 

The BLUPs are equivalent to the so-called Empirical Bayes (EB) prediction, which is the mean of the posterior distribution of $u_{j}$ given all the estimated parameters, as well as the random variables $y_{ij}$ and $x_{ij}$ for the cluster.  These predictions are called "Bayes" because they make use of the pre-specified prior distribution \footnote{We elaborate more on prior distributions in Section the full Bayesian approach section} $\alpha_j \sim N(\mu_\alpha, \sigma^2_\alpha)$, and by extension $u_j \sim N(0, \sigma^2_\alpha)$, and called "Empirical" because the parameters of this prior, $\mu_\alpha$ and $\sigma^2_{\alpha}$, in addition to $\beta$ and $\sigma^2_{y}$, are estimated from the data.

Compared to the Maximum Likelihood (ML) approach of predicting values for $u_j$ by using only the estimated parameters and data from cluster $j$, the EB approach additionally consider the prior distribution of $u_{j}$, and produces predicted values closer to $0$ (a phenomenon described as *shrinkage* or *partial pooling*).  To see why this phenomenon is called *shrinkage*, we usually express the estimates for $u_j$ obtained from EB prediction as $\hat{u}_j^{\text{EB}} = \hat{R}_j\hat{u}_j^{\text{ML}}$ where $\hat{u}_j^{\text{ML}}$ are the ML estimates, and $\hat{R}_j = \frac{\sigma_\alpha^2}{\sigma_\alpha^2 + \frac{\sigma_y^2}{n_j}}$ is the so-called Shrinkage factor.

By using the \textit{ranef} function, we can also show how much the intercept is shifted up or down in particular schools. For example, in the first school in the dataset, the estimated intercept is about \textbf{10.17} lower than average, so that the school-specific regression line is $69.73 - 10.17 + 6.74 x_{ij}$.


\subsubsection{Full Bayesian Inference Approach}
As previously mentioned, functions such as \textit{lmer()} are based on a combination of maximum likelihood (ML) estimation of the model parameters, and empirical Bayes (EB) predictions of the varying intercepts and/or slopes. However, in some instances, when the number of groups is small or when the model contains many varying coefficients or non-nested components, the ML approach may not work as well in part because there may not be enough information to estimate variance parameters precisely. In such cases, a fully Bayesian approach provides reasonable inferences with the added benefit of accounting for all the uncertainty in the parameter estimates when predicting the varying intercepts and slopes, and their associated uncertainty. This is one of the reasons why a fully Bayesian estimation is particularly interesting. Other reasons are discussed in section \ref{section:Deeper}. We now demonstate below, how to fit Models 1 from above in a fully bayesian framework using the \textit{rstammarm} package. \textit{Rstanarm} is a wrapper for the \textit{rstan} package that enables the most common applied regression models to be estimated using Markov Chain Monte Carlo (MCMC) but still be specified using customary R modeling syntax.

\subsubsection*{Model 1: Varying intercept and slope model with a single predictor}

We can implement a fully Bayesian estimation for multilevel models with only minimal changes to our existing code with \textit{lmer()} from the maximum likelihood application in the previous section. 

We specify Model 1 with default prior distributions for $\mu_{\alpha}$, $\sigma_{\alpha}$, and $\sigma_{y}$ by prepending \textit{stan\textunderscore} to the \textit{lmer} call. The \textit{stan\textunderscore lmer()} function is similar in syntax to \textit{lmer()} but rather than performing maximum likelihood estimation, Bayesian estimation is performed via MCMC. As each step in the MCMC estimation approach involves random draws from the parameter space, we include a seed option to ensure that each time the code is run, \textit{stan\textunderscore lmer} outputs the same results.

\begin{align}
	\Sigma &= 
	\left(\begin{matrix} 
		\sigma_\alpha^2 & \rho\sigma_\alpha \sigma_\beta \\ 
		\rho\sigma_\alpha\sigma_\beta&\sigma_\beta^2 
	\end{matrix} \right)\\ &= 
	\sigma_y^2\left(\begin{matrix} 
		\sigma_\alpha^2/\sigma_y^2 & \rho\sigma_\alpha \sigma_\beta/\sigma_y^2 \\ 
		\rho\sigma_\alpha\sigma_\beta/\sigma_y^2 & \sigma_\beta^2/\sigma_y^2 
	\end{matrix} \right)\\ &= 
	\sigma_y^2\left(\begin{matrix} 
		\sigma_\alpha/\sigma_y & 0 \\ 
		0&\sigma_\beta/\sigma_y
	\end{matrix} \right)
	\left(\begin{matrix} 
		1 & \rho\\ 
		\rho&1 
	\end{matrix} \right)
	\left(\begin{matrix} 
		\sigma_\alpha/\sigma_y & 0 \\ 
		0&\sigma_\beta/\sigma_y 
	\end{matrix} \right)\\ 
	&= \sigma_y^2VRV.
\end{align}


The correlation matrix $R$ is 2 by 2 matrix with 1's on the diagonal and $\rho$'s on the off-diagonal. \textit{stan\_lmer} assigns it an LKJ \footnote{For more details about the LKJ distribution, see \url{http://www.psychstatistics.com/2014/12/27/d-lkj-priors/} and \url{http://mc-stan.org/users/documentation/case-studies/lotka-volterra-predator-prey.html}} prior (\cite{lewandowski2009generating}), with regularization parameter 1.  This is equivalent to assigning a uniform prior for $\rho$.  The more the regularization parameter exceeds one, the more peaked the distribution for $\rho$ to take the value 0.  

The matrix of (scaled) variances $V$ can first be collapsed into a vector of (scaled) variances, and then decomposed into three parts, $J$, $\tau^2$ and $\pi$ as shown below. 

\begin{align}
	\left(\begin{matrix} 
		\sigma_\alpha^2/\sigma_y^2 \\ 
		\sigma_\beta^2/\sigma_y^2 
	\end{matrix} \right) = 
	2\left(\frac{\sigma_\alpha^2/\sigma_y^2 + \sigma_\beta^2/\sigma_y^2}{2}\right)\left(\begin{matrix} 
		\frac{\sigma_\alpha^2/\sigma_y^2}{\sigma_\alpha^2/\sigma_y^2 + \sigma_\beta^2/\sigma_y^2} \\ 
		\frac{\sigma_\beta^2/\sigma_y^2}{\sigma_\alpha^2/\sigma_y^2 + \sigma_\beta^2/\sigma_y^2} 
	\end{matrix} \right)=
	J\tau^2 \pi.
\end{align}
 


In this formulation, $J$ is the number of varying effects in the model (here, $J=2$), $\tau^2$ can be viewed as an average (scaled) variance across the varying effects $\alpha_j$ and $\beta_j$, and $\pi$ is a non-negative vector that sums to 1 (called a Simplex/probability vector).  A symmetric Dirichlet \footnote{The Dirichlet distribution is a multivariate generalization of the beta distribution with one concentration parameter, which can be interpreted as prior counts of a multinomial random variable (the simplex vector in our context), for details, see \url{https://cran.r-project.org/web/packages/rstanarm/vignettes/glmer.html\#detail}} distribution with concentration parameter set to 1 is then used as the prior for $\pi$.  By default, this implies a jointly uniform prior over all Simplex vectors of the same size.  A scale-invariant Gamma prior with shape and scale parameters both set to 1 is then assigned for $\tau$.  This is equivalent to assigning as a prior the exponential distribution with rate parameter set to 1 which is consistent with the prior assigned to $\sigma_y$. 
This discrepancy may be partly because the ML approach does not take into account the uncertainty in $\mu_{\alpha}$ when estimating $\sigma_{\alpha}$.

\subsubsection*{Prior distributions}
Model 1 is a varying intercept and slope model with normally distributed student residuals and school-level intercepts: $y_{ij} \sim N(\alpha_{j}, \sigma_{y}^{2}),$ and $\alpha_{j}\sim N(\mu_{\alpha}, \sigma_{\alpha}^{2})$. The normal distribution for the $\alpha_{j}$'s can be thought of as a prior distribution for these varying intercepts. The parameters of this prior distribution, $\mu_{\alpha}$ and $\sigma_{\alpha}$, are estimated from the data when using maximum likelihood estimation. In full Bayesian inference, all the hyperparameters ($\mu_{\alpha}$ and $\sigma_{\alpha}$), along with the other unmodeled parameters (in this case, $\sigma_{y}$) also need a prior distribution. For this illustration, we use weakly informative priors that provide moderate regularization \footnote{Regularization can be regarded as a technique to ensure that estimates are bounded within an acceptable range of values.} and help stabilize computation.

First, before accounting for the scale of the variables, $\mu_{\alpha}$ is given normal prior distribution with mean 0 and standard deviation 10.  That is, $\mu_{\alpha} \sim N(0, 10^2)$. The standard deviation of this prior distribution, 10, is five times as large as the standard deviation of the response if it were standardized. This should be a close approximation to a noninformative prior over the range supported by the likelihood, which should give inferences similar to those obtained by maximum likelihood methods if similarly weak priors are used for the other parameters. \textit{rstanarm} scales the priors in relation to the scale of variables in the estimation proccess.

Second, the (unscaled) prior for $\sigma_{y}$ is set to an exponential distribution with rate parameter set to 1.

Third, in order to specify a prior for the variances and covariances of the varying (or "random") effects, \textit{rstanarm} will decompose this matrix into a correlation matrix of the varying effects and a function of their variances.  Since there is only one varying effect in this example, the default (unscaled) prior for $\sigma_{\alpha}$ that the package uses reduces to an exponential distribution with rate parameter set to 1.

Additionally, we are also required to specify a prior for the covariance matrix $\Sigma$ for $\alpha_j$ and $\beta_j$ in this Model.  \textit{stan\_lmer} decomposes this covariance matrix (up to a factor of $\sigma_y$) into (i) a correlation matrix $R$ and (ii) a matrix of variances $V$, and assigns them separate priors as shown below. 

\subsubsection*{Model 1 results}  
As we see in the snippet of the summary below, the point estimate of $\mu_{\alpha}$ from the bayesian estimation is $73.75$ and this corresponds to the median of the posterior draws.  This is similar to the ML estimate obtained previously $73.72$.  The point estimate for $\sigma_{\alpha}$ from the bayesian estimation is $8.92$, which is larger than the ML estimate ($8.67$). 
\textcolor{red}{input results here}

When using the bayesian estimation function, standard errors are obtained by considering the median absolute deviation (MAD) of each draw from the median of those draws.  It is well known that ML tends to underestimate uncertainties because it relies on point estimates of hyperparameters. Full Bayes, on the other hand, propagates the uncertainty in the hyperparameters throughout all levels of the model and provides more appropriate estimates of uncertainty.

\textcolor{red}{check this for wording to use above}
As we see in the snippet of the summary below, the point estimate of $\mu_{\alpha}$ from the bayesian estimation is $73.75$ and this corresponds to the median of the posterior draws.  This is similar to the ML estimate obtained previously $73.72$.  The point estimate for $\sigma_{\alpha}$ from the bayesian estimation is $8.92$, which is larger than the ML estimate ($8.67$). This discrepancy may be partly because the ML approach does not take into account the uncertainty in $\mu_{\alpha}$ when estimating $\sigma_{\alpha}$.

\subsection{Looking Deeper into the Baysian Approach}
\label{section:Deeper}


\subsubsection{Pooling in Multilevel Models}
\subsubsection{Convergence}
\subsubsection{Ranking varying intercepts by school}
\subsubsection{School to school comparisons}
\subsubsection{Robustness checks}






