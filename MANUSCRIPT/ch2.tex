\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}
\section{Hierarchical Models}
\label{sec:hierachical_modeling}
In the following we consider hierarchical models and when they are applicable.
We begin by presenting the idea of hiearchical data and modeling.
We then show the general structure of hierarchical models and end this section by introducing the most widely used subclass, hierarchical linear models.

\subsection{Hierarchical Data and Modeling}
Here we consider what makes data \emph{hierarchical} and how we can use this further component to model additional structure.

Hierarchical data is present if the data can be clustered on some level; e.g. children in schools, survey responses in different years in different states, or experiments in multiple labs.
From the examples we see that there must not be a clear \emph{hierarchy} defined on the data.
This is why some authors nowadays prefer the more general terms \emph{multi-level data} and \emph{multi-level model}, see for instance \citet{GelmanHill2007}.

We categorize hierarchical models by the number of levels they incorporate and their use of \emph{nested} or \emph{non-nested} data.
In this paper we consider two-level models for nested data and refer (again) to \citet{GelmanHill2007} and \citet{snijders1999} for a treatment of more general settings.

%In practice we can store hierarchical data very efficiently using normalized relational dataframes.
Let us continue with the first example of children in schools.
Assume we observe, say, test results, parental income and number of teachers per child per school.
Figure \ref{fig:relational_table} portrays how multi-level data in this case could be stored.
\begin{figure}[!ht]
\begin{center}
\begin{tabular}{l l l l}
child & result & income & school\\
\hline
1 & 10 & 500 & 1\\
2 & 9 & 450 & 1\\
3 & 12 & 520 & 2
\end{tabular}
\quad
\begin{tabular}{l l}
school & teacher\\
\hline
1 & 0.5\\
2 & 0.7
\end{tabular}
\end{center}
\caption{Two tables containing fictional hierarchical data. Left: Data on the \emph{child-level} (test results, parental income, school id). Right: Data on the \emph{school-level} (number of teachers per child).}
\label{fig:relational_table}
\end{figure}

\vspace{-10pt}
To introduce formal notation, assume we observe data on $i=1,\mydots,n$ individuals which are clustered among $j=1,\mydots,J$ groups.
Naturally we consider outcomes $y_i$ on the individual-level.
Following the idea of different relations for different levels, we write $x_i$ for covariates that vary by individual and $u_j$ for covariates that only vary by group.
We link the two by writing $j[i]$ for the index of the group to which individual $i$ belongs, i.e. the full set of covariates from individual $i$ is given by $(x_i, u_{j[i]})$.
How can we utilize this hierarchical structure?

The main idea of hierarchical modeling is to build (simple) models on different levels, where the outcomes are modeled on the individual-level and we connect the levels by modeling on all higher levels the parameters used in the previous level.
Importantly, the level structure is given by the hierarchical structure of the data and we use features from level $\ell$ only in the modeling process of level $\ell$.

For a general two-level case we may write a model as
\begin{align}
  y_i \mid \theta_{j[i]} &\sim p(y_i \mid x_i, \theta_{j[i]}) \,,\tag{Individual Level}\\
  \theta_j \mid \phi &\sim p(\theta_j \mid u_j, \phi) \,,\tag{Group Level}\\
  \phi &\sim p(\phi \mid \zeta) \text{ with } \zeta \text{ fixed } \,, \tag{Prior}
\end{align}
where we suppress the dependence on $x_i$ and $u_j$.
On the individual-level the outcomes $y_i$ are modeled depending on the unit-level features $x_i$ and parameters $\theta_{j[i]}$.
As in a classical Bayesian model we continue by modeling the parameters $\theta_j$, however, in contrast to section 2 we do not assume a prior distribution but we explicitly model the parameter on the group-level using the group-level features $u_j$.
From a Bayesian viewpoint this can be seen as a generalization of prior modeling.
Despite this Bayesian interpretation, the first two equations define a proper non-Bayesian hierarchical model.
We will see that in the linear case these models are well known in the frequentist world as \emph{mixed effects models} or \emph{random coefficient models}.
To produce a Bayesian model we assign a prior distribution on all (hyper)parameters that are not explicitly modeled (here $\phi$).

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}%
  [vertex/.style={circle,draw=black,fill=white, minimum size=1cm},
  node distance=2.5cm,
  >=latex,
  on grid]
  \node[vertex] (phi) {$\phi$};
  \node[vertex,right=of phi] (theta) {$\theta_{j[i]}$};
  \node[vertex,below right= 1cm and 2.5cm of theta] (y) {$y_i$};
  \node[rectangle, draw=black, minimum size=0.9cm,left= of phi] (zeta) {$\zeta$};
  \node[rectangle, draw=black, minimum size=0.9cm,above=2cm of y] (u) {$u_{j[i]}$};
  \node[rectangle, draw=black, minimum size=0.9cm, above right=1cm and 2.5cm of y] (x) {$x_i$};
  \draw[->]
    (phi) edge (theta)
    (theta) edge (y)
    (zeta) edge (phi)
    (u) edge (theta)
    (x) edge (y);
\end{tikzpicture}
\end{center}
\caption{A generic two-level Bayesian hierarchical model depicted as a directed acyclical graph modeling a single generic observation. Circled parameters denote random quantities while parameters contained in squares denote fixed quantities.}
\label{fig:general_sem}
\end{figure}

Figure \ref{fig:general_sem} illustrates the structure of modeling a generic observation $y_i$ using the general model from above.
In contrast, figure \ref{fig:group_sem} in the appendix illustrates the conditional dependence structure when modeling a generic observation $y(j)$ in group $j$.
We depict random quantities in circles and fixed quantities in squares.

How do we solve for the posterior in these models?
Due to the hierarchical nature we can derive the joint posterior as $p(\phi, \theta \mid y) \propto p(y \mid \phi, \theta) p(\phi, \theta) = p(y \mid \theta) p(\theta \mid \phi) p(\phi)$ where we use that $y$ is independent of $\phi$ conditional on $\theta$ ---this can be seen immediatly from figure \ref{fig:general_sem}.
We note that all factors on the right-hand side are known, hence, we can directly apply the methods discussed in the previous section.


%Not only do these graphical representations help with understanding the structure of the model, but we will see that when solving for the posterior they give an immediate way to check which parameters are conditionally independent.

%\subsection{Solving for the Posterior}\label{sec:hierachical_solving}
%In this section we present the complexities of deriving the (marginal) posterior in hierarchical models by considering the steps necessary to fully solve an example model.
%We will not embark on this in as much detail as in the non-hierarchical case, as this should only provide an idea on how hierarchical models can be estimated.
%
%Say we observe $i=1,\mydots,n$ individuals which fall in $j=1,\mydots,J$ groups with groupsize $n_j$.
%Assume $y_i \mid \theta_{j[i]} \sim \normal{\theta_{j[i]}, \sigma^2}$ with known variance $\sigma^2$ and that $\theta_j \overset{\text{iid}}{\sim} \normal{\mu, \tau^2}$.
%Let us write $\bar{y}$ for the sample mean of all observations and $\bar{y}_j$ for the sample mean in group $j$.
%Then we get $\bar{y}_j \mid \theta_j \sim \normal{\theta_j, \sigma_j^2}$ with $\sigma_j^2 := \sigma^2 / n_j$.
%
%We can write the joint posterior as $p(\theta, \mu, \tau^2 \mid y) = p(\theta \mid \mu, \tau^2, y) p(\mu \mid \tau^2, y) p(\tau^2 \mid y)$.
%In fact, even for this simple setup the posterior has no known form, so we have to return to sampling methods.
%Say we are able to derive the factored distributions (on the right side) to the extent that we are able to draw samples, via e.g. the Metropolis-Hastings algorithm.
%In this case we can use the topological structure of the conditional dependencies in the model to apply \emph{forward sampling}, that is, first draw $\tau^0 \sim p(\tau^2 \mid y)$, then draw $\mu^0 \sim p(\mu \mid \tau^2=\tau^0, y)$ and at last draw $\theta^0 \sim p(\theta \mid \mu=\mu^0, \tau^2=\tau^0, y)$, which concludes one draw from the posterior (see figure \ref{fig:example_model} in the appendix).
%In more complex settings we might be unable to derive the densities with the appropriate conditioning set for forward sampling ---In these cases one can often apply \emph{Gibbs sampling} (see \citet{geman1984}) or if needed \emph{Metropolis-within-Gibbs sampling} (see \citet{gilks1995}).
%We conclude this section by providing a result stating that the joint posterior and its factors are all proper given that we assume a flat prior for $p(\mu \mid \tau^2)$.
%
%\begin{proposition}\label{prop:hierarchical_posterior}
%    Under the above setup, and the additional assumption that $p(\mu \mid \tau^2) \propto 1$ and $p(\tau^2)$ is a proper probability distribution, we get
%    \begin{center}
%      \begin{enumerate}[i.)]
%        \item $p(\theta \mid \mu, \tau^2, y) =  \prod_j p(\theta_j \mid \mu, \tau^2, y)$, with $\theta_j \mid \mu, \tau^2, y \sim \normal{\mu_j, \tau_j^2}$
%        \item $\mu \mid \tau^2, y \sim \normal{\bar{\mu}, \sigma_\mu^2}$
%        \item $p(\tau \mid y)$ is proper,
%      \end{enumerate}
%    \end{center}
%    where $\sigma_\mu^{-2} = \sum_j \frac{1}{\sigma_j^2 + \tau^2}\,,\,\, \tau_j^{-2}=\frac{1}{\sigma_j^2} + \frac{1}{\tau^2}\,,\,\, \mu_j=\tau_j^2(\frac{1}{\sigma_j^2} \bar{y}_j + \frac{1}{\tau^2}\mu)$ and $\bar{\mu} = \tau_j^2 \sum_j \frac{1}{\sigma_j^2 + \tau^2} \bar{y}_j$.
%\end{proposition}
%\textcolor{red}{\hrule}
%\begin{proof}
%See appendix.
%\end{proof}
%\textcolor{red}{\hrule}
%
%We refrain from a similar analysis of the results as was done before in the non-hierarchical case and only recommend the interested reader to perform one themself as an understanding of analytical Bayesian results can be especially rewarding.
%To end this section we remark that in this model we have to apply sampling methods as well; however, if we find a way to sample exactly from $p(\tau \mid y)$ we will be able to draw exact samples from the posterior.
%This is of course more accurate than using the MCMC methods introduced above.
%Unfortunately in more complex models this is not possible in the general case.

\subsection{Hierarchical Linear Models}
We end our segment on theory with the introduction of hierachical linear models, the class of models we will be using exclusively in our Monte Carlo and application part.

As in classical statistics, linear models are usually simpler to estimate and easier to interpret, which can explain their widespread use in practice.
Standard critiques on linear models, e.g. model misspecification issues, also apply here, however, we will see that due to the hiearchical nature we are able to model complex structure, even under a linearity assumption.
Being more precise, linearity in a hierarchical context means that on each level parameters enter the \emph{level-model} linearly.
Let us formalize this.

\paragraph{General Definition.}
As before, we consider outcomes $y_i$ for individuals $i=1,\mydots,n$ in groups $j=1,\mydots,J$, with individual-level characteristics $x_i$ and group-level characteristics $u_{j[i]}$.
A general two-level hierarchical linear model can then be written as
\begin{align}
  y_i &= \alpha_{j[i]} + x_i^{\intercal} \beta_{j[i]} + \epsilon_i \,, \tag{Individual Level}\\
  \beta_j &= \gamma_0 + u_j^{\intercal} \gamma + \eta_j \, \tag{Group Level},
\end{align}
where $\epsilon_i$ and $\eta_j$ denote error terms on the respective level.
Commonly the group-level errors are modeled as independent and identical, and the individual-level errors are modeled as independent and identical in their respective group.
Note that this can be extended for the heteroscedastic or autocorrelated case.
Just like in the previous section, this model does not define a Bayesian model per se.
We make the model Bayesian by imposing a prior distribution on all paramters that are not explicitly modeled themself.
In the above case this would be $\gamma_0, \gamma, \epsilon$ and $\eta$.

We consider the finite sample properties of this model for various types of priors in the next part and apply the model to real data in the last part.

%\paragraph{Varying Slopes Model with one Predictor in each Level.}
%To finish this section we will showcase the \emph{varying slopes model}, on which we rely heavily in the monte carlo study and which is closely related to the model considered in the application part.
%In particular we consider the case where each level features one regressor.
%Written formally we have
%\begin{align}
%  y_i = \alpha + \beta_{j[i]} x_i + \epsilon_i \,,
%\end{align}
%for the individual level, with $\epsilon_i \overset{\text{iid}}{\sim} \normal{0, \sigma_\epsilon^2}$.
%And for the group-level we get
%\begin{align}
%  \beta_j &= \gamma_0 + \gamma_1 u_j + \eta_j \,,
%\end{align}
%with $\eta_j \overset{\text{iid}}{\sim} \normal{0, \sigma_\eta^2}$.
%In a classicist interpretation we would call $\gamma_0$ and  $\gamma_1$ the \emph{fixed effects} as they do not vary group and the $\eta_j's$ the \emph{random effects} as they do vary by group.
%As \citet{GelmanHill2007} note, in a Bayesian interpretation this nomenclature is unfortunate as everything in a Bayesian setting is assumed to be random.
%
%Were we to let $\alpha$ vary by group and instead fix $\beta$ we would get a \emph{varying intercept model} as will be used in the application part.
%A justified question is if these comparably simple models have any sensible application.
%Using the example from \citet{GelmanHill2007}, consider modeling $J$ different experiments where in each experiment the baseline conditions were the same.
%In this setting we would like to measure the effect of some treatment (with treatment status $T_i$) on some outcome $y_i$.
%In this case we could model the outcomes as $y_i \mid \theta_{j[i]} \sim \normal{\alpha + \theta_{j[i]} T_i, \sigma^2}$,
%that is as a varying slopes model.
%Similar examples can be found to justify the use of varying intercept models.
%In a real application we usually combine varying intercept and varying slope models; however,
%this does not mean that every parameter must vary by group.
%Domain level knowledge can be used to guide the probabilistic modeler in choosing which parameters should vary and how many levels should be build.
