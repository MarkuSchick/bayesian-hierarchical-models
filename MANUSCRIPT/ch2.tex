\section{Hierarchical Models}
In the following we consider hierarchical models, when they are applicable and how to estimate them.
We begin by presenting the idea of hiearchical data and modeling.
Then we show how to solve a simple model analytically, before ending the section with introducing two main ideas.
One, hierarchical linear models, arguably the most important subclass of hiearchical models.
And two, a method to sample from the posteriors arising in more complex hierarchical models.

\subsection{Hierarchical Data and Modeling}

Here we consider what makes data hierarchical and how we can use this component to model additional structure.
Hierarchical data is present if the data can be clustered on some level; e.g. children in schools, survey responses on different years in different states or experiments in multiple labs.
From the examples we see that there must not be a clear \emph{hierarchy} defined on the data.
This is one of the reasons why some authors nowadays prefer the more general terms \emph{multi-level data} and \emph{multi-level model}, see for instance \citet{GelmanHill2007}.
We categorize models by the number of levels they incorporate and if they use \emph{nested} or \emph{non-nested} data.
In this paper we consider two-level models for nested data and refer again to \citet{GelmanHill2007} for a treatment of models with more levels and non-nested data.

In practice we can store hierarchical data very efficiently using normalized relational dataframes. Let us continue the example of children in schools and assume we observe their results on a test, the parental income and the number of teachers per child in the school.
Table \ref{tab:relational_table} portrays how multi-level data in this case could be stored.
Having gained some intuition let us define our formal notation.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{l l l l}
child & result & income & school\\
\hline
1 & 10 & 500 & 1\\
2 & 9 & 450 & 1\\
3 & 12 & 520 & 2\\
4 & 10 & 490 & 1
\end{tabular}
\quad
\begin{tabular}{l l}
school & teacher\\
\hline
1 & 0.5\\
2 & 0.7
\end{tabular}
\end{center}
\caption{Two tables containing fictional hierarchical data. Left: Data on the child-level, that is the test \emph{results}, parental \emph{income} and \emph{school} id. Right: Data on the school-level, in this case the number of \emph{teachers} per child.}
\label{tab:relational_table}
\end{table}

\noindent
Assume we observe data on $i=1,\mydots,n$ units which are clustered among $j=1,\mydots,J$ groups.
Naturally we consider some outcome $y_i$ on the unit-level.
Following the idea of different tables for different levels from above, we write $x_i$ for the covariates that vary by unit and $u_j$ for the covariates that only vary on the group-level.
We link the two by writing $j[i]$ for the index of the group to which individual $i$ belongs, i.e. the full set of covariates from individual $i$ is given by $(x_i, u_{j[i]})$. But how can we utilize this hierarchical structure?

The main idea behind hierarchical modeling is that we build (simple) models on each level while using the dependent variables from higher levels as input parameters on lower levels.
For a general (two-level) case we may write
\begin{align}
  y_i \mid \theta_j &\sim p(y_i \mid x_i, \theta_j) \,,\\
  \theta_j \mid \phi &\sim p(\theta_j \mid u_j, \phi) \,,\\
  \phi &\sim p(\phi \mid \zeta) \text{ with } \zeta \text{ fixed } \,,
\end{align}
where we suppress the dependence on $x_i$ and $u_j$ by assuming they are fixed.
In the first level we model the observations $y_i$ depending on the covariates $x_i$ and parameters $\theta_j$.
As in a classical Bayesian model we continue by modeling the parameters.
However in contrast to section 2 we do not just assume some prior distribution for the parameters but we explicitly model the parameter.
From a Bayesian viewpoint this can be seen as a generalization of the prior.
Despite this Bayesian interpretation, the first two equations define a proper non-Bayesian hierarchical model.
To put the Bayesian in Bayesian hiearchical model we have to assign a prior distributions on the parameters $\phi$.
Therefore one often speaks of priors $\theta_j$ and \emph{hyperpriors} $\phi$.

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}%
  [vertex/.style={circle,draw=black,fill=white, minimum size=1cm},
  node distance=2.5cm,
  >=latex,
  on grid]
  \node[vertex] (phi) {$\phi$};
  \node[vertex,right=of phi] (theta) {$\theta_{j[i]}$};
  \node[vertex,below right= 1cm and 2.5cm of theta] (y) {$y_i$};
  \node[rectangle, draw=black, minimum size=0.9cm,left= of phi] (zeta) {$\zeta$};
  \node[rectangle, draw=black, minimum size=0.9cm,above=1.5cm of theta] (u) {$u_j$};
  \node[rectangle, draw=black, minimum size=0.9cm, above right=1cm and 2.5cm of y] (x) {$x_i$};
  \draw[->]
    (phi) edge (theta)
    (theta) edge (y)
    (zeta) edge (phi)
    (u) edge (theta)
    (x) edge (y);
\end{tikzpicture}
\end{center}
\label{tik:general_sem}
\caption{A general two-level Bayesian hierarchical model depicted as a directed acyclical graph. Circled parameters denote random quantities while parameters contained in squares denote fixed quantities.}
\end{figure}





\newpage
\subsubsection*{Formal Example}
Let us assume that we observe test scores $y_i$ for $i = 1,\mydots,n$ children
in $j=1,\mydots,J$ different schools. For convenience let us write $j[i]$ for child
$i$'s school. We assume that for all children in school $j$, the outcome $y_i$
follows a common data generating process governed by some parameter $\theta_j$.
To make this model hierarchical we assume that these $\theta_1,\mydots,\theta_J$
also follow a common data generating process governed by some hyperparameter $\phi$.
This gives
\begin{align}
  p(y_i \mid \theta_{j[i]}, \phi) = p(y_i \mid \theta_{j[i]})
\end{align}
for our model of test scores given all parameters. Note that since $\phi$ only
affects $y_i$ through $\theta_{j[i]}$ we can drop it from the conditioning set.
Further we model the common structure of the $\theta_j$'s through $p(\theta_j \mid \phi)$.
To analyze this problem using Bayesian techniques we must at last assign a prior
to $\phi$, that is, we define $p(\phi)$.

\subsection{Solving for the posterior analytically}
As in the previous section, we will first present the analytical derivation of
the posterior in a simple normal hierarchical model. We continue using the
example of modeling observed test scores of children in different schools.

The objects of our interest are given by the joint posterior $p(\theta_j, \phi \mid y)$
and its two marginal posteriors. In order to derive these analytically we must
make convenient distributional assumptions. For the sake of exposition, we choose


\subsection{Hierarchical Linear Models}

\subsubsection{Varying Slopes Model With One Predictor In Each Level}
We assume that units $i = 1,\mydots,n$ can be divided into $J$ distinct groups.
We start with a very simple model assuming that intercept is fixed for all
groups, that is
\begin{align}
  y = \alpha + \beta_j x + \epsilon \,,
\end{align}
with $\epsilon$ following a mean zero normal distribution with variance
$\sigma_{\epsilon}^2$.
To incorporate the idea that the groups follow a common structure we also
assume
\begin{align}
  \beta_j &= \gamma_0 + \gamma_1 u_j + \eta \,,
\end{align}
for $j = 1,\mydots,J$, with $\eta$ mean zero normal with variance $\sigma_\eta^2$.

Since $\gamma_0$ and $\gamma_1$ do not vary by group they are sometimes referred
to as \emph{fixed effects}. Similary as $\eta$ is drawn randomly for each group
it is sometimes called a \emph{random effect}. Put together this shows the close
resemblance of the hierarchical linear model to classical mixed effects models
\textcolor{red}{(some reference here would be nice!)}

Following the notation of \textcolor{red}{Gelman and Hill (2007)} we describe the
model equation of a single individual $i$ by
\begin{align}
  y_i = \alpha + \beta_{j[i]} x_i + \epsilon_i \,,
\end{align}
where $j[i]$ denotes the group to which individual $i$ belongs.

The model defined by the assumptions and equations above \textcolor{red}{(where and what)} can of course be made
arbitrarily complex. For example we could add higher order polynomial terms or
more predictors, as in regular linear regression modeling. Further, the
normality assumption of the errors could be relaxed and most importantly, why
stop at two levels? Naturally we could model the group-level coefficients using
a third level. For the sake of a simpler explaination however, we will stick to
the presented model.

%\subsubsection{Varying Intercept and Slope Model with One Predictor in Each Level}
%We assume that units $i = 1,\mydots,n$ can be divided into $J$ distinct groups.
%In each group $j$ we model our outcome variable $y$ as a linear function in $x$,
%that is
%\begin{align}
  %y = \alpha_j + \beta_j x + \epsilon \,,
%\end{align}
%with $\epsilon$ following a mean zero normal distribution with variance
%$\sigma_{\epsilon}^2$.
%To incorporate the idea that the groups follow a common structure we also
%assume
%\begin{align}
  %\alpha_j &= \gamma_0^{\alpha} + \gamma_1^{\alpha} u_j + \eta_\alpha \\
  %\beta_j &= \gamma_0^{\beta} + \gamma_1^{\beta} u_j + \eta_\beta \,,
%\end{align}
%for $j = 1,\mydots,J$, with
%\begin{align}
  %\sqmat{\eta_\alpha \\ \eta_\beta} \sim \normal{\sqmat{\sigma_\alpha^2 & \rho \sigma_\alpha \sigma_\beta \\ & \sigma_\beta^2}}
%\end{align}
%
%Following the notation of \textcolor{red}{Gelman and Hill (2007)} we describe the model equation of a single individual $i$ by
%\begin{align}
  %y_i = \alpha_{j[i]} + \beta_{j[i]} x_i + \epsilon_i \,,
%\end{align}
%where $j[i]$ denotes the group to which individual $i$ belongs.
%This particular model is known as the two-level varying intercept / varying slope model with one unit-level predictor (here $x_i$) and one group-level predictor
%(here $u_i$).
%The model defined by equations \textcolor{red}{(1) - (4)} can of
%course be made arbitrarily complex by adding higher order polynomial terms or
%more predictors, as in a regular linear regression models. Further the normality
%assumption of equation \textcolor{red}{(4)} is not mandotory and can be swapped
%with nearly any other distributional assumption. Also, why stop at two levels?
%We could naturally model the coefficients in equation \textcolor{red}{2 and 3}
%using a third level. All these extensions can in practice be necessary when
%modelling complex structures; however for the sake of simplicity and clarity
%we will stick to our clean model.
