\section{Hierarchical Models}
In the following we consider hierarchical models, when they are applicable and how to estimate them.
We begin by presenting the idea of hiearchical data and modeling.
Then we show how to solve a simple model analytically, before ending the section with introducing two main ideas.
One, hierarchical linear models, arguably the most important subclass of hiearchical models.
And two, a method to sample from the posteriors arising in complex settings.

\subsection{Hierarchical Data and Modeling}
Here we consider what makes data hierarchical and how we can use this component to model additional structure.
Hierarchical data is present if the data can be clustered on some level; e.g. children in schools, survey responses on different years in different states or experiments in multiple labs.
From the examples we see that there must not be a clear \emph{hierarchy} defined on the data.
This is one of the reasons why some authors nowadays prefer the more general terms \emph{multi-level data} and \emph{multi-level model}, see for instance \citet{GelmanHill2007}.
We categorize models by the number of levels they incorporate and if they use \emph{nested} or \emph{non-nested} data.
In this paper we consider two-level models for nested data and refer again to \citet{GelmanHill2007} for a treatment of models with more levels and non-nested data.

In practice we can store hierarchical data very efficiently using normalized relational dataframes. Let us continue the example of children in schools and assume we observe their results on a test, the parental income and the number of teachers per child in the school.
Table \ref{tab:relational_table} portrays how multi-level data in this case could be stored.
Having gained some intuition let us define our formal notation.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{l l l l}
child & result & income & school\\
\hline
1 & 10 & 500 & 1\\
2 & 9 & 450 & 1\\
3 & 12 & 520 & 2\\
4 & 10 & 490 & 1
\end{tabular}
\quad
\begin{tabular}{l l}
school & teacher\\
\hline
1 & 0.5\\
2 & 0.7
\end{tabular}
\end{center}
\caption{Two tables containing fictional hierarchical data. Left: Data on the child-level, that is the test \emph{results}, parental \emph{income} and \emph{school} id. Right: Data on the school-level, in this case the number of \emph{teachers} per child.}
\label{tab:relational_table}
\end{table}

\noindent
Assume we observe data on $i=1,\mydots,n$ units which are clustered among $j=1,\mydots,J$ groups.
Naturally we consider some outcome $y_i$ on the unit-level.
Following the idea of different tables for different levels from above, we write $x_i$ for the covariates that vary by unit and $u_j$ for the covariates that only vary on the group-level.
We link the two by writing $j[i]$ for the index of the group to which individual $i$ belongs, i.e. the full set of covariates from individual $i$ is given by $(x_i, u_{j[i]})$. But how can we utilize this hierarchical structure?

The main idea behind hierarchical modeling is that we build (simple) models on each level while using the dependent variables from higher levels as input parameters on lower levels.
For a general (two-level) case we may write
\begin{align}
  y_i \mid \theta_{j[i]} &\sim p(y_i \mid x_i, \theta_{j[i]}) \,,\\
  \theta_j \mid \phi &\sim p(\theta_j \mid u_j, \phi) \,,\\
  \phi &\sim p(\phi \mid \zeta) \text{ with } \zeta \text{ fixed } \,,
\end{align}
where we suppress the dependence on $x_i$ and $u_j$ by assuming they are fixed.
In the first level we model the observations $y_i$ depending on the covariates $x_i$ and parameters $\theta_j$.
As in a classical Bayesian model we continue by modeling the parameters; However, in contrast to section 2 we do not just assume some prior distribution for the parameters but we \emph{explicitly} model the parameter.
From a Bayesian viewpoint this can be seen as a generalization to prior modeling.
Despite this Bayesian interpretation, the first two equations define a proper non-Bayesian hierarchical model.
We will see that in the linear case these models are well known in the frequentist world as \emph{mixed effects models} or \emph{random coefficient models}.
To put the Bayesian in Bayesian hiearchical model we have to assign a prior distributions on the parameters $\phi$.
As $\phi$ are themselves parameters for the parameter $\theta_j$, one often speaks of priors on $\theta_j$ and \emph{hyperpriors} on the \emph{hyperparameter} $\phi$.

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}%
  [vertex/.style={circle,draw=black,fill=white, minimum size=1cm},
  node distance=2.5cm,
  >=latex,
  on grid]
  \node[vertex] (phi) {$\phi$};
  \node[vertex,right=of phi] (theta) {$\theta_{j[i]}$};
  \node[vertex,below right= 1cm and 2.5cm of theta] (y) {$y_i$};
  \node[rectangle, draw=black, minimum size=0.9cm,left= of phi] (zeta) {$\zeta$};
  \node[rectangle, draw=black, minimum size=0.9cm,above=1.5cm of theta] (u) {$u_{j[i]}$};
  \node[rectangle, draw=black, minimum size=0.9cm, above right=1cm and 2.5cm of y] (x) {$x_i$};
  \draw[->]
    (phi) edge (theta)
    (theta) edge (y)
    (zeta) edge (phi)
    (u) edge (theta)
    (x) edge (y);
\end{tikzpicture}
\end{center}
\label{fig:general_sem}
\caption{A generic two-level Bayesian hierarchical model depicted as a directed acyclical graph modeling a single generic observation. Circled parameters denote random quantities while parameters contained in squares denote fixed quantities.}
\end{figure}


Figure \ref{fig:general_sem} illustrates the conditional dependence structure of modeling a generic observation $y_i$.
In contrast, consider figure \ref{fig:group_sem} which illustrates the conditional dependence structure when modeling a generic observation $y(j)$ in group $j$.
We depict random quantities in circles and fixed quantities in squares.
Not only do these graphical representations help with understanding the structure of the model but we will see that when solving for the posterior they give an immediate way to check which parameters are conditionally independent.

\begin{figure}[ht]
\begin{center}
\begin{tikzpicture}%
  [vertex/.style={circle,draw=black,fill=white, minimum size=1cm},
  node distance=2.5cm,
  >=latex,
  on grid]
  \node[vertex] (phi) {$\phi$};
  \node[rectangle, draw=black, minimum size=0.9cm,left=2cm of phi] (zeta) {$\zeta$};
  \node[vertex,below left=1.5cm and 2cm of phi] (theta1) {$\theta_1$};
  \node[vertex,below right=1.5cm and 2cm of phi] (thetaJ) {$\theta_J$};
  \node[below=1.5cm of phi] (dots1) {$\dots$};
  \node[rectangle, draw=black, minimum size=0.9cm, left=2cm of theta1] (u1) {$u_1$};
  \node[rectangle, draw=black, minimum size=0.9cm, right=2cm of thetaJ] (uJ) {$u_J$};
  \node[vertex,below=2cm of theta1] (y1) {$y(1)$};
  \node[vertex,below=2cm of thetaJ] (yJ) {$y(J)$};
  \node[rectangle, draw=black, minimum size=0.9cm, left=2cm of y1] (x1) {$x(1)$};
  \node[rectangle, draw=black, minimum size=0.9cm, right=2cm of yJ] (xJ) {$x(J)$};
  \node[below=3.5cm of phi] (dots1) {$\dots$};
  \draw[->]
    (zeta) edge (phi)
    (phi) edge (theta1)
    (phi) edge (thetaJ)
    (u1) edge (theta1)
    (uJ) edge (thetaJ)
    (theta1) edge (y1)
    (thetaJ) edge (yJ)
    (x1) edge (y1)
    (xJ) edge (yJ);
\end{tikzpicture}
\end{center}
\label{fig:group_sem}
\caption{A generic two-level Bayesian hierarchical model depicted as a directed acyclical graph modeling generic observations $y(j)$ in groups $j = 1,\mydots, J$. Circled parameters denote random quantities while parameters contained in squares denote fixed quantities.}
\end{figure}

\subsection{\textcolor{red}{Solving for the Posterior Analytically}}
\textcolor{red}{
As in the previous section we consider first an analytical derivation of the posterior distributions using a simple normal model, before dealing with more complex settings.
}
The objects of our interest are given by the joint posterior $p(\theta_j, \phi \mid y)$
and its two marginal posteriors. In order to derive these analytically we must
make convenient distributional assumptions. For the sake of exposition, we choose

\subsection{Hierarchical Linear Models}
The following subsection presents hierarchical linear models, an important subclass of the general multi-level model.
As in classical statistics linear models are usually simpler to estimate and easier to interpret.
The standard critique on linear models from classical statistics, i.e. model misspecification, also applies here; however, we will see that due to the hiearchical nature we are able to model very complex structure even under a linearity assumption.

Being more precise, linearity here means that on each level that is being modeled, parameters enter the \emph{level-model} lineary.
We will continue by shortly presenting the general (two-level) hierarchical linear model.
In the last subsection we consider the \emph{varying slopes model} which will be further analyzed in the monte carlo study.
We also note here that the varying slopes model is closely related to the \emph{varying intercepts model} which will be used in the application part.

\subsubsection*{General Definition}
As before we consider outcomes $y_i$ for $i=1,\mydots,n$ in groups $j=1,\mydots,J$ with individual-level characteristics $x_i$ and group-level characteristics $u_{j[i]}$.
When talking about hierarchical linear models, we consider models that can be written as follows
\begin{align}
  y_i &= \alpha_{j[i]} + x_i^\prime \beta_{j[i]} + \epsilon_i \,,\\
  \beta_j &= \gamma_0 + u_j^\prime \gamma + \eta_j \,,
\end{align}
where $\epsilon_i$ and $\eta_j$ denote innovation terms on the respective level with distributional assumptions $\epsilon_i \sim p(\epsilon; \theta_\epsilon)$ and $\eta_j \sim p(\eta; \theta_\eta)$.
Note that this can be extended for the heteroscedastic or autocorrelated case.
The model becomes Bayesian as we impose prior distributions on all parameters.

\subsubsection*{Varying Slopes Model with one Predictor in each Level}
To finish this section we will showcase the \emph{varying slopes model}, on which we rely heavily in the monte carlo study and which is closely related to the model considered in the application part.
In particular we consider the case where each level features one regressor.
Written formally we have
\begin{align}
  y_i = \alpha + \beta_{j[i]} x_i + \epsilon_i \,,
\end{align}
for the individual level, with $\epsilon_i \overset{\text{iid}}{\sim} \normal{0, \sigma_\epsilon^2}$.
And for the group-level we get
\begin{align}
  \beta_j &= \gamma_0 + \gamma_1 u_j + \eta_j \,,
\end{align}
with $\eta_j \overset{\text{iid}}{\sim} \normal{0, \sigma_\eta^2}$.
In a classicist interpretation we would call $\gamma_0$ and  $\gamma_1$ the \emph{fixed effects} as they do not vary group and the $\eta_j's$ the \emph{random effects} as they do vary by group.
As \citet{GelmanHill2007} note, in a Bayesian interpretation this nomenclature is unfortunate as everything in a Bayesian setting is assumed to be random.

Were we to let $\alpha$ vary by group and instead fix $\beta$ we would get a \emph{varying intercept model} as will be used in the application part.
A justified question is if these comparably simple models have any sensible application.
Using the example from \citet{GelmanHill2007}, consider modeling $J$ different experiments where in each experiment the baseline conditions were the same.
In this setting we would like to measure the effect of some treatment (with treatment status $T_i$) on some outcome $y_i$.
In this case we could model the outcomes as $y_i \mid \theta_{j[i]} \sim \normal{\alpha + \theta_{j[i]} T_i, \sigma^2}$,
that is as a varying slopes model.
Similar examples can be found to justify the use of varying intercept models.
In a real application we usually combine varying intercept and varying slope models; however,
this does not mean that every parameter must vary by group.
Domain level knowledge can be used to guide the probabilistic modeler in choosing which parameters should vary and how many levels should be build.
