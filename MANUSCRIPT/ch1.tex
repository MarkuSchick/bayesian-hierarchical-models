\section{Bayesian Thinking and Estimation}
In this section we will introduce the core topics of Bayesian data analysis, and whenever possible compare proposed methods and results to their frequentist counterpart.
As we will see, Bayesian statistics differs from mainstream statistics on a fundamental level; Thus, we have to start there.

\textcolor{red}{What remains to be introduced:}
\begin{enumerate}
  \item Notation: $p(...)$
\end{enumerate}

\subsection{(Probabilistic) Modeling}
Before going further, let us first formalize our notion of stochastic modeling.
Imagine being interested in some \textit{phenomena}, for example the effect of bigger class sizes on school children performance.
Most phenomena cannot be observed directly and only manifest themself through some latent variable system.
We can still hope to learn about the phenomena by studying the observational process around it.
Clearly the way in which a phenomena reveals itself is dependent on its environment;
The observed effects for school-children in sub-saharan africa might look very different to the ones in central europe.
To derive sensible results from our analysis we need to postulate the existence of a \textit{true data generating process}, which captures the way in which the observational process adheres to the effects of the phenomena of interest given its environment.
We define this object as a probability distribution $p_0$ on the observational space $\mathbb{Z}$.
In this step we move from a model to a probabilistic model, in that we allow our system of interest to be influenced by randomness and not only be characterized by a deterministic process.
In practice $p_0$ is rarely known, therefore the challenge lies in recovering a distribution using the observed data which is as close as possible to $p_0$.
This is usually done by assuming that the true data generating process falls in some class of models, for example a linear model with normal errors.
Formally, we restrict our attention to a subset of potential observational processes $\mathbb{M}$ over the whole space of distributions on $\mathbb{Z}$.
The beauty of using a model class approach in the construction of $\mathbb{M}$ is that for each distribution $p \in \mathbb{M}$, we can find a parameterization $\theta$ in the configuration space $\Theta$; For example, the class of multivariate normal distributions is parameterized by its mean and covariance $(\mu, \Sigma) = \theta \in \Theta$.
The goal of all subsequent statistical analysis is then to utilize the observed data to determine the regions in $\Theta$ which are most consistent with $p_0$ and simultaneously to capture our uncertainty about these statements.
If $p_0 \in \mathbb{M}$ we can find a parameterization $\theta_0$ which corresponds to the true data generating process; naturally we seek estimators that determine regions close to $\theta_0$. If, however, $p_0 \not\in \mathbb{M}$ we enter the world of model misspecification which leads to all sorts of problems.
For everything that follows let us therefore make the omnipresent assumption that $p_0 \in \mathbb{M}$.

\subsection{Schools of Thought}
Next we discuss how the Bayesian and the classical mindset differ. In particular we focus on the predominant way of thinking for most of statistical history, \textit{frequentist statistics}. We do not aim at an exhaustive overview here nor do we presume that the individual statistician belongs to one and only one of the following categories.

\textbf{Frequentist.} The frequentist approach assumes that the true data generating process is completely specified by an unkown but fixed quantity $\theta_0 \in \Theta$.
\textcolor{red}{FOLLOWING IS NOT TRUE, MAKE IT CLEAR WHAT YOU MEAN.}
Either implicitly or explicitly we define the \textit{likelihood} $p(z; \theta)$ by modeling the observational process for $z \in \mathbb{Z}$ using a parameterization $\theta$.
The main challenges include finding a (point) estimator for $\theta_0$, quantifying the uncertainty of the estimate and testing hypothesis.
One fundamental idea which stretches over all these topics is the interpretation of probability as the limit of an infinite sequence of relative frequencies ---hence the name.
That is, the probability of an event happening is just the limit of the frequency of that event happening over infinitely many independent experiments.
What are the implications of this understanding of probability?
Many interesting questions do not provide us with a thought experiment in which we can consider an ever increasing sequence of experiments.
In these cases using probability is either trivial or lacking an indisputable interpretation.
As we use the mathematical rigor of probability theory in our formal derivations, we will obtain (mathematically) correct results; however, the interpretation of these results might be highly unintuitive ---for example consider confidence intervals.
Since $\theta_0$ is fixed all probability statements regarding this object are trivial, that is, either one or zero. This propagates to the problem of hypothesis testing.
All hypothesis are either true or false and therefore have probabilities of one or zero.
A hypothesis $H_0$ is rejected if conditional on $H_0$ being true the probability of observing the data in the given sample is lower than some threshold, i.e. $P(\text{data} \mid H_0) < \alpha$.
Note that this statement does not tell us anything about $H_0$ directly, but only about the data at hand.

\textbf{Bayesian.} The Bayesian approach also assumes that there may be a true data generating process specified by some (maybe fixed) quantity $\theta_0 \in \Theta$.
The main difference is their understanding of probability as a subjective quantification of uncertainty.
In this view one is not limited to assigning non trivial probability statements only to objects that appear random in sequential experiments.
We can see the direct utility of this liberation by considering a special case of Bayes theorem
\begin{align}
  p(\theta \mid \text{data}) = \frac{p(\text{data} \mid \theta) p(\theta)}{p(\text{data})} \propto p(\text{data} \mid \theta) p(\theta) \,,
\end{align}
which reads
\begin{align}
  posterior = \frac{likelihood \times prior}{evidence} \propto likelihood \times prior \,.
\end{align}
In the frequentist setting this is of no use, since the statement $p(\theta)$ is nonsensical ---remember that probabilistic statements about fixed quantities are meaningless from a frequentist perspective.
This already outlines the main criticism of Bayesian analysis: Where does the prior $p(\theta)$ come from?
With the scientific goal of objectivity in mind, many feel uneasy with results being dependent on a subjective choice of a prior.
In what follows we will embark on the Bayesian idea without providing much more fundamental criticism; nonetheless, when adequate we will consider the influence of different priors on the posterior.
To end this comparison, what then are the main tasks associated with a Bayesian analysis?
These can be categorized by (i) obtaining the posterior distribution (or something equivalent) and (ii) communicating the information held in the posterior.
The first consists of defining the likelihood and (additionally to the frequentist approach) constructing a prior distribution and afterwards combining those to compute the posterior.
This computation can sometimes be achieved analytically, but in most cases one has to rely on algorithms to obtain samples of the posterior.
The second part consists of plotting the marginal posterior distributions, computing expectations of the form $\Exp{h(\theta) \mid \text{data}}$ and testing hypothesis.
All of the above can be done independent of the posterior being available analytically or through samples.
A clear difference can be seen when considering hypothesis testing.
Sacrificing \textit{objectivity} allows us to answer the questions we usually want to ask:
\begin{align}
  P(H_0: \theta \in S \mid \text{data}) = \int_{\theta \in S} p(\theta \mid \text{data}) \mathrm{d}\theta \,.
\end{align}
In the subsequent paragraphs we will be mostly occupied with the first category, computing the posterior, with occasional remarks on the second; in particular, the approximation of expectations.

\subsection{Solving for the posterior analytically}
In this subsection we present the analytical derivation of the posterior distribution of mean and variance parameters in a univariate normal model for two priors.
We will compare the results to the appropriate classical method, namely maximum likelihood.

In both cases, let us assume that we observe an iid sample $y = (y_1, \mydots, y_n)$
with $y_i \sim \normal{\mu, \sigma^2}$.
Our interest lies in solving for the marginal posteriors $p(\mu \mid y)$ and $p(\sigma^2 \mid y)$.

To be precise, in a full bayesian analysis we assume $\theta = (\mu, \sigma^2)$ to be a random quantity, thus the correct statement should be $y_i \mid \theta \sim \normal{\mu, \sigma^2}$.
In situations where this conditional dependence is clear many writers will use the first notation.
Here we try to be as pedantic as possible to avoid any confusion and will therefore stick to the second notation.

As it will be of major importance in the subsequent sections we remind the reader of some probability distributions uncommon in the non-Bayesian world.

\begin{definition}{(Scaled inverse $\chi^2$ distribution).}
  Let $\nu > 0$ and $\tau^2 > 0$ be parameters representing degrees of freedom and scale, respectively. The family of \emph{scaled inverse $\chi^2$ distributions} is characterized by its probability density function, namely
  \begin{align}
    p(x) \propto x^{-(1 + \nu / 2)} \EXP{\frac{-\nu \tau^2}{2 x}} \quad \text{for} \, x \in (0, \infty) \,,
  \end{align}
  where the constant of integration is ignored for clarity.
  We write $X \sim \scaledInvChi{\nu, \tau^2}$ to denote that the random variable $X$ follows a scaled inverse $\chi^2$ distribution with parameters $\nu$ and $\tau^2$.
\end{definition}

\textcolor{red}{
\begin{definition}{(Normal scaled inverse $\chi^2$ distribution).}
\end{definition}
}

\subsubsection*{Uninformative Prior}
We start our first Bayesian analysis by considering a prior which contains virtually no information.
This results in an analysis being mainly, if not completely, driven by the likelihood.
A common assumption is independence of the individual priors, that is $p(\theta) = p(\mu, \sigma^2) = p(\mu) p(\sigma^2)$.
A natural choice of declaring full ignorance of prior information is to assign a prior over the complete domain of the random parameter. For our case this mean $p(\mu) \propto 1$.
We note that this does not define proper proability distribution, which will not matter in this case but can lead to problems in others.\footnote{LINK TO PAPER}
Since the variance is restricted to be positive we impose a uniform prior on the log-transform thereof: $p(\log \sigma) \propto 1$.
This leads to the improper prior\footnote{\textcolor{red}{See appendix for a derivation.}}
\begin{align}
  p(\mu, \sigma^2) \propto (\sigma^2)^{-1} \,.
\end{align}
As usual the likelihood is given by
\begin{align}
  p(y \mid \mu, \sigma^2) \propto (\sigma^2)^{-n/2} \EXP{-\frac{1}{2\sigma^2}\sum_i (y_i - \mu)^2} \,,
\end{align}
where we dropped all proportionality constants.
Using the above we can apply Bayes theorem to yield
\begin{align}
  p(\mu, \sigma^2 \mid y) &\propto p(y \mid \mu, \sigma^2) p(\mu, \sigma^2)\\
  &\propto (\sigma^2)^{-(n+2)/2} \EXP{-\frac{1}{2\sigma^2}\sum_i (y_i - \mu)^2} \,.
\end{align}
From here we can derive the marginals by integrating out the respective other parameter. This is formalized in the following two proposition.

\begin{proposition}
  Under the uniform prior from above we find
  \begin{align}
    \mu \mid y &\sim t_{n-1}(\bar{y}, s^2/n) \,,\\
    \sigma^2 \mid y &\sim \scaledInvChi{n-1, s^2} \,,
  \end{align}
  where $s^2 = \frac{1}{n-1} \sum_i (y_i - \bar{y})^2$ denotes the sample variance and $\bar{y} = \frac{1}{n} \sum_i y_i$ the sample mean.
\end{proposition}
\begin{proof}
See appendix.
\end{proof}

Having derived the marginal posterior distributions, we can compare the results to their maximum likelihood (ML) counterpart.
Since the ML estimates $\left(\argmax{\theta \in \Theta}p(y \mid \theta)\right)$ are point estimates, we consider the similar \emph{maximum a posteriori} (MAP) estimate $\left(\argmax{\theta \in \Theta} p(\theta \mid y)\right)$, as well as the posterior mean and variance. All results are summarized in table \ref{table:comp_uniform_bay_ml}.

\begin{table}[ht]
\centering
\def\arraystretch{1.3}
{\small
 \begin{tabular}{c | c c c c c}
 Parameter & ML Estimate & ML Variance & MAP & Posterior Mean & Posterior Variance\\[0.5ex]
 \hline
 $\mu$ & $\bar{y}$ & $\sigma^2/n$ & $\bar{y}$ & $\bar{y}$ & $s^2 / n$\\
 $\sigma^2$ & $\frac{n-1}{n} s^2$ & $2 \sigma^4 /n$ & ? & $\frac{n-1}{n-3} s^2$ & $\frac{2 (n-1)^2}{(n-3)^2 (n-5)} s^4$\\
 \end{tabular}
 }
\caption{\small {Comparison of Bayesian estimates using an uninformative prior and ML estimates. See appendix for a detailed derivation.}}
\label{table:comp_uniform_bay_ml}
\end{table}


\subsubsection*{Conjugate Prior}
We have seen that using an uninformative prior leads to results that are very similar to the ones obtained by a ML approach.
In case information on the parameters is available prior to observing the data we can utilize this fact by properly modeling the prior distribution.
Since we are interested in analytical results in this section,b we cannot mix any prior with any likelihood, as the product might not be of known form.
This leads us to the class of \emph{conjugate priors}.

\begin{definition}{(Conjugate prior).}
Let the likelihood $p(y \mid \theta)$ be given and assume that the prior distribution $p(\theta)$ is a member of some family $\mathcal{F}$ of probability distributions.
We say that $p(\theta)$ is a \emph{conjugate prior} if the posterior $p(\theta \mid y)$ is also a member of $\mathcal{F}$.
\end{definition}

Conjugate priors were of particular importance in the early stages of Bayesian statistics since these give the practitioner certainty that the posterior follows a distribution which is known and computable. Moreover, nowadays we still see conjugate priors in use as they allow for a full or partial analytical derivation, which increases the accuracy of results or shortens the runtime of programs. For more complex models however conjugate priors can become too restrictive. We discuss solutions to this problem in the next section.

Consider again the likelihood but written to demonstrate its dependence on $\mu$ and $\sigma^2$
\begin{align}
  p(y \mid \mu, \sigma^2) \propto (\sigma^2)^{n/2} \EXP{-\frac{1}{2\sigma^2} n\left[ (\mu - \bar{y})^2 + (\bar{y^2} - \bar{y})^2 \right]} \,.
  \label{eq:likelihood2}
\end{align}
We want to construct a two dimensional prior for $(\mu, \sigma^2)$.
A theme to which we will be coming back is that modeling higher dimensional parameters by modeling many lower dimensional (sub)parameters using conditioning is often easier than modeling the complete distribution.
Here we utilize the equality $p(\mu, \sigma^2) = p(\mu \mid \sigma^2) p(\sigma^2)$.
By looking at the likelihood (equation \ref{eq:likelihood2}) we note that in order to \emph{not} change the inherent structual dependence on the parameters, $\mu \mid \sigma^2$ has to be distributed according to $\normal{\mu_0, \sigma^2 / \kappa_0}$ with so called \emph{hyperparameters} $\mu_0$ and $\kappa_0 > 0$.
Similary, we note that an informative prior for $\sigma^2$ has to respect the structure in which $\sigma^2$ appears in the likelihood.
We achieve this when $\sigma^2 \sim \scaledInvChi{\nu_0, \sigma_0^2}$ with hyperparameters $\nu_0$ and $\sigma_0^2 > 0$.
Following  \textcolor{red}{\citet{gelmanbda04}} we write $(\mu, \sigma^2) \sim \NormalscaledInvChi{\mu_0, \sigma_0^2 / \kappa_0; \nu_0, \sigma_0^2}$ with corresponding density function
\begin{align}
  p(\mu, \sigma^2) = p(\mu \mid \sigma^2) p(\sigma^2) \propto (\sigma^2)^{\frac{3 + \nu_0}{2}} \EXP{-\frac{1}{2\sigma^2} \left[\nu_0 \sigma_0^2 + \kappa_0(\mu_0 - \mu)^2 \right]} \,.
\end{align}

Multiplying the likelihood with our constructed prior we get the joint posterior (up to an integration constant)
\begin{align}
  p(\mu, \sigma^2 \mid y) \propto& (\sigma^2)^{-\frac{3 + n + \nu_0}{2}} \times\\
  & \times \EXP{-\frac{1}{2 \sigma^2} \left[ (\mu - \bar{y})^2 + (\bar{y^2} - \bar{y})^2 + \nu_0\sigma_0^2 + \kappa_0(\mu - \mu_0)^2 \right]} \,.
  \label{eq:conjugate_posterior}
\end{align}

\begin{proposition}
  The posterior distribution of $(\mu, \sigma^2) \mid y$, as given by the conditional density in equation \ref{eq:conjugate_posterior}, is $\NormalscaledInvChi{\mu_n, \sigma_n^2/\kappa_n; \nu_n, \sigma_n^2}$ distributed, where
  \begin{align*}
    \nu_n &= \nu_0 + n \,; \quad \kappa_n = \kappa_0 + n \,; \quad \mu_n =\frac{\kappa_0}{\kappa_0 + n}\mu_0 + \frac{n}{\kappa_0 + n}\bar{y} \,,\\
    \sigma_n^2 &= \left[\nu_0 \sigma_0^2 + (n-1)s^2 + \frac{\kappa_0 n}{\kappa_0 + n} (\bar{y} - \mu_0)^2\right] /\nu_n \,.
  \end{align*}
  \label{prop:posterior_conjugate}
\end{proposition}
\begin{proof}
  See appendix.
\end{proof}

Since the prior and the posterior are both normal scaled inverse $\chi^2$ distributed, we can speak of a conjugate prior.
Using the intermediate finding from proposition \ref{prop:posterior_conjugate} we can derive the main result of this section.

\begin{proposition}
  The marginal posterior distributions are given by
  \begin{align*}
    \mu \mid y &\sim t_{\nu_n}(\mu_n, \sigma_n^2 / \kappa_n)\\
    \sigma^2 \mid y &\sim \scaledInvChi{\nu_n, \sigma_n^2}\,,
  \end{align*}
  where $\nu_n, \sigma_n^2, \mu_n$ and $\kappa_n$ are as in proposition \ref{prop:posterior_conjugate}.
\end{proposition}
\begin{proof}
  See appendix.
\end{proof}

\begin{table}[ht]
\centering
\def\arraystretch{1.3}
{\small
 \begin{tabular}{c | c c c c c}
 Parameter & ML Estimate & ML Variance & MAP & Posterior Mean & Posterior Variance\\
 \hline
 $\mu$ & $\bar{y}$ & $\sigma^2 / n$ & $\mu_n$ & $\mu_n$ & $\sigma_n^2 / \kappa_n$\\
 $\sigma^2$ & $\frac{n-1}{n}s^2$ & $2 \sigma^4 / n$ & $\frac{\nu_n}{\nu_n + 2} \sigma_n^2$ & $\frac{\nu_n}{\nu_n - 2} \sigma_n^2$ & $\frac{2 \nu_n^2}{(\nu_n - 2)^2(\nu_n - 4)} \sigma_n^4$
 \end{tabular}
 }
\caption{{\small Comparison of Bayesian estimates using a conjugate prios and ML estimates.}}
\label{table:comp_conjugate_bay_ml}
\end{table}


Let us first consider the parameter $\mu$.
We note that the posterior mean (and MAP) is given by $\mu_n =\frac{\kappa_0}{\kappa_0 + n}\mu_0 + \frac{n}{\kappa_0 + n}\bar{y}$, which forms a convex combination of the prior $\mu_0$ and the sample average $\bar{y}$, with weights given by the sample size and $\kappa_0$.
For any fixed $n$ this pulls our estimate of the posterior mean away from $\bar{y}$ and closer to $\mu_0$ (and vice versa).
Further, we can use the hyperparameter $\kappa_0$ to express our uncertainty in $\mu_0$ (or $\bar{y}$ for that matter).
Rewritting the posterior variance using the \emph{Laundau notation} we get $\sigma_n^2 / \kappa_n = \frac{n-1}{\nu_n \kappa_n} s^2 + \mathcal{O}(\frac{1}{\nu_n\kappa_n}) = \frac{n}{(\nu_0 + n)(\kappa_0 + n)} s^2 + \mathcal{O}(1/n^2)$.
As the sample size $n$ grows the information contained in the likelihood should dominate the prior.
We observe this phenomena as the approximate asymptotic behavior of the posterior resembles that of the maximum likelihood estimator.
First, as $n$ tends to infinity $\nu_n = \nu_0 + n$ tends to infinity and the t distribution becomes indistinguishable from a normal.
Second, as $n$ grows the posterior mean is dominated by $\bar{y}$. And at last, for large $n$ the posterior variance is accurately approximated by $\sigma^2 / n$.
We refrain from an analogous analysis for $\sigma^2$ and only note that similar results hold, as can be seen in table \ref{table:comp_conjugate_bay_ml}.

What is gained from using an informative prior here?
Using the conjugate prior from above we have four hyperparameters at hand to model our prior knowledge about the parameters.
These can be used to represent very detailed to very vague information.
In any case, we were able to see that as we collect more and more data, the likelihood dominates our results.
A clear advantage of an analytical derivation is that we know exactly how the prior influences the posterior.
However, we have also seen that even for this \emph{very} simple model, the derivation is far from obvious.
As we consider more complex models using more parameters we have to make more restrictive assumptions on the way we model our prior information, if an analytical analysis is even possible.
For this reason among others, in the next section we present a method which trades off the clarity of an analytical result for the generality of being able to combine near arbitrary priors with complex, possibly high-dimensional likelihoods.


\newpage
\subsection{Sampling From The Posterior}
In this section we consider approaches that allow us to characterize the posterior distribution in complex settings using sampling methods.

For the rest of this section let us assume that we observe data $z \in \mathbb{Z}$ and can compute the likelihood $p(z \mid \theta)$ and prior $p(\theta)$ for $\theta \in \Theta$.
As before, our goal lies in analyzing the posterior distribution given by $p(\theta \mid z) \propto p(z \mid \theta) p(\theta)$.
Unlike before however, we now consider cases where the posterior is highly complex or even non-existent in analytical form, which happens for example when the likelihood contribution stems from a algorithmic computational model.

Say we are somehow able to draw independent samples $\theta_1, \mydots, \theta_n$ from $p(\theta \mid z)$.
By independence we get the well known result $\frac{1}{n} \sum_i h(\theta_i) \overset{d}{\longrightarrow} \mathcal{N} \left( \Exp{h(\theta) \mid z}, \var{h(\theta) \mid z} / \sqrt{n} \right)$, under mild conditions on $h$ and $p(\theta \mid z)$.
As we are able to formulate many quantities of interest using expectations ---probability statements can be written as expectations--- and as we can approximate percentiles from a (large) sample, we should be able to adequately summarize the posterior distribution if we are able to draw (independent) samples from it.

In the subsequent paragraphs we will discuss efficient methods to sample from the posterior, even if we cannot compute the integration constant $\int p(z \mid \theta) p(\theta) \mathrm{d}\theta$.
We will see that these methods do \emph{not} produce independent but autocorrelated samples.
With this in mind, we follow the creational process of these methods and first state the assumptions which have to be satisfied by the sampling process in order to yield good properties as for example a central limit theorem for dependent samples.
Then we present the \emph{Metropolis-Hastings algorithm}, which creates samples that fulfill the above criteria.
At last we talk about cases in which the Metropolis-Hastings algorithm fails and what can be done instead.

\subsubsection*{Markov Chain Monte Carlo}

Say we are able to construct a \emph{Markov chain} with unique invariant distribution equal to the posterior distribution we want to sample from.
Given we know the transition kernel, Markov chains are very easy to simulate.
Hence, we could start a chain, let it run \emph{long enough} and at some point consider all subsequent realizations as draws from the posterior; this is the core idea of MCMC ---we defer questions regarding the creation of transition kernels which result in specific invariant distributions until next paragraph.
In practice we never know for sure when a chain is run \emph{long enough}.
In part 3 we present some measures that help during the application.
Here we do what statisticians do best: obsess over central limit theorems.
Under mild conditions we can get something similar to a law of large numbers for Markov chains (\textcolor{red}{see e.g. \citet{roberts2004}, fact 5}).
This tells us that if we run the chain forever, our average will eventually converge to the number we seek.
However, forever is a very long time.
That is why we focus on assumptions which admit a central limit theorem with the usual $\sqrt{n}$ convergence rate, as it allows us to make more rigorous statements about our confidence in the whereabouts of the estimator for large but finite samples.


\begin{remark}
  As is often the case, there are many different sets of assumptions that allow for a CLT.
  The following theorem presents two sets of assumptions which allow for the desired result.
  We remark that we will \emph{not} formally introduce all concepts and will provide only a heuristic explaination of the assumptions.
  This is due to the fact that Markov chain theory on general state spaces requires a good understanding of measure theory which we do not want to assume as a prerequisite.
  The interested reader is refered to \textcolor{red}{\citet{meynandtweedie09}}.
  %A \emph{Markov chain} on the \emph{state space} $\Theta$ is a sequence of random variables $\{X_t : t \geqslant 0\}$ which assume values in $\Theta$ and fulfill the Markov property, that is
  %\begin{align}
  %P(X_{t+1} \in A \mid X_0 = x_0,\mydots,X_t=x_t) = P(X_{t+1} \in A \mid X_t=x_t) \,,
  %\label{eq:markov_property}
  %\end{align}
  %for all $t \geqslant 0$ and for all measurable $A \subset \Theta$. We consider time-homogenous chains for which (\ref{eq:markov_property}) does not depend on $t$.
  %In this case we can characterize the chain by its \emph{transition kernel} $\mathbb{P}$ with $P(X_{t+1} \in A \mid X_t = x) = \mathbb{P}(x, A)$ for all $t \geqslant 0, x \in \Theta$ and for all measurable $A \subset \Theta$.
  %That means $\mathbb{P}(x, A)$ gives the probability of the chain falling into $A$ given it starts at point $x$ in the previous period. Note that if we work on a subset of $\reals^d$ the \emph{Radon-Nikodym derivate} with respect to \emph{Lebesgue measure} is the probility density function $\pi$ corresponding to $\mathbb{P}$.
\end{remark}


\begin{theorem}{(A Central Limit Theorem for Markov Chains).}
  Let $\{X_t: t \geqslant 0 \}$ be a positive Harris Markov chain with invariant distribution $\pi$.
  Let $h$ be measurable with $\int h^2 \mathrm{d}\pi < \infty$.
  Assume either of the following holds:
  \begin{enumerate}
    \item $\{X_t: t \geqslant 0\}$ is uniformly ergodic,
    \item $\{X_t: t \geqslant 0\}$ is $\pi$-reversible and geometrically ergodic.
  \end{enumerate}
  Then, there exists a constant $\sigma^2(h) < \infty$ such that
  \begin{align}
    \sqrt{t}\left(\frac{1}{t}\sum_t h(X_t) - \int h \mathrm{d}\pi \right) \overset{d}{\longrightarrow} \normal{0, \sigma^2(h)} \,.
  \end{align}
\end{theorem}
\begin{proof}
  See \textcolor{red}{\citet{cogburn1972}} for 1 and \textcolor{red}{\citet{roberts1997}} for 2.
\end{proof}
\textcolor{red}{Extend paragraph with heuristic explaination of assumption.}

\subsubsection*{Metropolis-Hastings Algorithm}
From the above we know that given a Markov chain with invariant distribution equal to the posterior distribution $p(\theta \mid z)$, we can treat the realizations of the chain as samples from the posterior, under some regularity conditions.
Here we consider one method which implicitly defines such a chain, namely the Metropolis-Hastings algorithm \textcolor{red}{(\citet{Metropolis1953}, \citet{hastings70})}.
For other approaches and more involved algorithms see for example \textcolor{red}{\citet{roberts2004}} or \textcolor{red}{\citet{liang10}}.

\begin{algorithm}
\caption{Metropolis-Hastings}\label{alg:metropolis-hastings}
\begin{algorithmic}[1]
  \Require $(\pi, q, T) =$ (target density, proposal density, number of samples to draw)
\State initialize $x_0$ with an arbitrary point from the support of $q$
\For{$t = 0,\mydots,T-1$}
  \State sample a canditate: $y \sim q(\cdot \mid x_t)$
  \State compute the acceptance probability: $\alpha(x_t, y) \gets \min\left\{ \ddfrac{\pi(y)}{\pi(x_t)} \ddfrac{q(y \mid x_t)}{q(x_t \mid y)}, 1\right\}$
  \State update the chain: $x_{t+1} \gets \begin{cases} y &\mbox{,with probability } \alpha(x_t, y)\\ x_t &\mbox{,with remaining probability} \end{cases}$
\EndFor{}
\State \textbf{return} $\{x_t : t = 1,\mydots,T\}$
\end{algorithmic}
\end{algorithm}

\noindent
Algorithm \ref{alg:metropolis-hastings} displays the Metropolis-Hastings algorithm.
Line 4 shows why we can use this algorithm with the unnormalized posterior, as the integration constant cancels out in the first fraction.
For different settings different proposal distributions are appropriate.
A common choice are so called \emph{random walk} proposals which add some random number to the current position of the chain; for example a gaussian random walk proposal is given by $q(\cdot \mid x_t) = \normal{\cdot \mid x_t, \sigma^2}$ or equivalently stated $y = x_t + \normal{0, \sigma^2}$.
If the resulting chain has an unique invariant distribution we only know that after some time the chain starts behaving accordingly.
Therefore in practice we choose $T = B + T^*$ and drop the first $B$ samples, where $B$, the so called \emph{burn-in} samples, is large and $T^*$ represents the actual size of samples we want to draw.
See \textcolor{red}{\citet{sherlock2010}} for a recent survey on random walk proposals.

The simplicity of the algorithm is remarkable, but the main question of concern is if the resulting Markov chain inherits favorable properties.
\textcolor{red}{Need to list what properties can be derived in general and what can only be derived for specific proposal densities.}

The ability to sample draws in complex settings using the Metropolis-Hastings algorithm (and other Markov chain Monte Carlo methods for that matter) made Bayesian statistics applicable for real problems.
Still, \textcolor{red}{\citet{Au2001EstimationOS}} show that the classical Metropolis-Hastings algorithm is highly dependent on the proposal density and fails in higher dimensions; \textcolor{red}{\citet{zuev08}} provide a geometric intution.
The following paragraph illustrates one of the problems of working in higher dimensions.

\subsubsection*{Volume in Higher Dimensions}
Classical MCMC methods can have too slow convergence rates.
In higher dimensions this might be due to probability mass being distributed very far from where it is expected.
In this paragraph we motivate this phenomena and in the following we present methods which utilize it.

Let $B_d$ denote the unit ball in $\reals^d$ and define $C_d$ as the smallest cube containing $B_d$.
We consider two questions.
First, how does the ratio $\nicefrac{\vol{B_d}}{\vol{C_d}}$ changes as $d$ increases.
And second, how does the ratio of probability mass distributed by a standard gaussian on these regions changes as $d$ increases.
Since closed form expressions of volumina of geometrical objects exist the first questions needs little work.
Similary we can easily compute $P(X \in C_d) = \left[\Phi(1) - \Phi(-1)\right]^d$, where $\Phi$ denotes the one-dimensional gaussian cumulative distribution function.
However, to compute $P(X \in B_d)$ we need to integrate over the unit ball with respect to a gaussian distribution, which is non-trivial.
For this reason we decide to report an upper bound, as this is sufficient for our motivation.
In particular we compute $\overline{P(X \in B_d)} := \sup_{\bm{x} \in B_d} \phi(\bm{x}) \vol{B_d} = \sup_{\bm{x} \in B_d} \phi(\bm{x}) \int_{B_d}1 \mathrm{d}\bm{x} \geqslant \int_{B_d} \phi(\bm{x}) \mathrm{d}\bm{x} = P(X \in B_d)$.
The results of these computations are depicted in table \ref{tab:vol_high_dim}.
We note that both ratios tend to zero very fast as $d$ increases.
With this phenomena in mind one has to be cautious when working in high-dimensional spaces, since the regions of interest, that is the regions containing non-negligible probability mass, might not be located where our low-dimensional intution says.
This idea is formalized by the \emph{Gaussian Annulus Theorem} (\textcolor{red}{\citet{blum2017foundations}; theorem 2.9}) which states, inter alia, that most probability mass lies within an annulus centered at the origin with an average distance to the origin of $\sqrt{d}$.


\begin{table}[ht]
\def\arraystretch{1.3}
\centering
 \begin{tabular}{c | c c c c c c c}
 $d$ & 1 & 2 & 3 & 5 & 7 & 10 & 15\\
 \hline
 ${\vol{B_d}} / {\vol{C_d}}$ & 1.00000 & 0.78540 & 0.52360 & 0.16449 & 0.03691 & 0.00249 & 0.00001\\
 ${\overline{P(X \in B_d)}} / {P(X \in C_d)}$ & 1.16874 & 1.07281 & 0.83589 & 0.35870 & 0.10995 & 0.01184 & 0.00012
 \end{tabular}
 \caption{Comparison of volume ratio of unit ball and cube, and probability ratio of gaussian falling in unit ball and cube for varying dimension $d$. Numbers are rounded to five decimal places.}
\label{tab:vol_high_dim}
\end{table}


\subsubsection*{Hamiltonian Monte Carlo}

We conclude our digression on Bayesian thinking by presenting \emph{Hamiltonian Monte Carlo}, a innovative Markov chain Monte Carlo method from the statistical physics literature which works in higher dimensions  \textcolor{red}{Duane, Kennedy et al. 1987; }.
There are of course multiple MCMC algorithms which work in higher dimensions with many more being actively developed.
Here we focus on Hamiltonian Monte Carlo as it is the main algorithm used in the probabilistic programming language STAN \textcolor{red}{CITE STAN}, which we will be using in our Monte Carlo study and application part.

\textcolor{red}{EXPLAIN HMC HERE.}
