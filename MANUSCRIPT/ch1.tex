\section{Bayesian Thinking and Estimation}
In this section we will introduce the core topics of Bayesian data analysis, and whenever possible compare proposed methods and results to their frequentist counterpart.
As we will see, Bayesian statistics differs from mainstream statistics on a fundamental level; Thus, we have to start there.

\textcolor{red}{What remains to be introduced:}
\begin{enumerate}
  \item Notation: $p(...)$
\end{enumerate}

\subsection{(Probabilistic) Modeling}
Before going further, let us first formalize our notion of stochastic modeling.
Imagine being interested in some \textit{phenomena}, for example the effect of bigger class sizes on school children performance.
Most phenomena cannot be observed directly and only manifest themself through some latent variable system.
We can still hope to learn about the phenomena by studying the observational process around it.
Clearly the way in which a phenomena reveals itself is dependent on its environment;
The observed effects for school-children in sub-saharan africa might look very different to the ones in central europe.
To derive sensible results from our analysis we need to postulate the existence of a \textit{true data generating process}, which captures the way in which the observational process adheres to the effects of the phenomena of interest given its environment.
We define this object as a probability distribution $p_0$ on the observational space $\mathbb{Z}$.
In this step we move from a model to a probabilistic model, in that we allow our system of interest to be influenced by randomness and not only be characterized by a deterministic process.
In practice $p_0$ is rarely known, therefore the challenge lies in recovering a distribution using the observed data which is as close as possible to $p_0$.
This is usually done by assuming that the true data generating process falls in some class of models, for example a linear model with normal errors.
Formally, we restrict our attention to a subset of potential observational processes $\mathbb{M}$ over the whole space of distributions on $\mathbb{Z}$.
The beauty of using a model class approach in the construction of $\mathbb{M}$ is that for each distribution $p \in \mathbb{M}$, we can find a parameterization $\theta$ in the configuration space $\Theta$; For example, the class of multivariate normal distributions is parameterized by its mean and covariance $(\mu, \Sigma) = \theta \in \Theta$.
The goal of all subsequent statistical analysis is then to utilize the observed data to determine the regions in $\Theta$ which are most consistent with $p_0$ and simultaneously to capture our uncertainty about these statements.
If $p_0 \in \mathbb{M}$ we can find a parameterization $\theta_0$ which corresponds to the true data generating process; naturally we seek estimators that determine regions close to $\theta_0$. If, however, $p_0 \not\in \mathbb{M}$ we enter the world of model misspecification which leads to all sorts of problems.
For everything that follows let us therefore make the omnipresent assumption that $p_0 \in \mathbb{M}$.

\subsection{Schools of Thought}
Next we discuss how the Bayesian and the classical mindset differ. In particular we focus on the predominant way of thinking for most of statistical history, \textit{frequentist statistics}. We do not aim at an exhaustive overview here nor do we presume that the individual statistician belongs to one and only one of the following categories.

\textbf{Frequentist.} The frequentist approach assumes that the true data generating process is completely specified by an unkown but fixed quantity $\theta_0 \in \Theta$.
\textcolor{red}{FOLLOWING IS NOT TRUE, MAKE IT CLEAR WHAT YOU MEAN.}
Either implicitly or explicitly we define the \textit{likelihood} $p(z; \theta)$ by modeling the observational process for $z \in \mathbb{Z}$ using a parameterization $\theta$.
The main challenges include finding a (point) estimator for $\theta_0$, quantifying the uncertainty of the estimate and testing hypothesis.
One fundamental idea which stretches over all these topics is the interpretation of probability as the limit of an infinite sequence of relative frequencies ---hence the name.
That is, the probability of an event happening is just the limit of the frequency of that event happening over infinitely many independent experiments.
What are the implications of this understanding of probability?
Many interesting questions do not provide us with a thought experiment in which we can consider an ever increasing sequence of experiments.
In these cases using probability is either trivial or lacking an indisputable interpretation.
As we use the mathematical rigor of probability theory in our formal derivations, we will obtain (mathematically) correct results; however, the interpretation of these results might be highly unintuitive ---for example consider confidence intervals.
Since $\theta_0$ is fixed all probability statements regarding this object are trivial, that is, either one or zero. This propagates to the problem of hypothesis testing.
All hypothesis are either true or false and therefore have probabilities of one or zero.
A hypothesis $H_0$ is rejected if conditional on $H_0$ being true the probability of observing the data in the given sample is lower than some threshold, i.e. $P(\text{data} \mid H_0) < \alpha$.
Note that this statement does not tell us anything about $H_0$ directly, but only about the data at hand.

\textbf{Bayesian.} The Bayesian approach also assumes that there may be a true data generating process specified by some (maybe fixed) quantity $\theta_0 \in \Theta$.
The main difference is their understanding of probability as a subjective quantification of uncertainty.
In this view one is not limited to assigning non trivial probability statements only to objects that appear random in sequential experiments.
We can see the direct utility of this liberation by considering a special case of Bayes theorem
\begin{align}
  p(\theta \mid \text{data}) = \frac{p(\text{data} \mid \theta) p(\theta)}{p(\text{data})} \propto p(\text{data} \mid \theta) p(\theta) \,,
\end{align}
which reads
\begin{align}
  posterior = \frac{likelihood \times prior}{evidence} \propto likelihood \times prior \,.
\end{align}
In the frequentist setting this is of no use, since the statement $p(\theta)$ is nonsensical ---remember that probabilistic statements about fixed quantities are meaningless from a frequentist perspective.
This already outlines the main criticism of Bayesian analysis: Where does the prior $p(\theta)$ come from?
With the scientific goal of objectivity in mind, many feel uneasy with results being dependent on a subjective choice of a prior.
In what follows we will embark on the Bayesian idea without providing much more fundamental criticism; nonetheless, when adequate we will consider the influence of different priors on the posterior.
To end this comparison, what then are the main tasks associated with a Bayesian analysis?
These can be categorized by (i) obtaining the posterior distribution (or something equivalent) and (ii) communicating the information held in the posterior.
The first consists of defining the likelihood and (additionally to the frequentist approach) constructing a prior distribution and afterwards combining those to compute the posterior.
This computation can sometimes be achieved analytically, but in most cases one has to rely on algorithms to obtain samples of the posterior.
The second part consists of plotting the marginal posterior distributions, computing expectations of the form $\Exp{h(\theta) \mid \text{data}}$ and testing hypothesis.
All of the above can be done independent of the posterior being available analytically or through samples.
A clear difference can be seen when considering hypothesis testing.
Sacrificing \textit{objectivity} allows us to answer the questions we usually want to ask:
\begin{align}
  P(H_0: \theta \in S \mid \text{data}) = \int_{\theta \in S} p(\theta \mid \text{data}) \mathrm{d}\theta \,.
\end{align}
In the subsequent paragraphs we will be mostly occupied with the first category, computing the posterior, with occasional remarks on the second; in particular, the approximation of expectations.

\subsection{Solving for the posterior analytically}
In this subsection we present the analytical derivation of the posterior distribution of mean and variance parameters in a univariate normal model for two priors.
We will compare the results to the appropriate classical method, namely maximum likelihood.

In both cases, let us assume that we observe an iid sample $y = (y_1, \mydots, y_n)$
with $y_i \sim \normal{\mu, \sigma^2}$.
Our interest lies in solving for the marginal posteriors $p(\mu \mid y)$ and $p(\sigma^2 \mid y)$.

To be precise, in a full bayesian analysis we assume $\theta = (\mu, \sigma^2)$ to be a random quantity, thus the correct statement should be $y_i \mid \theta \sim \normal{\mu, \sigma^2}$.
In situations where this conditional dependence is clear many writers will use the first notation.
Here we try to be as pedantic as possible to avoid any confusion and will therefore stick to the second notation.

As it will be of major importance in the subsequent sections we remind the reader of some probability distributions uncommon in the non-Bayesian world.

\begin{definition}{(Scaled inverse $\chi^2$ distribution).}
  Let $\nu > 0$ and $\tau^2 > 0$ be parameters representing degrees of freedom and scale, respectively. The family of \emph{scaled inverse $\chi^2$ distributions} is characterized by its probability density function, namely
  \begin{align}
    p(x) \propto x^{-(1 + \nu / 2)} \EXP{\frac{-\nu \tau^2}{2 x}} \quad \text{for} \, x \in (0, \infty) \,,
  \end{align}
  where the constant of integration is ignored for clarity.
  We write $X \sim \scaledInvChi{\nu, \tau^2}$ to denote that the random variable $X$ follows a scaled inverse $\chi^2$ distribution with parameters $\nu$ and $\tau^2$.
\end{definition}

\textcolor{red}{
\begin{definition}{(Normal scaled inverse $\chi^2$ distribution).}
\end{definition}
}

\subsubsection*{Uninformative Prior}
We start our first Bayesian analysis by considering a prior which contains virtually no information.
This results in an analysis being mainly, if not completely, driven by the likelihood.
A common assumption is independence of the individual priors, that is $p(\theta) = p(\mu, \sigma^2) = p(\mu) p(\sigma^2)$.
A natural choice of declaring full ignorance of prior information is to assign a prior over the complete domain of the random parameter. For our case this mean $p(\mu) \propto 1$.
We note that this does not define proper proability distribution, which will not matter in this case but can lead to problems in others.\footnote{LINK TO PAPER}
Since the variance is restricted to be positive we impose a uniform prior on the log-transform thereof: $p(\log \sigma) \propto 1$.
This leads to the improper prior\footnote{\textcolor{red}{See appendix for a derivation.}}
\begin{align}
  p(\mu, \sigma^2) \propto (\sigma^2)^{-1} \,.
\end{align}
As usual the likelihood is given by
\begin{align}
  p(y \mid \mu, \sigma^2) \propto (\sigma^2)^{-n/2} \EXP{-\frac{1}{2\sigma^2}\sum_i (y_i - \mu)^2} \,,
\end{align}
where we dropped all proportionality constants.
Using the above we can apply Bayes theorem to yield
\begin{align}
  p(\mu, \sigma^2 \mid y) &\propto p(y \mid \mu, \sigma^2) p(\mu, \sigma^2)\\
  &\propto (\sigma^2)^{-(n+2)/2} \EXP{-\frac{1}{2\sigma^2}\sum_i (y_i - \mu)^2} \,.
\end{align}
From here we can derive the marginals by integrating out the respective other parameter. This is formalized in the following two proposition.

\begin{proposition}
  Under the uniform prior from above we find
  \begin{align}
    \mu \mid y &\sim t_{n-1}(\bar{y}, s^2/n) \,,\\
    \sigma^2 \mid y &\sim \scaledInvChi{n-1, s^2} \,,
  \end{align}
  where $s^2 = \frac{1}{n-1} \sum_i (y_i - \bar{y})^2$ denotes the sample variance and $\bar{y} = \frac{1}{n} \sum_i y_i$ the sample mean.
\end{proposition}
\begin{proof}
See appendix.
\end{proof}

Having derived the marginal posterior distributions, we can compare the results to their maximum likelihood (ML) counterpart.
Since the ML estimates $\left(\argmax{\theta \in \Theta}p(y \mid \theta)\right)$ are point estimates, we consider the similar \emph{maximum a posteriori} (MAP) estimate $\left(\argmax{\theta \in \Theta} p(\theta \mid y)\right)$, as well as the posterior mean and variance. All results are summarized in table \ref{table:comp_uniform_bay_ml}.

\begin{table}[ht]
\centering
\def\arraystretch{1.3}
{\small
 \begin{tabular}{c c c c c c}
 Parameter & ML Estimate & ML Variance & MAP & Posterior Mean & Posterior Variance\\[0.5ex]
 \hline
 $\mu$ & $\bar{y}$ & $\sigma^2/n$ & $\bar{y}$ & $\bar{y}$ & $s^2 / n$\\
 $\sigma^2$ & $\frac{n-1}{n} s^2$ & $2 \sigma^4 /n$ & ? & $\frac{n-1}{n-3} s^2$ & $\frac{2 (n-1)^2}{(n-3)^2 (n-5)} s^4$\\
 \end{tabular}
 }
\caption{\small {Comparison of Bayesian estimates using an uninformative prior and ML estimates. See appendix for a detailed derivation.}}
\label{table:comp_uniform_bay_ml}
\end{table}


\subsubsection*{Conjugate Prior}
We have seen that using an uninformative prior leads to results that are very similar to the ones obtained by a ML approach.
In case information on the parameters is available prior to observing the data we can utilize this fact by properly modeling the prior distribution.
Since we are interested in analytical results in this section,b we cannot mix any prior with any likelihood, as the product might not be of known form.
This leads us to the class of \emph{conjugate priors}.

\begin{definition}{(Conjugate prior).}
Let the likelihood $p(y \mid \theta)$ be given and assume that the prior distribution $p(\theta)$ is a member of some family $\mathcal{F}$ of probability distributions. We say that $p(\theta)$ is a \emph{conjugate prior} if $p(\theta \mid y)$ is also a member of $\mathcal{F}$.
\end{definition}

Conjugate priors were of particular importance in the early stages of Bayesian statistics since these give the practitioner certainty that the posterior follows a distribution which is known and computable. Moreover, nowadays we still see conjugate priors in use as they allow for a full or partial analytical derivation, which increases the accuracy of results or shortens the runtime of programs. For more complex models however conjugate priors can become too restrictive. We discuss solutions to this problem in the next section.

Consider again the likelihood but written to demonstrate its dependence on $\mu$ and $\sigma^2$
\begin{align}
  p(y \mid \mu, \sigma^2) \propto (\sigma^2)^{n/2} \EXP{-\frac{1}{2\sigma^2} n\left[ (\mu - \bar{y})^2 + (\bar{y^2} - \bar{y})^2 \right]} \,.
  \label{eq:likelihood2}
\end{align}
We want to construct a two dimensional prior for $(\mu, \sigma^2)$.
A theme to which we will be coming back is that modeling higher dimensional parameters by modeling many lower dimensional (sub)parameters using conditioning is often easier than modeling the complete distribution.
Here we utilize the equality $p(\mu, \sigma^2) = p(\mu \mid \sigma^2) p(\sigma^2)$.
By looking at the likelihood (equation \ref{eq:likelihood2}) we note that in order to \emph{not} change the inherent structual dependence on the parameters, $\mu \mid \sigma^2$ has to be distributed according to $\normal{\mu_0, \sigma^2 / \kappa_0}$ with so called \emph{hyperparameters} $\mu_0$ and $\kappa_0 > 0$.
Similary, we note that an informative prior for $\sigma^2$ has to respect the structure in which $\sigma^2$ appears in the likelihood.
We achieve this when $\sigma^2 \sim \scaledInvChi{\nu_0, \sigma_0^2}$ with hyperparameters $\nu_0$ and $\sigma_0^2 > 0$.
Following \textcolor{red}{GELMAN ET AL} we write $(\mu, \sigma^2) \sim \NormalscaledInvChi{\mu_0, \sigma_0^2 / \kappa_0; \nu_0, \sigma_0^2}$ with corresponding density function
\begin{align}
  p(\mu, \sigma^2) = p(\mu \mid \sigma^2) p(\sigma^2) \propto (\sigma^2)^{\frac{3 + \nu_0}{2}} \EXP{-\frac{1}{2\sigma^2} \left[\nu_0 \sigma_0^2 + \kappa_0(\mu_0 - \mu)^2 \right]} \,.
\end{align}

Multiplying the likelihood with our constructed prior we get the joint posterior (up to an integration constant)
\begin{align}
  p(\mu, \sigma^2 \mid y) \propto& (\sigma^2)^{-\frac{3 + n + \nu_0}{2}} \times\\
  & \times \EXP{-\frac{1}{2 \sigma^2} \left[ (\mu - \bar{y})^2 + (\bar{y^2} - \bar{y})^2 + \nu_0\sigma_0^2 + \kappa_0(\mu - \mu_0)^2 \right]} \,.
  \label{eq:conjugate_posterior}
\end{align}

\begin{proposition}
  The posterior distribution of $(\mu, \sigma^2) \mid y$, as given by the conditional density in equation \ref{eq:conjugate_posterior}, is $\NormalscaledInvChi{\mu_n, \sigma_n^2/\kappa_n; \nu_n, \sigma_n^2}$ distributed, where
  \begin{align*}
    \nu_n &= \nu_0 + n \,; \quad \kappa_n = \kappa_0 + n \,; \quad \mu_n =\frac{\kappa_0}{\kappa_0 + n}\mu_0 + \frac{n}{\kappa_0 + n}\bar{y} \,,\\
    \sigma_n^2 &= \left[\nu_0 \sigma_0^2 + (n-1)s^2 + \frac{\kappa_0 n}{\kappa_0 + n} (\bar{y} - \mu_0)^2\right] /\nu_n \,.
  \end{align*}
  \label{prop:posterior_conjugate}
\end{proposition}
\begin{proof}
  See appendix.
\end{proof}

Since the prior and the posterior are both normal scaled inverse $\chi^2$ distributed, we can speak of a conjugate prior.
Using the intermediate finding from proposition \ref{prop:posterior_conjugate} we can derive the main result of this section.

\begin{proposition}
  The marginal posterior distributions are given by
  \begin{align*}
    \mu \mid y &\sim t_{\nu_n}(\mu_n, \sigma_n^2 / \kappa_n)\\
    \sigma^2 \mid y &\sim \scaledInvChi{\nu_n, \sigma_n^2}\,,
  \end{align*}
  where $\nu_n, \sigma_n^2, \mu_n$ and $\kappa_n$ are as in proposition \ref{prop:posterior_conjugate}.
\end{proposition}
\begin{proof}
  See appendix.
\end{proof}

\begin{table}[ht]
\centering
\def\arraystretch{1.3}
{\small
 \begin{tabular}{c c c c c c}
 Parameter & ML Estimate & ML Variance & MAP & Posterior Mean & Posterior Variance\\
 \hline
 $\mu$ & $\bar{y}$ & $\sigma^2 / n$ & $\mu_n$ & $\mu_n$ & $\sigma_n^2 / \kappa_n$\\
 $\sigma^2$ & $\frac{n-1}{n}s^2$ & $2 \sigma^4 / n$ & $\frac{\nu_n}{\nu_n + 2} \sigma_n^2$ & $\frac{\nu_n}{\nu_n - 2} \sigma_n^2$ & $\frac{2 \nu_n^2}{(\nu_n - 2)^2(\nu_n - 4)} \sigma_n^4$
 \end{tabular}
 }
\caption{{\small Comparison of Bayesian estimates using a conjugate prios and ML estimates.}}
\label{table:comp_conjugate_bay_ml}
\end{table}


Let us first consider the parameter $\mu$.
We note that the posterior mean (and MAP) is given by $\mu_n =\frac{\kappa_0}{\kappa_0 + n}\mu_0 + \frac{n}{\kappa_0 + n}\bar{y}$, which forms a convex combination of the prior $\mu_0$ and the sample average $\bar{y}$, with weights given by the sample size and $\kappa_0$.
For any fixed $n$ this pulls our estimate of the posterior mean away from $\bar{y}$ and closer to $\mu_0$ (and vice versa).
Further, we can use the hyperparameter $\kappa_0$ to express our uncertainty in $\mu_0$ (or $\bar{y}$ for that matter).
Rewritting the posterior variance using the \emph{Laundau notation} we get $\sigma_n^2 / \kappa_n = \frac{n-1}{\nu_n \kappa_n} s^2 + \mathcal{O}(\frac{1}{\nu_n\kappa_n}) = \frac{n}{(\nu_0 + n)(\kappa_0 + n)} s^2 + \mathcal{O}(1/n^2)$.
As the sample size $n$ grows the information contained in the likelihood should dominate the prior.
We observe this phenomena as the approximate asymptotic behavior of the posterior resembles that of the maximum likelihood estimator.
First, as $n$ tends to infinity $\nu_n = \nu_0 + n$ tends to infinity and the t distribution becomes indistinguishable from a normal.
Second, as $n$ grows the posterior mean is dominated by $\bar{y}$. And at last, for large $n$ the posterior variance is accurately approximated by $\sigma^2 / n$.
We refrain from an analogous analysis for $\sigma^2$ and only note that similar results also hold, as can be seen in table \ref{table:comp_conjugate_bay_ml}.

What is gained from using an informative prior here?
Using the conjugate prior from above we have four hyperparameters at hand to model our prior knowledge about the parameters.
These can be used to represent very detailed to very vague information.
In any case, we were able to see that as we collect more and more data, the likelihood dominates our results.
A clear advantage of an analytical derivation is that we know exactly how the prior influences the posterior.
However, we have also seen that even for this \emph{very} simple model, the derivation is far from obvious.
As we consider more complex models using more parameters we have to make more restrictive assumptions on the way we model our prior information, if an analytical analysis is even possible.
For this reason among others, in the next section we present a method which trades off the clarity of an analytical result for the generality of being able to combine near arbitrary priors with complex, possibly high-dimensional likelihoods.


\newpage
\subsection{Sampling from the posterior}
In this section we consider approaches that allow us to characterize the posterior distribution in complex settings using sampling methods.

For the rest of this section let us assume that we are able to compute the value of the likelihood $p(z \mid \theta)$ for $\theta \in \Theta$ given some observed data $z \in \mathbb{Z}$, and that we have constructed a prior $p(\theta)$ on $\Theta$.
As before, our goal lies in summarizing the information contained in the posterior $p(\theta \mid z) \propto p(z \mid \theta) p(\theta)$; however, here we consider the case in which we are unable to derive the posterior via an analytical approach.

What happens if we are still able to draw samples from the posterior?
Can we adequately summarize $p(z \mid \theta)$ using samples alone?
In one or two dimensions an efficient practice is to plot the posterior.
In higher dimensions we cannot plot the complete posterior but only the marginal posteriors.
Of course if we are able to draw arbitrarily many samples from the posterior we can approximate the density using a histogram.
Are there quantities we cannot approximate?
It turns out that using independent samples $\theta^{1},\theta^{2},\mydots$ from $p(\theta \mid z)$ the central limit theorem holds, so that
\begin{align}
  \frac{1}{B} \sum_b h(\theta^{b}) \overset{a}{\sim} \mathcal{N} \left( \Exp{h(\theta) \mid z}, \var{h(\theta) \mid z} / \sqrt{B} \right) \,,
\end{align}
for some arbitrary measurable real-valued function $h$.
Remembering that probabilities can be represented as expectations this allows us to estimate a wide range of quantities from the posterior, if we are able to draw (independent) samples from it.

In the subsequent paragraphs we will discuss efficient methods to sample from the posterior even if we cannot compute the integration constant $\int p(z \mid \theta) p(\theta) \mathrm{d}\theta$.
We will see that these methods do \emph{not} produce independent but autocorrelated samples.
Therefore we end this section by stating the assumptions necessary for a central limit theorem to hold in the case of autocorrelated samples.

\subsubsection*{Markov Chain Monte Carlo}

\subsubsection*{Volume in Higher Dimensions}
Let $S_d$ and $C_d$ denote the unit sphere and unit cube in $\reals^d$, respectively.
We consider two questions.
First, how does the ratio $\frac{\vol{S_d}}{\vol{C_d}}$ changes as $d$ increases.
And second, how does the ratio of probability mass distributed by a standard gaussian on these regions changes as $d$ increases.  


\subsubsection*{Hamiltonian Monte Carlo}

\textbf{Foundations of Hamiltonian Monte Carlo.}
Using random walk metropolis or similar guess and verify algorithms is too costly
from a computational perspective in higher dimensions.

Question: How can we use the geometry of the typical set to get information
on how to move through it?
Answer: For continuous spaces we could have a vector field which is aligned with the typical set?
Starting at some point in the typical set we would only have to move in the direction with
the given momentum as given by the vector at this point and would again land in the typical set with a new vector.

But this defers the question only to another question: How do we get a vector field
which is aligned with the typical set?

A usual starting point for question of this nature is looking at the implied differential structure of the target.
This we get via the gradient. In particular, the gradient defines a vector field in the given space which is
sensitive to the structure of the target in a way s.t. it points us to the extrema.
But as we have seen above, the extrema (modes) are not necessarily where we expect
a lot of probability mass. In fact, we've seen that the higher the dimension the more
mass is concentrated exactly around some region centered at the mode.
A clever analogy from physics can help us out here. Think of the modes as centers of gravity, for example a planet.
The typical set floats around the planet. With higher dimensions we've observed that
the orbit tends to be farther away from the planet. That is, we want to place an object
in the space, such that it does not come to close to the planet but also does not drift away.
In particular we need to give the object a certain velocity so that the gravitational (gradient)
vector field keeps the object at a steady distance to the center (on the typical set).
In our probabilistic setting this means that we have to expand the original probabilistc system with
an auxiliary momentum (velocity) parameter.

\textbf{Phase space and Hamilton's Equations.}

\subsubsection*{Central Limit Theorem for Markov Chains}

\newpage
These quantities can be very different objects.
For instance, we might be interested in visualizing the marginal posterior distribution
of some specific parameter, or we want to propagate the posterior distribution
through some not necessarily linear transformation function.
In Bayesian and frequentist statistics alike, a common approach of summarizing
results of this kind is to compute expectations.

In the following we will review methods to draw samples from the posterior
distribution which, as we will see, goes hand in hand with approximating expectations.
To illustrate potential problems of approximating expectations in higher dimensions,
we will first consider the case in which we are equipped with an unimodal normalized posterior density.

Let $p$ be the target density on some smooth space $\mathrm{Q}$.
Let $f$ be some function.
The quantity of interest is given by the corresponding expectation
\begin{align}
    \mathbb{E}_p \left[f\right] = \int_{\mathrm{Q}} f(q) p(q) \mathrm{d}q \,.
\end{align}
For complex models we are usually not able to evaluate these integrals analytically,
therefore we have to approximate the integral numerically.
It is clear that for higher dimensions we run into problems when using naive
quadrature methods which is known as the \emph{curse of dimensionality}.
In particular when considering high-dimensional spaces most regions will not
contribute to the expectation. This can have three causes. One, the region has a
negligible density $p(q)$; two, the function has a negligible value $f(q)$;
and three, the volume $\mathrm{d}q$ is negligible. Since we care about general
methods that can be applied to various functions $f$ we will assume from here on
that $f$ has non-negligible values on the whole space.\footnote{Still, for specific applications
in which only one expectation is relevant we can make use of the functional form of
$f$.}
Using the intuition from above we are interested in examining the regions of the
space in more detail for which the density has a significant impact and the
volume is non-negligible. We will call this set the typical set.\footnote{\textcolor{red}{REEFFEEERENCE.}}
To gain more intuition on how this set looks we will consider the normal density
for various dimensions. Divide the space into boxes with
side length 4 in such a way that we have one box centered at the mode of the
density and one more adjacent box at each side. I.e., in one dimensions we
find 3 boxed, in two dimensions we find 9 boxes and so on. Now for each dimension
we compute the probability mass falling into the box containing the mode and the
boxes surrounding the mode. For the normal case we can compute these values exactly.

\begin{lstlisting}
from scipy.stats import norm
rv = norm(loc=0, scale=1)
def inner_volume(dim):
  return (rv.cdf(2)-rv.cdf(-2))**dim
def outer_volume(dim):
  return (rv.cdf(6)-rv.cdf(-6))**dim-(rv.cdf(2)-rv.cdf(-2))**dim
\end{lstlisting}


\begin{table}
\centering
 \begin{tabular}{|l l l|}
 \hline
 Dimension & Inner Volume & Outer Volume \\ [0.5ex]
 \hline\hline
 1 & 0.9544997361036416 & 0.045500261923183016 \\
 2 & 0.9110697462219214 & 0.08893024983172781 \\
 5 & 0.7922806756813302 & 0.2077193144527928 \\
 10 & 0.6277086690580651 & 0.3722913112101811 \\
 50 & 0.0974519722529203 & 0.9025479290883144 \\
 100 & 0.009496886895983958 & 0.9905029157864953 \\ [1ex]
 \hline
 \end{tabular}
 \caption{Comparison of volume close to mode and in the regions around the mode for different dimensions.}
\label{tab:volume}
\end{table}

Table \ref{tab:volume} shows the probability mass contained in the inner box centered
at the mode in comparison to the aggregated probability mass of the boxes surrounding
the inner box. As the dimensionality of our problem increases we observe that more
and more mass is concentrated not in the regions where the density is highest, around the mode,
but in the its adjacent neighborhoods. When constructing estimation techniques we
have to account for this counter-intuitive behavior. In particular, we are intersted in
exploring some sub-manifold of the relevant space which contains the non-negligible
information on the expectation which can be defined to be invariant to the dimensionality of the problem, i.e. the typical set.

\subsubsection{Markov Chain Monte Carlo}
Next we present a simple class of estimators which produce samples from the posterior
distribution by constructing a \emph{Markov Chain} that has the posterior distribution
as its stationary distribution.

Let $T(q', q)$ denote the markov transition kernel. If we can find a $T$ s.t.
\begin{align}
  p(q) = \int_{\mathrm{Q}} T(q', q) p(q') \mathrm{d}q' \,,
  \label{eq:markov_int}
\end{align}
then the markov chain will have $p$ as its limiting distribution.\textcolor{red}{PROOOOOF}.
The intuition behind equation \ref{eq:markov_int} is that if we sample from the
target distribution and apply the transition kernel, we also want the new ensemble of
samples to be distributed accoding the target distribution. But how do we generate
actual samples?


Let us assume we can draw samples $\{q_0, \mydots, q_N\}$ from the posterior distribution through following
the markov chain. Then a straightforward estimator is $\hat{f}_N := \frac{1}{N} \sum_i f(q_i)$.
Under some conditions \textcolor{red}{WHAT ARE THEEESE??}, we get
\begin{align}
  \hat{f}_N \overset{\mathrm{p}}{\longrightarrow} \mathbb{E}_p(f) \,.
\end{align}
Unfortunately this only tells us something about the limit. To learn more about the
finite sample behavior we can make use of a central limit theorem for MCMC estimators.
Under ideal circumstances we get
\begin{align}
  \hat{f}_N^{MCMC} \overset{a}{\sim} \normal{\mathbb{E}_p(f), \text{MCMC-SE}} \,,
  \label{eq:mcmc_clt}
\end{align}

where $\text{MCMC-SE} = \sqrt{\text{Var}_p(f) / \text{ESS}}$ with ESS denoting the effective sample size.
ESS can be estimated via $\hat{\text{ESS}} = N / (1 + 2 \sum_{l=1}^\infty \rho_l)$, where $\rho_l$ represents
the autocorrelation of lag $l$ of the simulated markov chain.

Since we start the markov chain with arbitrary starting values it can take some
steps until the chain meanders through the relevant regions. Therefore in practice
we throw away the first few hundred samples. This is known as \emph{warm up} or
\emph{burn-in} in the literature.

What are the \emph{ideal} circumstances so that equation \ref{eq:mcmc_clt} holds?
A sufficient condition is given by the assumption of \emph{geometric ergodicity}.
\textcolor{red}{WHERE WHERE WHERE??}.

\subsubsection*{Metropolis-Hastings Algorithm}
The Metropolis-Hastings algorithm provides us with a way to jump from on point
in a space to another using the established stochastic structure of the density
from which we are drawing samples and the transition kernel. The transition kernel
can be thought of as a proposal density. Given $q$ we propose a draw $q'$ from $T(q' \mid q)$.
The idea of the algorithm is that we accept this new point only with some probability $a$,
where
\begin{align}
  a(q' \mid q) = \min \left (1, \frac{T(q\mid q') p(q')}{T(q' \mid q) p(q)} \right) \,.
\end{align}
Hence with some starting value $q_0$, the target density $p$ and some transition kernel $T$
we can simulate a markov chain which will at some point produce points that resemble draws
from $p$. However, in higher dimensions this is still not enough, as the estimator does
not scale and becomes highly inefficient \textcolor{red}{REFERERERECNCE}.







\subsection{APPENDIX}
\textbf{Uninformative prior}
Thus the marginal posterior of $\sigma^2$ can be obtained by integrating the joint posterior
over $\theta$. Let $s^2 = \frac{1}{n-1} \sum_i (y_i - \bar{y})^2$ denote the sample
variance and note that it is easy to show to
$\theta \mid \sigma^2, y \sim \normal{\bar{y}, \sigma^2/n}$ (see appendix for a proof).
Then,
\begin{align}
  p(\sigma^2 \mid y) &\propto \int p(\theta, \sigma^2 \mid y) \mathrm{d} \theta\\
  &\propto \int \sigma^{-(n+2)} \EXP{-\frac{1}{2\sigma^2}\sum_i (y_i - \theta)^2} \mathrm{d} \theta\\
  &= \sigma^{-(n+2)} \int \EXP{-\frac{1}{2\sigma^2}\sum_i (y_i - \theta)^2} \mathrm{d} \theta\\
  &= \sigma^{-(n+2)} \int \EXP{-\frac{1}{2\sigma^2}[(n-1) s^2 + n(\bar{y} - \theta)^2]} \mathrm{d} \theta\\
  &= \sigma^{-(n+2)} \EXP{-\frac{1}{2\sigma^2}[(n-1) s^2]} \int \EXP{\frac{1}{2\sigma^2/n}(\bar{y} - \theta)^2} \mathrm{d} \theta\\
  &= \sigma^{-(n+2)} \EXP{-\frac{1}{2\sigma^2}[(n-1) s^2]} \sqrt{2 \pi \sigma^2 / n} \\
  &\propto (\sigma^2)^{-(n+1)/2} \EXP{-\frac{1}{2\sigma^2}[(n-1) s^2]} \,.
\end{align}
Hence, $\sigma^2 \mid y \sim \text{scaled-Inv-} \chi^2(n-1, s^2)$.

To finish our analysis we integrate the joint posterior over $\sigma^2$ to get
the marginal posterior of $\theta$. We evaluate the integral by substitution using
$z = \frac{a}{2 \sigma^2}$ with $a = (n-1)s + n(\theta - \bar{y})$.\footnote{\textcolor{red}{See appendix for explicit derivation}}
Then,
\begin{align}
  p(\theta \mid y) &= \int_{(0, \infty)} p(\theta, \sigma^2 \mid y) \mathrm{d}\sigma^2\\
  &\propto \sigma^{-(n+2)} \int_{(0, \infty)} \EXP{-\frac{1}{2\sigma^2}[(n-1) s^2 + n(\bar{y} - \theta)^2]} \mathrm{d} \sigma^2\\
  &\propto a^{-n/2} \int_{(0, \infty)} z^{(n-2)/2}\EXP{-z} \mathrm{d}z\\
  &\propto a^{-n/2}\\
  &= [(n-1)s + n(\theta - \bar{y})]^{-n/2}\\
  &\propto \left[1 + \frac{(\theta - \bar{y})^2}{(n-1) s^2 / n}\right]^{-n/2} \,
\end{align}
which concludes our first analysis implying that $\theta \mid y \sim t_{n-1}(\bar{y}, \sigma^2/n)$.




We know that for the problem at hand the standard maximum likelihood estimators
and their variances are given by\footnote{\textcolor{red}{See appendix for proof.}}
\begin{align}
  \hat{\theta}_{ML} &= \frac{1}{n} \sum_i y_i = \bar{y}\,\\
  \hat{\sigma}_{ML}^2 &= \frac{1}{n} \sum_i (y_i - \bar{y})^2 = \frac{n-1}{n} s^2\,\\
  \mathrm{I}(\theta, \sigma)^{-1} &= \sqmat{\sigma^2 / n & 0\\0&\sigma^2/(2n)} \,.
\end{align}
The Bayesian counterpart to the ML-Estimator is the \emph{maximum a posteriori estimate},
or in short \emph{MAP}. For $\theta \mid y$ we derived a noncentral Student's t-distribution
with mean $\bar{y}$ and variance $\sigma^2/n$. Since this distribution is unimodal and
symmetric the MAP estimate is simply the mean. We note that it is equivalent to
the ML estimate on both the point estimate and included variance. For $\sigma^2$
the results look slightly different. The mode and the mean of $\sigma^2 \mid y$
are given by $\frac{n-1}{n+1} s^2$ and $\frac{n-1}{n-3} s^2$, respectively.
Still we see a very close resemblance of the Bayesian estimator to the ML estimator.
One could say that this is a property which is desirable for Bayesian estimators
using uninformative priors. One obvious advantage of the Bayesian approach is the
ease with which we can compute arbitrary probabilities using the posterior density.


\textbf{Conjugate prior}
