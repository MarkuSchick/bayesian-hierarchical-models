\section{Bayesian Thinking}
\subsection{(Probabilistic) Modeling}
\begin{enumerate}
  \item What is Statistics? What is a model? Why are models useful? \\
    	Want to model some \emph{phenomena} which manifests itself in some latent variable system / observational process to learn how it works. What is an observation? What is an environment of the latent system?
  \item True Data Generating Process (What even is that)? (Define it as a probability distribution on the observational space: $p^1$.
  \item The Observational Model (Which we assume; Is some subset $\mathbf{S} \subset \mathbf{P}$, where $\mathbf{P}$ is the set of all probability distributions on the observational space $Y$. (Because $\mathbf{P}$ is too big.)
  \item Each $s \in \mathbf{S}$ defines a data generating process, and in doing so provides a (formal) narrative on how the observed data might have been generated.
  \item In practice we define the elements of $\mathbf{S}$ via a parameterization. So that we have a one-to-one mapping from $\mathbf{S}$ to some parameter configuration space $\Theta \subset \mathbf{R}^K$.
  \item Once we identified $\mathbf{S}$ and the mapping to $\Theta$ we want to identify the configuration which is most consistent with the true underlying data generating process $p^1$. Since we do not know $p^1$ this comes down to finding the narrative that best explains the observed data and adheres to domain knowledge on the given application. (But what does consistency even mean?)
\end{enumerate}

\subsection{Frequentist Inference}
\begin{enumerate}
  \item What is a probability anyways? Define it as the limiting frequency of an event happening in infinitely many repetions of the same experiment. Realistic? Formally use Kolmogorov's axioms. So what is allowed to be stochastic in this framework? What is the source of variability in models? Model configuration parameters are constants. (? said where?)
\end{enumerate}

\subsection{Bayesian Inference}
\begin{enumerate}
  \item Can we also put weights on different model configurations prior to observing data? Humans (therefore also scientiest) certainly do; although sometimes subconciously. We want to formalize this by allowing probability distributions on the configurations space, \emph{priors}.
  \item What changes? Before we had some space $\mathbf{S}$ and parameterization $\theta \in \Theta$ which defined our observational model through a probability distribution (density) $p_{\mathbf{S}}(y; \theta)$ for $y \in Y$.
    Since we now allow $\theta$ to be stochastic we have to work with the conditional density that is, we model the stochastic relationship of the observations given a certain parameterization, i.e. $$p_{\mathbf{S}}(y \mid \theta) = p_{\mathbf{S}}(y; \theta)$$
  \item Why would we want to do this anyways? Bayes' Theorem!
    Given a distribution $p_{\mathbf{S}}(\theta)$ on the configuration space we can apply Bayes' Theorem. (State it here or later?)
  \item The likelihood function.$$\ell_{y} : \Theta \to \mathbf{R}_+, \theta \mapsto p_{\mathbf{S}}(y \mid \theta)$$
    The likelihood function maps model configurations to a numerical quantification which increases for model configurations which are more consistent with the data and decreases with configurations that are less consistent.
    Hence the likelihood function quantifies the relative consistency of each model configuration with the observed data.
  \item The posterior distribution. Applying Bayes' Theorem we get $$p_{\mathbf{S}}(\theta \mid y) = \frac{p_{\mathbf{S}}(y \mid \theta)}{\int p_{\mathbf{S}}(y \mid \theta) p_{\mathbf{S}}(\theta) \mathrm{d}\theta} p_{\mathbf{S}} (\theta) \propto p_{\mathbf{S}}(y \mid \theta) p_{\mathbf{S}}(\theta)$$
  \item The goal of analysis can be inference or prediction
    When being concerned with the former we would like to understand how the phenomena of interest interacts with the latent variable system and the observational process we measure, as this might give us insights into the phenomena itself.
    Having found a parameterization this means that we want to know which parameters are likely to cause the observed data.
    In the Bayesian setting we answer this question by construction the posterior distribution.
    That is, a conditional distribution on the configuration space given the observed data.
    The use of Bayes' Theorem (which makes this possible in the first place) explains the name.
    But it is not the application of Bayes' Theorem which makes Bayesian statistics different to classical (frequentist) statistics; it is the liberation of the model parameters, which are allowed to vary according to some prior distribution.
  \item We can interpret this statement as an updating process: we have beliefs on the model calibration parameter in the form of a prior distribution and we update this belief using the likelihood function.
    During this updating step three commom patterns can occur. (i) contraction (ii) containment (iii) compromise. [add pictures and gaussian example].
  \item Identification of model parameters.
    If we observe very informative data in the sense that the likelihood is concentrated around a small area then all vague priors will do fine and in a sense we let the data speak.
    If, however, the observational process was not sensitive to the phenomena of interest, we might observe data with a very low level of information regarding the model parameters.
    In this case we speak of weakly-identified parameters.
    This manifests in the likelihood dispersing over large regions of the configuration space.
    Choosing a prior careless in these situation can result in weak-identifibility of the likelihood propagating to the posterior.
  \item Okay now we have $p_{\mathbf{S}}(\theta \mid y)$ so what?
    Let $g$ be any function on $\Theta$.
    Compute $$\mathbf{E}\left[g(\theta) \mid y\right] = \int g(\theta) p_{\mathbf{S}}(\theta \mid y) \mathrm{d} \theta$$
    [Insert analytical gaussian exmaple here:]
    For very simple models with convenient assumptions we can compute the posterior density in closed-form. Using this we might even be able to compute the above integral for some functions $g$ analytically. For more complicated, i.e. realistic, models this does not work.
  \item For more complicated models we utilize the fact that for most questions we do not need the analytical form of the posterior but we are happy with being able to draw from it. If we are able to draw from the posterior correctly we can approximate quantiles and arbitrary expectations. But how do we draw from a density?
  \item Some blabla on how to draw from densities and the normalization constant and this is why there is \emph{Gibbs Sampling} and \emph{Metropolis Hasting Algorithm} and \emph{Monte Carlo Markov Chain} in general.
\end{enumerate}

\subsection{Solving for the posterior analytically}
In this subsection we present the analytical estimation of the mean and variance parameters
in a univariate normal model using an uninformative and conjugate prior, respectively.
We will compare the derived posteriors to the usual maximum likelihood estimate.

In both cases we assume that we observe an iid data sample $y = (y_1, \mydots, y_n)$
with $y_i \sim \normal{\theta, \sigma^2}$. Our interest lies in solving for the
marginal posterior distributions $p(\theta \mid y)$ and $p(\sigma^2 \mid y)$.
Since it will be use frequently, note that by dropping all irrelevant constants we get
\begin{align}
  p(y \mid \theta, \sigma^2) \propto (\sigma^2)^{-n/2} \EXP{-\frac{1}{2\sigma^2}\sum_i (y_i - \theta)^2} \,.
\end{align}

\textcolor{red}{ADD STUFF TO MAP AND MEDIAN ESTIMATE AND COMPARE TO MAXIMUM LIKELIHOOD!}

\subsubsection*{Uninformative Prior}
A reasonable choice for an uninformative prior for $(\theta, \sigma^2)$ stems
from defining a uniform prior on the transformed parameters, i.e. $p(\theta, \log \sigma) \propto 1$.
The log transformation is necessary since $\sigma$ is constrained to be positive.
This leads to the improper prior\footnote{\textcolor{red}{See appendix for a derivation.}}
\begin{align}
  p(\theta, \sigma^2) \propto (\sigma^2)^{-1} \,,
\end{align}
which gives the joint posterior as
\begin{align}
  p(\theta, \sigma^2 \mid y) &\propto p(y \mid \theta, \sigma^2) p(\theta, \sigma^2)\\
  &\propto (\sigma^2)^{-(n+2)/2} \EXP{-\frac{1}{2\sigma^2}\sum_i (y_i - \theta)^2} \,.
\end{align}

Thus the marginal posterior of $\sigma^2$ can be obtained by integrating the joint posterior
over $\theta$. Let $s^2 = \frac{1}{n-1} \sum_i (y_i - \bar{y})^2$ denote the sample
variance and note that it is easy to show to
$\theta \mid \sigma^2, y \sim \normal{\bar{y}, \sigma^2/n}$ (see appendix for a proof).
Then,
\begin{align}
  p(\sigma^2 \mid y) &\propto \int p(\theta, \sigma^2 \mid y) \mathrm{d} \theta\\
  &\propto \int \sigma^{-(n+2)} \EXP{-\frac{1}{2\sigma^2}\sum_i (y_i - \theta)^2} \mathrm{d} \theta\\
  &= \sigma^{-(n+2)} \int \EXP{-\frac{1}{2\sigma^2}\sum_i (y_i - \theta)^2} \mathrm{d} \theta\\
  &= \sigma^{-(n+2)} \int \EXP{-\frac{1}{2\sigma^2}[(n-1) s^2 + n(\bar{y} - \theta)^2]} \mathrm{d} \theta\\
  &= \sigma^{-(n+2)} \EXP{-\frac{1}{2\sigma^2}[(n-1) s^2]} \int \EXP{\frac{1}{2\sigma^2/n}(\bar{y} - \theta)^2} \mathrm{d} \theta\\
  &= \sigma^{-(n+2)} \EXP{-\frac{1}{2\sigma^2}[(n-1) s^2]} \sqrt{2 \pi \sigma^2 / n} \\
  &\propto (\sigma^2)^{-(n+1)/2} \EXP{-\frac{1}{2\sigma^2}[(n-1) s^2]} \,.
\end{align}
Hence, $\sigma^2 \mid y \sim \text{scaled-Inv-} \chi^2(n-1, s^2)$.

To finish our analysis we integrate the joint posterior over $\sigma^2$ to get
the marginal posterior of $\theta$. We evaluate the integral by substitution using
$z = \frac{a}{2 \sigma^2}$ with $a = (n-1)s + n(\theta - \bar{y})$.\footnote{\textcolor{red}{See appendix for explicit derivation}}
Then,
\begin{align}
  p(\theta \mid y) &= \int_{(0, \infty)} p(\theta, \sigma^2 \mid y) \mathrm{d}\sigma^2\\
  &\propto \sigma^{-(n+2)} \int_{(0, \infty)} \EXP{-\frac{1}{2\sigma^2}[(n-1) s^2 + n(\bar{y} - \theta)^2]} \mathrm{d} \sigma^2\\
  &\propto a^{-n/2} \int_{(0, \infty)} z^{(n-2)/2}\EXP{-z} \mathrm{d}z\\
  &\propto a^{-n/2}\\
  &= [(n-1)s + n(\theta - \bar{y})]^{-n/2}\\
  &\propto \left[1 + \frac{(\theta - \bar{y})^2}{(n-1) s^2 / n}\right]^{-n/2} \,
\end{align}
which concludes our first analysis implying that $\theta \mid y \sim t_{n-1}(\bar{y}, \sigma^2/n)$.

We know that for the problem at hand the standard maximum likelihood estimators
and their variances are given by\footnote{\textcolor{red}{See appendix for proof.}}
\begin{align}
  \hat{\theta}_{ML} &= \frac{1}{n} \sum_i y_i = \bar{y}\,\\
  \hat{\sigma}_{ML}^2 &= \frac{1}{n} \sum_i (y_i - \bar{y})^2 = \frac{n-1}{n} s^2\,\\
  \mathrm{I}(\theta, \sigma)^{-1} &= \sqmat{\sigma^2 / n & 0\\0&\sigma^2/(2n)} \,.
\end{align}
The Bayesian counterpart to the ML-Estimator is the \emph{maximum a posteriori estimate},
or in short \emph{MAP}. For $\theta \mid y$ we derived a noncentral Student's t-distribution
with mean $\bar{y}$ and variance $\sigma^2/n$. Since this distribution is unimodal and
symmetric the MAP estimate is simply the mean. We note that it is equivalent to
the ML estimate on both the point estimate and included variance. For $\sigma^2$
the results look slightly different. The mode and the mean of $\sigma^2 \mid y$
are given by $\frac{n-1}{n+1} s^2$ and $\frac{n-1}{n-3} s^2$, respectively.
Still we see a very close resemblance of the Bayesian estimator to the ML estimator.
One could say that this is a property which is desirable for Bayesian estimators
using uninformative priors. One obvious advantage of the Bayesian approach is the
ease with which we can compute arbitrary probabilities using the posterior density.

\subsubsection*{Conjugate Prior}
Consider again the likelihood function
\begin{align}
  p(y \mid \theta, \sigma^2) \propto \sigma^{-n} \EXP{-\frac{1}{2\sigma^2}a} \,.
\end{align}
A conjugate prior for $(\theta, \sigma^2)$ therefore has to be of the form
\begin{align}
  p(\theta, \sigma^2) \propto \sigma^\alpha \EXP{-\frac{1}{2\sigma^2} \beta} \,
\end{align}
for some $\alpha, \beta$. Using the factorization $p(\theta, \sigma^2) = p(\sigma^2) p(\theta \mid \sigma^2)$
we see that for some $\alpha_0, \alpha_1, \beta_0, \beta_1$ we find
\begin{align}
  p(\sigma^2) \propto \sigma^{\alpha_0} \EXP{-\frac{1}{2\sigma^2} \beta_0}
\end{align}
and
\begin{align}
  p(\theta \mid \sigma^2) \propto \sigma^{\alpha_1} \EXP{-\frac{1}{2\sigma^2} \beta_1} \,.
\end{align}
This then tells us that $\theta \mid \sigma^2$ is normal with variance proportional
to $\sigma^2$, while $\sigma^2$ has to be $\text{scaled-Inv-}\chi^2$ distributed.
We parameterize the distributions as follows
\begin{align}
  \theta \mid \sigma^2 &\sim \normal{\theta_0, \sigma^2/\kappa_0}\\
  \sigma^2 &\sim \text{scaled-Inv-}\chi^2(\nu_0, \sigma_0^2) \,,
\end{align}
for some suitable hyperparameters $\theta_0, \kappa_0, \nu_0, \sigma_0$ and follow
\textcolor{red}{Gelman BDA} in terming the resulting joint density
$p(\theta, \sigma^2)$ by $\text{Normal-Inv-}\chi^2(\theta_0, \kappa_0; \nu_0, \sigma_0)$.
This in turn ensures that the joint posterior $p(\theta, \sigma^2 \mid y)$ is again
$\text{Normal-Inv-}\chi^2$. Using an equivalent approach as applied above we can
compute the marginal posteriors by intregrating the respective parameters out.
This results in
\begin{align}
  \sigma^2 \mid y &\sim \text{Inv-}\chi^2(\nu_n, \sigma_n^2)\\
  \theta \mid y &\sim t_{\nu_n}(\theta_n, \sigma_n^2 / \kappa_n) \,,
\end{align}
for
\begin{align}
  \nu_n &= \nu_0 + n\\
  \kappa_n &= \kappa_0 + n\\
  \sigma_n^2 &= \left[\nu_0 \sigma_0^2 + (n-1)s^2 + \frac{\kappa_n n}{\kappa_0 + n} (\bar{y} - \theta_0)^2\right] /\nu_n\\
  \theta_n &=\frac{\kappa_n}{\kappa_0 + n}\theta_0 + \frac{n}{\kappa_0 + n}\bar{y} \,.
\end{align}
\textcolor{red}{A detailed derivation is given in the appendix.}


\subsection{Asymptotics}
Let $y = \{y_1,\mydots,y_n\}$.
\begin{enumerate}
  \item Assume the likelihood function is smooth and consider the maximum likelihood estimator $$\theta_{ML}(y) = \argmax{\theta \in \Theta} p_{\mathbf{S}}(y; \theta)$$
    If there is a $\theta^1 \in \Theta$ such that $p^1 = p_{\mathbf{S}}(;\theta^1)$ then under some minor assumption (what are theeez???) we get the well known result that $\theta_{ML}(y)$ converges in (prob, a.s., ...) to $\theta^1$ as $n \to \infty$.
\end{enumerate}


\subsection{Sampling from the posterior}
\subsubsection*{Markov Chain Monte Carlo Methods}
\begin{enumerate}
  \item Metropolis-Hastings Algorithm
\end{enumerate}
