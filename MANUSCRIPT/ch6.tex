\appendix
\section{Appendix}

\subsection{Propositions}
\textbf{Uninformative prior}
Thus the marginal posterior of $\sigma^2$ can be obtained by integrating the joint posterior
over $\theta$. Let $s^2 = \frac{1}{n-1} \sum_i (y_i - \bar{y})^2$ denote the sample
variance and note that it is easy to show to
$\theta \mid \sigma^2, y \sim \normal{\bar{y}, \sigma^2/n}$ (see appendix for a proof).
Then,
\begin{align}
  p(\sigma^2 \mid y) &\propto \int p(\theta, \sigma^2 \mid y) \mathrm{d} \theta\\
  &\propto \int \sigma^{-(n+2)} \EXP{-\frac{1}{2\sigma^2}\sum_i (y_i - \theta)^2} \mathrm{d} \theta\\
  &= \sigma^{-(n+2)} \int \EXP{-\frac{1}{2\sigma^2}\sum_i (y_i - \theta)^2} \mathrm{d} \theta\\
  &= \sigma^{-(n+2)} \int \EXP{-\frac{1}{2\sigma^2}[(n-1) s^2 + n(\bar{y} - \theta)^2]} \mathrm{d} \theta\\
  &= \sigma^{-(n+2)} \EXP{-\frac{1}{2\sigma^2}[(n-1) s^2]} \int \EXP{\frac{1}{2\sigma^2/n}(\bar{y} - \theta)^2} \mathrm{d} \theta\\
  &= \sigma^{-(n+2)} \EXP{-\frac{1}{2\sigma^2}[(n-1) s^2]} \sqrt{2 \pi \sigma^2 / n} \\
  &\propto (\sigma^2)^{-(n+1)/2} \EXP{-\frac{1}{2\sigma^2}[(n-1) s^2]} \,.
\end{align}
Hence, $\sigma^2 \mid y \sim \text{scaled-Inv-} \chi^2(n-1, s^2)$.

To finish our analysis we integrate the joint posterior over $\sigma^2$ to get
the marginal posterior of $\theta$. We evaluate the integral by substitution using
$z = \frac{a}{2 \sigma^2}$ with $a = (n-1)s + n(\theta - \bar{y})$.\footnote{\textcolor{red}{See appendix for explicit derivation}}
Then,
\begin{align}
  p(\theta \mid y) &= \int_{(0, \infty)} p(\theta, \sigma^2 \mid y) \mathrm{d}\sigma^2\\
  &\propto \sigma^{-(n+2)} \int_{(0, \infty)} \EXP{-\frac{1}{2\sigma^2}[(n-1) s^2 + n(\bar{y} - \theta)^2]} \mathrm{d} \sigma^2\\
  &\propto a^{-n/2} \int_{(0, \infty)} z^{(n-2)/2}\EXP{-z} \mathrm{d}z\\
  &\propto a^{-n/2}\\
  &= [(n-1)s + n(\theta - \bar{y})]^{-n/2}\\
  &\propto \left[1 + \frac{(\theta - \bar{y})^2}{(n-1) s^2 / n}\right]^{-n/2} \,
\end{align}
which concludes our first analysis implying that $\theta \mid y \sim t_{n-1}(\bar{y}, \sigma^2/n)$.




We know that for the problem at hand the standard maximum likelihood estimators
and their variances are given by\footnote{\textcolor{red}{See appendix for proof.}}
\begin{align}
  \hat{\theta}_{ML} &= \frac{1}{n} \sum_i y_i = \bar{y}\,\\
  \hat{\sigma}_{ML}^2 &= \frac{1}{n} \sum_i (y_i - \bar{y})^2 = \frac{n-1}{n} s^2\,\\
  \mathrm{I}(\theta, \sigma)^{-1} &= \sqmat{\sigma^2 / n & 0\\0&\sigma^2/(2n)} \,.
\end{align}
The Bayesian counterpart to the ML-Estimator is the \emph{maximum a posteriori estimate},
or in short \emph{MAP}. For $\theta \mid y$ we derived a noncentral Student's t-distribution
with mean $\bar{y}$ and variance $\sigma^2/n$. Since this distribution is unimodal and
symmetric the MAP estimate is simply the mean. We note that it is equivalent to
the ML estimate on both the point estimate and included variance. For $\sigma^2$
the results look slightly different. The mode and the mean of $\sigma^2 \mid y$
are given by $\frac{n-1}{n+1} s^2$ and $\frac{n-1}{n-3} s^2$, respectively.
Still we see a very close resemblance of the Bayesian estimator to the ML estimator.
One could say that this is a property which is desirable for Bayesian estimators
using uninformative priors. One obvious advantage of the Bayesian approach is the
ease with which we can compute arbitrary probabilities using the posterior density.
\textbf{Conjugate prior}

\subsection{Tables}
