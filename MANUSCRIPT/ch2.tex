\section{Hierarchical Models}
In the following we consider hierarchical models, when they are applicable and how to estimate them.
We begin by presenting the idea of hiearchical data and modeling.
Then we show how to solve a simple model analytically, before ending the section with introducing two main ideas.
One, hierarchical linear models, arguably the most important subclass of hiearchical models.
And two, a method to sample from the posteriors arising in complex settings.

\subsection{Hierarchical Data and Modeling}
Here we consider what makes data hierarchical and how we can use this component to model additional structure.
Hierarchical data is present if the data can be clustered on some level; e.g. children in schools, survey responses on different years in different states or experiments in multiple labs.
From the examples we see that there must not be a clear \emph{hierarchy} defined on the data.
This is one of the reasons why some authors nowadays prefer the more general terms \emph{multi-level data} and \emph{multi-level model}, see for instance \citet{GelmanHill2007}.
We categorize models by the number of levels they incorporate and if they use \emph{nested} or \emph{non-nested} data.
In this paper we consider two-level models for nested data and refer again to \citet{GelmanHill2007} for a treatment of models with more levels and non-nested data.

In practice we can store hierarchical data very efficiently using normalized relational dataframes. Let us continue the example of children in schools and assume we observe their results on a test, the parental income and the number of teachers per child in the school.
Table \ref{tab:relational_table} portrays how multi-level data in this case could be stored.
Having gained some intuition let us define our formal notation.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{l l l l}
child & result & income & school\\
\hline
1 & 10 & 500 & 1\\
2 & 9 & 450 & 1\\
3 & 12 & 520 & 2\\
4 & 10 & 490 & 1
\end{tabular}
\quad
\begin{tabular}{l l}
school & teacher\\
\hline
1 & 0.5\\
2 & 0.7
\end{tabular}
\end{center}
\caption{Two tables containing fictional hierarchical data. Left: Data on the child-level, that is the test \emph{results}, parental \emph{income} and \emph{school} id. Right: Data on the school-level, in this case the number of \emph{teachers} per child.}
\label{tab:relational_table}
\end{table}

\noindent
Assume we observe data on $i=1,\mydots,n$ units which are clustered among $j=1,\mydots,J$ groups.
Naturally we consider some outcome $y_i$ on the unit-level.
Following the idea of different tables for different levels from above, we write $x_i$ for the covariates that vary by unit and $u_j$ for the covariates that only vary on the group-level.
We link the two by writing $j[i]$ for the index of the group to which individual $i$ belongs, i.e. the full set of covariates from individual $i$ is given by $(x_i, u_{j[i]})$. But how can we utilize this hierarchical structure?

The main idea behind hierarchical modeling is that we build (simple) models on each level while using the dependent variables from higher levels as input parameters on lower levels.
For a general (two-level) case we may write
\begin{align}
  y_i \mid \theta_{j[i]} &\sim p(y_i \mid x_i, \theta_{j[i]}) \,,\\
  \theta_j \mid \phi &\sim p(\theta_j \mid u_j, \phi) \,,\\
  \phi &\sim p(\phi \mid \zeta) \text{ with } \zeta \text{ fixed } \,,
\end{align}
where we suppress the dependence on $x_i$ and $u_j$ by assuming they are fixed.
In the first level we model the observations $y_i$ depending on the covariates $x_i$ and parameters $\theta_j$.
As in a classical Bayesian model we continue by modeling the parameters; However, in contrast to section 2 we do not just assume some prior distribution for the parameters but we \emph{explicitly} model the parameter.
From a Bayesian viewpoint this can be seen as a generalization to prior modeling.
Despite this Bayesian interpretation, the first two equations define a proper non-Bayesian hierarchical model.
We will see that in the linear case these models are well known in the frequentist world as \emph{mixed effects models} or \emph{random coefficient models}.
To put the Bayesian in Bayesian hiearchical model we have to assign a prior distributions on the parameters $\phi$.
As $\phi$ are themselves parameters for the parameter $\theta_j$, one often speaks of priors on $\theta_j$ and \emph{hyperpriors} on the \emph{hyperparameter} $\phi$.

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}%
  [vertex/.style={circle,draw=black,fill=white, minimum size=1cm},
  node distance=2.5cm,
  >=latex,
  on grid]
  \node[vertex] (phi) {$\phi$};
  \node[vertex,right=of phi] (theta) {$\theta_{j[i]}$};
  \node[vertex,below right= 1cm and 2.5cm of theta] (y) {$y_i$};
  \node[rectangle, draw=black, minimum size=0.9cm,left= of phi] (zeta) {$\zeta$};
  \node[rectangle, draw=black, minimum size=0.9cm,above=1.5cm of theta] (u) {$u_{j[i]}$};
  \node[rectangle, draw=black, minimum size=0.9cm, above right=1cm and 2.5cm of y] (x) {$x_i$};
  \draw[->]
    (phi) edge (theta)
    (theta) edge (y)
    (zeta) edge (phi)
    (u) edge (theta)
    (x) edge (y);
\end{tikzpicture}
\end{center}
\label{tik:general_sem}
\caption{A generic two-level Bayesian hierarchical model depicted as a directed acyclical graph modeling a single generic observation. Circled parameters denote random quantities while parameters contained in squares denote fixed quantities.}
\end{figure}


Figure \ref{tik:general_sem} illustrates the conditional dependence structure of modeling a generic observation $y_i$.
In contrast, consider figure \ref{tik:group_sem} which illustrates the conditional dependence structure when modeling a generic observation $y(j)$ in group $j$.
We depict random quantities in circles and fixed quantities in squares.
Not only do these graphical representations help with understanding the structure of the model but we will see that when solving for the posterior they give an immediate way to check which parameters are conditionally independent.

\begin{figure}[ht]
\begin{center}
\begin{tikzpicture}%
  [vertex/.style={circle,draw=black,fill=white, minimum size=1cm},
  node distance=2.5cm,
  >=latex,
  on grid]
  \node[vertex] (phi) {$\phi$};
  \node[rectangle, draw=black, minimum size=0.9cm,left=2cm of phi] (zeta) {$\zeta$};
  \node[vertex,below left=1.5cm and 2cm of phi] (theta1) {$\theta_1$};
  \node[vertex,below right=1.5cm and 2cm of phi] (thetaJ) {$\theta_J$};
  \node[below=1.5cm of phi] (dots1) {$\dots$};
  \node[rectangle, draw=black, minimum size=0.9cm, left=2cm of theta1] (u1) {$u_1$};
  \node[rectangle, draw=black, minimum size=0.9cm, right=2cm of thetaJ] (uJ) {$u_J$};
  \node[vertex,below=2cm of theta1] (y1) {$y(1)$};
  \node[vertex,below=2cm of thetaJ] (yJ) {$y(J)$};
  \node[rectangle, draw=black, minimum size=0.9cm, left=2cm of y1] (x1) {$x(1)$};
  \node[rectangle, draw=black, minimum size=0.9cm, right=2cm of yJ] (xJ) {$x(J)$};
  \node[below=3.5cm of phi] (dots1) {$\dots$};
  \draw[->]
    (zeta) edge (phi)
    (phi) edge (theta1)
    (phi) edge (thetaJ)
    (u1) edge (theta1)
    (uJ) edge (thetaJ)
    (theta1) edge (y1)
    (thetaJ) edge (yJ)
    (x1) edge (y1)
    (xJ) edge (yJ);
\end{tikzpicture}
\end{center}
\label{tik:group_sem}
\caption{A generic two-level Bayesian hierarchical model depicted as a directed acyclical graph modeling generic observations $y(j)$ in groups $j = 1,\mydots, J$. Circled parameters denote random quantities while parameters contained in squares denote fixed quantities.}
\end{figure}

\subsection{Solving for the Posterior Analytically}
As in the previous section we consider first an analytical derivation of the posterior distributions using a simple normal model, before dealing with more complex settings.

The objects of our interest are given by the joint posterior $p(\theta_j, \phi \mid y)$
and its two marginal posteriors. In order to derive these analytically we must
make convenient distributional assumptions. For the sake of exposition, we choose


\subsection{Hierarchical Linear Models}
The following subsection presents hierarchical linear models, an important subclass of the general multi-level model.
As in classical statistics linear models are usually simpler to estimate and easier to interpret.
The standard critique on linear models from classical statistics also applies here; however, we will see that due to the hiearchical nature we are able to model very complex structure even under a linearity assumption.

\subsubsection{Varying Slopes Model With One Predictor In Each Level}
We assume that units $i = 1,\mydots,n$ can be divided into $J$ distinct groups.
We start with a very simple model assuming that intercept is fixed for all
groups, that is
\begin{align}
  y = \alpha + \beta_j x + \epsilon \,,
\end{align}
with $\epsilon$ following a mean zero normal distribution with variance
$\sigma_{\epsilon}^2$.
To incorporate the idea that the groups follow a common structure we also
assume
\begin{align}
  \beta_j &= \gamma_0 + \gamma_1 u_j + \eta \,,
\end{align}
for $j = 1,\mydots,J$, with $\eta$ mean zero normal with variance $\sigma_\eta^2$.

Since $\gamma_0$ and $\gamma_1$ do not vary by group they are sometimes referred
to as \emph{fixed effects}. Similary as $\eta$ is drawn randomly for each group
it is sometimes called a \emph{random effect}. Put together this shows the close
resemblance of the hierarchical linear model to classical mixed effects models
\textcolor{red}{(some reference here would be nice!)}

Following the notation of \textcolor{red}{Gelman and Hill (2007)} we describe the
model equation of a single individual $i$ by
\begin{align}
  y_i = \alpha + \beta_{j[i]} x_i + \epsilon_i \,,
\end{align}
where $j[i]$ denotes the group to which individual $i$ belongs.

The model defined by the assumptions and equations above \textcolor{red}{(where and what)} can of course be made
arbitrarily complex. For example we could add higher order polynomial terms or
more predictors, as in regular linear regression modeling. Further, the
normality assumption of the errors could be relaxed and most importantly, why
stop at two levels? Naturally we could model the group-level coefficients using
a third level. For the sake of a simpler explaination however, we will stick to
the presented model.

%\subsubsection{Varying Intercept and Slope Model with One Predictor in Each Level}
%We assume that units $i = 1,\mydots,n$ can be divided into $J$ distinct groups.
%In each group $j$ we model our outcome variable $y$ as a linear function in $x$,
%that is
%\begin{align}
  %y = \alpha_j + \beta_j x + \epsilon \,,
%\end{align}
%with $\epsilon$ following a mean zero normal distribution with variance
%$\sigma_{\epsilon}^2$.
%To incorporate the idea that the groups follow a common structure we also
%assume
%\begin{align}
  %\alpha_j &= \gamma_0^{\alpha} + \gamma_1^{\alpha} u_j + \eta_\alpha \\
  %\beta_j &= \gamma_0^{\beta} + \gamma_1^{\beta} u_j + \eta_\beta \,,
%\end{align}
%for $j = 1,\mydots,J$, with
%\begin{align}
  %\sqmat{\eta_\alpha \\ \eta_\beta} \sim \normal{\sqmat{\sigma_\alpha^2 & \rho \sigma_\alpha \sigma_\beta \\ & \sigma_\beta^2}}
%\end{align}
%
%Following the notation of \textcolor{red}{Gelman and Hill (2007)} we describe the model equation of a single individual $i$ by
%\begin{align}
  %y_i = \alpha_{j[i]} + \beta_{j[i]} x_i + \epsilon_i \,,
%\end{align}
%where $j[i]$ denotes the group to which individual $i$ belongs.
%This particular model is known as the two-level varying intercept / varying slope model with one unit-level predictor (here $x_i$) and one group-level predictor
%(here $u_i$).
%The model defined by equations \textcolor{red}{(1) - (4)} can of
%course be made arbitrarily complex by adding higher order polynomial terms or
%more predictors, as in a regular linear regression models. Further the normality
%assumption of equation \textcolor{red}{(4)} is not mandotory and can be swapped
%with nearly any other distributional assumption. Also, why stop at two levels?
%We could naturally model the coefficients in equation \textcolor{red}{2 and 3}
%using a third level. All these extensions can in practice be necessary when
%modelling complex structures; however for the sake of simplicity and clarity
%we will stick to our clean model.
